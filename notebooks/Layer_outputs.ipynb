{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import tables\n",
    "from tqdm import tqdm\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.signal import butter, filtfilt, lombscargle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from dlml.utils import collect_experiments\n",
    "from dlml.data import read_area_values, load_data_areas, load_data_slide\n",
    "from dlml.nn import predict, DownSampling1D, SpectralPooling, MaxPooling1DWithArgmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36871b",
   "metadata": {},
   "source": [
    "#### Find the best experiment given a set of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b90255",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_ID = 1\n",
    "area_measure = 'momentum'\n",
    "stoch_load_bus_IDs = []\n",
    "rec_bus_IDs = [3]\n",
    "H_G1, D, DZA = None, None, None # 500, 2, 0\n",
    "additional_tags = ['ReLU_none', 'converted_from_PowerFactory', 'all_stoch_loads', 'data_subset']\n",
    "missing_tags = []\n",
    "use_FFT = False\n",
    "if use_FFT:\n",
    "    additional_tags.append('fft')\n",
    "else:\n",
    "    missing_tags.append('fft')\n",
    "pooling_type = 'argmax'\n",
    "if pooling_type is not None and pooling_type != '':\n",
    "    additional_tags.append(pooling_type + '_pooling')\n",
    "\n",
    "# training on frequency data, 2 output values\n",
    "# experiment_ID = '9ea493c789b542bf979c51a6031f4044'\n",
    "# training on frequency data, 4 output values\n",
    "# experiment_ID = 'f6d9a03f1cfe450288e9cb86da94235f'\n",
    "# training on time series data, 2 output values\n",
    "# experiment_ID = '034a1edb0797475b985f0e1335dab383'\n",
    "# training on time series data, 4 output values\n",
    "# experiment_ID = 'b346a89d384c4db2ba4058a2c83c4f12'\n",
    "\n",
    "# training on time series data, 2 output values, with MaxPooling1DWithArgmax layer\n",
    "experiment_ID = '9034f8bc4f874c938dfa5f1f9ee04e82'\n",
    "\n",
    "if experiment_ID is not None:\n",
    "    from comet_ml.api import API, APIExperiment\n",
    "    api = API(api_key = os.environ['COMET_API_KEY'])\n",
    "    experiment = api.get_experiment('danielelinaro', 'inertia', experiment_ID)\n",
    "    sys.stdout.write(f'Getting metrics for experiment {experiment_ID[:6]}... ')\n",
    "    sys.stdout.flush()\n",
    "    metrics = experiment.get_metrics()\n",
    "    sys.stdout.write('done.\\n')\n",
    "    val_loss = []\n",
    "    for m in metrics:\n",
    "        if m['metricName'] == 'val_loss':\n",
    "            val_loss.append(float(m['metricValue']))\n",
    "        elif m['metricName'] == 'mape_prediction':\n",
    "            MAPE = float(m['metricValue'])\n",
    "    val_loss = np.array(val_loss)\n",
    "else:\n",
    "    # find the best experiment that matches the set of tags above\n",
    "    experiments = collect_experiments(area_ID, area_measure=area_measure, D=D, DZA=DZA, \\\n",
    "                                      stoch_load_bus_IDs=stoch_load_bus_IDs, H_G1=H_G1, \\\n",
    "                                      rec_bus_IDs=rec_bus_IDs, additional_tags=additional_tags, \\\n",
    "                                      missing_tags=missing_tags, verbose=True)\n",
    "    experiment_IDs = list(experiments.keys())\n",
    "    experiment_ID = experiment_IDs[np.argmin([expt['val_loss'].min() for expt in experiments.values()])]\n",
    "    experiment_ID = experiment_IDs[np.argmin([expt['MAPE'] for expt in experiments.values()])]\n",
    "\n",
    "    MAPE = experiments[experiment_ID]['MAPE']\n",
    "    loss = experiments[experiment_ID]['loss']\n",
    "    val_loss = experiments[experiment_ID]['val_loss']\n",
    "    batch_loss = experiments[experiment_ID]['batch_loss']\n",
    "    tags = experiments[experiment_ID]['tags']\n",
    "print(f'Selected experiment is {experiment_ID[:6]} (val_loss = {val_loss.min():.4f}, MAPE = {MAPE:.4f}%).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0a7fe",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81320651",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_path = '../experiments/neural_network/'\n",
    "network_parameters = pickle.load(open(os.path.join(experiments_path, experiment_ID, 'parameters.pkl'), 'rb'))\n",
    "checkpoint_path = experiments_path + experiment_ID + '/checkpoints/'\n",
    "checkpoint_files = glob.glob(checkpoint_path + '*.h5')\n",
    "try:\n",
    "    epochs = [int(os.path.split(file)[-1].split('.')[1].split('-')[0]) for file in checkpoint_files]\n",
    "    best_checkpoint = checkpoint_files[epochs.index(np.argmin(val_loss) + 1)]\n",
    "except:\n",
    "    best_checkpoint = checkpoint_files[-1]\n",
    "try:\n",
    "    model = keras.models.load_model(best_checkpoint)\n",
    "except:\n",
    "    if pooling_type == 'downsample':\n",
    "        custom_objects = {'DownSampling1D': DownSampling1D}\n",
    "    elif pooling_type == 'spectral':\n",
    "        custom_objects = {'SpectralPooling': SpectralPooling}\n",
    "    elif pooling_type == 'argmax':\n",
    "        custom_objects = {'MaxPooling1DWithArgmax': MaxPooling1DWithArgmax}\n",
    "    with keras.utils.custom_object_scope(custom_objects):\n",
    "        model = keras.models.load_model(best_checkpoint)\n",
    "if pooling_type == 'argmax':\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, MaxPooling1DWithArgmax):\n",
    "            print(f'Setting store_argmax = True for layer \"{layer.name}\".')\n",
    "            layer.store_argmax = True\n",
    "x_train_mean = network_parameters['x_train_mean']\n",
    "x_train_std  = network_parameters['x_train_std']\n",
    "try:\n",
    "    x_train_min = network_parameters['x_train_min']\n",
    "    x_train_max = network_parameters['x_train_max']\n",
    "except:\n",
    "    pass\n",
    "var_names = network_parameters['var_names']\n",
    "print(f'Loaded network from {best_checkpoint}.')\n",
    "print(f'Variable names: {var_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e540f",
   "metadata": {},
   "source": [
    "#### Plot the model topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f8316",
   "metadata": {},
   "source": [
    "#### Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08887b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = 'test'\n",
    "use_fft = network_parameters['use_fft'] if 'use_fft' in network_parameters else False\n",
    "data_dirs = []\n",
    "for area_ID,data_dir in zip(network_parameters['area_IDs'], network_parameters['data_dirs']):\n",
    "    data_dirs.append(data_dir.format(area_ID))\n",
    "data_dir = os.path.join('..', data_dirs[0])\n",
    "data_files = sorted(glob.glob(data_dir + os.path.sep + f'*_{set_name}_set.h5'))\n",
    "ret = load_data_areas({set_name: data_files}, network_parameters['var_names'],\n",
    "                        network_parameters['generators_areas_map'][:1],\n",
    "                        network_parameters['generators_Pnom'],\n",
    "                        network_parameters['area_measure'],\n",
    "                        trial_dur=network_parameters['trial_duration'],\n",
    "                        max_block_size=100,\n",
    "                        use_tf=False, add_omega_ref=True,\n",
    "                        use_fft=use_fft)\n",
    "y = ret[2][set_name]\n",
    "if use_fft:\n",
    "    X = [(ret[1][set_name][i] - m) / (M - m) for i,(m,M) in enumerate(zip(x_train_min, x_train_max))]\n",
    "    F = ret[0]\n",
    "else:\n",
    "    X = [(ret[1][set_name][i] - m) / s for i,(m,s) in enumerate(zip(x_train_mean, x_train_std))]\n",
    "    t = ret[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f46974",
   "metadata": {},
   "source": [
    "#### Predict the momentum using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6537dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [np.where(y == mom)[0] for mom in np.unique(y)]\n",
    "n_mom_values = len(idx)\n",
    "momentum = [np.squeeze(model.predict(X[0][jdx])) for jdx in idx]\n",
    "mean_momentum = [m.mean() for m in momentum]\n",
    "stddev_momentum = [m.std() for m in momentum]\n",
    "print('Mean momentum:', mean_momentum)\n",
    "print(' Std momentum:', stddev_momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4a70d",
   "metadata": {},
   "source": [
    "### Plot the inputs and their FFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8366a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('tab10', n_mom_values)\n",
    "if use_fft:\n",
    "    Xf = X\n",
    "else:\n",
    "    data_files_training = sorted(glob.glob(data_dir + os.path.sep + f'*_training_set.h5'))\n",
    "    if len(data_files_training) == 0:\n",
    "        data_files_training = sorted(glob.glob(data_dir + os.path.sep + f'*_test_set.h5'))\n",
    "    ret_fft = load_data_areas({'training': data_files_training}, network_parameters['var_names'],\n",
    "                        network_parameters['generators_areas_map'][:1],\n",
    "                        network_parameters['generators_Pnom'],\n",
    "                        network_parameters['area_measure'],\n",
    "                        trial_dur=network_parameters['trial_duration'],\n",
    "                        max_block_size=200,\n",
    "                        use_tf=False, add_omega_ref=True,\n",
    "                        use_fft=True)\n",
    "    x_train_min_fft = np.array([val.min() for val in ret_fft[1]['training']], dtype=np.float32)\n",
    "    x_train_max_fft = np.array([val.max() for val in ret_fft[1]['training']], dtype=np.float32)\n",
    "    ret = load_data_areas({set_name: data_files}, network_parameters['var_names'],\n",
    "                        network_parameters['generators_areas_map'][:1],\n",
    "                        network_parameters['generators_Pnom'],\n",
    "                        network_parameters['area_measure'],\n",
    "                        trial_dur=network_parameters['trial_duration'],\n",
    "                        max_block_size=100,\n",
    "                        use_tf=False, add_omega_ref=True,\n",
    "                        use_fft=True)\n",
    "    F = ret[0]\n",
    "    Xf = [(ret[1][set_name][i] - m) / (M - m) for i,(m,M) in enumerate(zip(x_train_min_fft,\n",
    "                                                                           x_train_max_fft))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for j,jdx in enumerate(idx):\n",
    "    mean = X[0][jdx].mean(axis=0)\n",
    "    stddev = X[0][jdx].std(axis=0)\n",
    "    ci = 1.96 * stddev / np.sqrt(jdx.size)\n",
    "    ax[0].fill_between(t, mean + ci, mean - ci, color=cmap(j))\n",
    "    mean = Xf[0][jdx].mean(axis=0)\n",
    "    stddev = Xf[0][jdx].std(axis=0)\n",
    "    ci = 1.96 * stddev / np.sqrt(jdx.size)\n",
    "    ax[1].fill_between(F, mean + ci, mean - ci, color=cmap(j))\n",
    "\n",
    "for a in ax:\n",
    "    for side in 'right','top':\n",
    "        a.spines[side].set_visible(False)\n",
    "    a.grid(which='major', axis='both', ls=':', lw=0.5, color=[.6,.6,.6])\n",
    "ax[1].set_xscale('log')\n",
    "ax[0].set_xlabel('Time [min]')\n",
    "ax[0].set_ylabel('Normalized trace')\n",
    "ax[1].set_xlabel('Frequency [Hz]')\n",
    "ax[1].set_ylabel('Normalized FFT')\n",
    "fig.tight_layout()\n",
    "fig.savefig(f'spectra_{n_mom_values}_momentum_levels.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74371b1c",
   "metadata": {},
   "source": [
    "#### Make as many submodels as there are layers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a2934f6",
   "metadata": {},
   "source": [
    "multi_output_model = keras.Model(inputs=model.inputs,\n",
    "                                 outputs=[layer.output for layer in model.layers[1:]])\n",
    "Y = [multi_output_model.predict(X[0][jdx]) for jdx in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_layers = len(model.layers)\n",
    "N_vars = len(var_names)\n",
    "submodels = [\n",
    "    keras.Model(inputs=model.inputs,\n",
    "                outputs=[model.layers[j].output for j in range(i,i+N_vars)])\n",
    "     for i in range(N_vars, N_layers - N_vars - 3, N_vars)\n",
    "]\n",
    "for layer in model.layers[-4:]:\n",
    "    submodels.append(keras.Model(inputs=model.inputs, outputs=layer.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf02c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_id = 1\n",
    "submodel = submodels[layer_id]\n",
    "submodel.summary()\n",
    "# Y = [submodel.predict(X[0][jdx]) for jdx in idx]\n",
    "# print(Y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_trials = X[0].shape[0]\n",
    "T, Y = [], []\n",
    "_, n_samples, n_kernels = submodel.output.shape\n",
    "for jdx in idx:\n",
    "    n_trials = jdx.size\n",
    "    T.append(np.zeros((n_trials, n_samples, n_kernels)))\n",
    "    Y.append(np.zeros((n_trials, n_samples, n_kernels)))\n",
    "    for j in tqdm(range(len(jdx))):\n",
    "        trial = jdx[j]\n",
    "        # the input to the model\n",
    "        x = X[0][trial:trial+1, :]\n",
    "        # the output of the model\n",
    "        Y[-1][j, :, :] = submodel(x)\n",
    "        t_out = t[np.newaxis, :]\n",
    "        for layer in submodel.layers:\n",
    "            if isinstance(layer, keras.layers.Conv1D):\n",
    "                kernel_size = layer.kernel_size[0]\n",
    "                n_filters = layer.filters\n",
    "                t_out = np.tile(t_out[:, kernel_size-1:], [n_filters,1])\n",
    "            elif isinstance(layer, MaxPooling1DWithArgmax):\n",
    "                ndx = tf.squeeze(layer.argmax).numpy().T // kernel_size\n",
    "                t_out = np.array([t_out[k, kdx] for k, kdx in enumerate(ndx)])\n",
    "        T[-1][j, :, :] = t_out.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yf = []\n",
    "hp_filter = False\n",
    "if layer_id == 0:\n",
    "    m = n_samples // 2\n",
    "    dt = np.diff(t[:2])[0]\n",
    "    filter_order = 10\n",
    "    cutoff = 0.1\n",
    "    b,a = butter(filter_order//2, cutoff, 'hp', fs=1/dt)\n",
    "    for y in Y:\n",
    "        n_samples = y.shape[1]\n",
    "        if hp_filter:\n",
    "            tmp = filtfilt(b, a, y, axis=1)\n",
    "        else:\n",
    "            tmp = y\n",
    "        tmp = fft(tmp, axis=1)\n",
    "        Yf.append(2.0 / n_samples * np.abs(tmp[:, :n_samples//2, :]))    \n",
    "    freq = fftfreq(n_samples, dt)[:n_samples//2]\n",
    "else:\n",
    "    if os.path.isfile(f'Yf_{layer_id}.npz'):\n",
    "        data = np.load(f'Yf_{layer_id}.npz')\n",
    "        Yf = data['Yf']\n",
    "        freq = data['freq']\n",
    "    else:\n",
    "        for q in range(len(Y)):\n",
    "            Yf.append(np.zeros((n_traces, F.size-1, n_kernels)))\n",
    "            for i in tqdm(range(n_traces)):\n",
    "                for k in range(n_kernels):\n",
    "                    x = np.squeeze(T[q][i, :, k]).copy()\n",
    "                    y = np.squeeze(Y[q][i, :, k]).copy()\n",
    "                    Yf[-1][i, :, k] = lombscargle(x, y, F[1:])\n",
    "        freq = F[1:]\n",
    "        np.savez_compressed(f'Yf_{layer_id}.npz', Yf=Yf, freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becde011",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_traces, n_samples, n_kernels = Y[0].shape\n",
    "rows, cols = n_kernels // 8, 8\n",
    "fig,ax = plt.subplots(rows, cols, figsize=(cols*1.5, rows),\n",
    "                      sharex=True, sharey=False)\n",
    "show_fft = True\n",
    "hp_filter = False\n",
    "if use_fft:\n",
    "    n = -1\n",
    "    m = int(n_samples / 3)\n",
    "    show_fft = False\n",
    "else:\n",
    "    n = -1\n",
    "    if layer_id == 0:\n",
    "        m = n_samples\n",
    "    else:\n",
    "        m = freq.size\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        k = i * cols + j\n",
    "        if n > 0:\n",
    "            for q in range(len(Y)):\n",
    "                ax[i][j].plot(Y[q][:n, :m, k].T, color=cmap(q), lw=0.5)\n",
    "        else:\n",
    "            for q in range(len(Y)):\n",
    "                if show_fft:\n",
    "                    mean = Yf[q][:, :, k].mean(axis=0)\n",
    "                    stddev = Yf[q][:, :, k].std(axis=0)\n",
    "                    ci = 1.96 * stddev / np.sqrt(Yf[q].shape[0])\n",
    "                    ax[i][j].fill_between(freq[:m], mean[:m] + ci[:m], mean[:m] - ci[:m], color=cmap(q))\n",
    "                else:\n",
    "                    mean = Y[q][:, :, k].mean(axis=0)\n",
    "                    stddev = Y[q][:, :, k].std(axis=0)\n",
    "                    ci = 1.96 * stddev / np.sqrt(Y[q].shape[0])\n",
    "                    if layer_id > 0:\n",
    "                        ax[i][j].fill_between(np.arange(m), mean[:m] + ci[:m], mean[:m] - ci[:m], color=cmap(q))\n",
    "                        ax[-1][j].set_xlabel('Sample')\n",
    "                        ax[i][j].set_xticks(np.linspace(0, n_samples, 3, dtype=np.int32))\n",
    "                    else:\n",
    "                        ax[i][j].fill_between(T[0][0,:,0], mean + ci, mean - ci, color=cmap(q))\n",
    "                        ax[i][j].set_xticks(np.linspace(0, 60, 3))\n",
    "                        ax[-1][j].set_xlabel('Time [min]')\n",
    "        if show_fft:\n",
    "            ax[i][j].set_xscale('log')\n",
    "#         else:\n",
    "#             ax[i][j].set_xticks([0, m])\n",
    "        for side in 'right','top':\n",
    "            ax[i][j].spines[side].set_visible(False)\n",
    "        ax[i][j].set_yticklabels([])\n",
    "fig.tight_layout(pad=0)\n",
    "output_file = experiment_ID[:6] + f'_layer_{layer_id}_outputs'\n",
    "if show_fft:\n",
    "    output_file += '_spectra'\n",
    "fig.savefig(output_file + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.squeeze(model.layers[layer_id].weights[0].numpy())\n",
    "print('weights shape:', weights.shape)\n",
    "if len(weights.shape) > 2:\n",
    "    input_id = 0\n",
    "    weights = np.squeeze(weights[:, input_id, :])\n",
    "else:\n",
    "    input_id = 0\n",
    "kernel_size, n_kernels = weights.shape\n",
    "if show_fft:\n",
    "    weightsf = fft(weights, axis=0)\n",
    "    weightsf = 2.0 / kernel_size * np.abs(weightsf[:kernel_size//2, :])\n",
    "rows, cols = n_kernels // 8, 8\n",
    "fig,ax = plt.subplots(rows, cols, figsize=(cols*2, rows*1.5), sharex=True, sharey=True)\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        k = i * 8 + j\n",
    "        if show_fft:\n",
    "            ax[i][j].plot(weightsf[:,k], 'k', lw=1)\n",
    "            ax[i][j].set_xticks([0, kernel_size//2])\n",
    "        else:\n",
    "            ax[i][j].plot(weights[:,k], 'k', lw=1)\n",
    "            ax[i][j].set_xticks([0, kernel_size])\n",
    "        for side in 'right','top':\n",
    "            ax[i][j].spines[side].set_visible(False)\n",
    "fig.tight_layout(pad=0)\n",
    "output_file = experiment_ID[:6] + f'_layer_{layer_id}_input_{input_id}_weights'\n",
    "if show_fft:\n",
    "    output_file += '_spectra'\n",
    "fig.savefig(output_file + '.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
