{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import dtw\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from dlml.utils import collect_experiments\n",
    "from dlml.data import read_area_values, load_data_areas, load_data_slide\n",
    "from dlml.nn import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aacd03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_ID = 1\n",
    "area_measure = 'momentum'\n",
    "stoch_load_bus_IDs = [3]\n",
    "rec_bus_IDs = [3, 14, 17]\n",
    "H_G1, D, DZA = None, None, None # 500, 2, 0\n",
    "additional_tags = ['ReLU_none', 'converted_from_PowerFactory']\n",
    "\n",
    "experiments = collect_experiments(area_ID, area_measure=area_measure, D=D, DZA=DZA, \\\n",
    "                                  stoch_load_bus_IDs=stoch_load_bus_IDs, H_G1=H_G1, \\\n",
    "                                  rec_bus_IDs=rec_bus_IDs, additional_tags=additional_tags, \\\n",
    "                                  verbose=True)\n",
    "experiment_IDs = list(experiments.keys())\n",
    "experiment_ID = experiment_IDs[np.argmin([expt['val_loss'].min() for expt in experiments.values()])]\n",
    "MAPE = experiments[experiment_ID]['MAPE']\n",
    "loss = experiments[experiment_ID]['loss']\n",
    "val_loss = experiments[experiment_ID]['val_loss']\n",
    "batch_loss = experiments[experiment_ID]['batch_loss']\n",
    "tags = experiments[experiment_ID]['tags']\n",
    "print(f'The best experiment is {experiment_ID[:6]} (val_loss = {val_loss.min():.4f}, MAPE = {MAPE:.4f}%).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_path = '../experiments/neural_network/'\n",
    "checkpoint_path = experiments_path + experiment_ID + '/checkpoints/'\n",
    "checkpoint_files = glob.glob(checkpoint_path + '*.h5')\n",
    "network_parameters = pickle.load(open(experiments_path + experiment_ID \\\n",
    "                                      + '/parameters.pkl', 'rb'))\n",
    "epochs = [int(os.path.split(file)[-1].split('.')[1].split('-')[0]) for file in checkpoint_files]\n",
    "best_checkpoint = checkpoint_files[epochs.index(np.argmin(val_loss) + 1)]\n",
    "model = keras.models.load_model(best_checkpoint, compile=False)\n",
    "model.compile()\n",
    "data_dirs = ['..' + os.path.sep +\n",
    "             os.path.sep.join([d for d in data_dir.split(os.path.sep) if '{}' not in d])\n",
    "             for data_dir in network_parameters['data_dirs']]\n",
    "for i in range(len(data_dirs)):\n",
    "#              f'/H_G1_{H_G1}/stoch_load_bus_' + '-'.join(map(str, stoch_load_bus_IDs))\n",
    "    if H_G1 is not None:\n",
    "        data_dirs[i] += f'/H_G1_{H_G1}'\n",
    "    data_dirs[i] += '/stoch_load_bus_' + '-'.join(map(str, stoch_load_bus_IDs))\n",
    "# we need mean and standard deviation of the training set to normalize the data\n",
    "x_train_mean = network_parameters['x_train_mean']\n",
    "x_train_std  = network_parameters['x_train_std']\n",
    "data_dir = data_dirs[0]\n",
    "tmp = [re.findall('.*_bus', var_name)[0] for var_name in network_parameters['var_names'] if 'bus' in var_name]\n",
    "var_names_fmt = OrderedDict({k + '{}': [] for k in tmp})\n",
    "tmp = [re.findall('.*_line', var_name)[0] for var_name in network_parameters['var_names'] if 'line' in var_name]\n",
    "for k in tmp:\n",
    "    var_names_fmt[k + '_{}_{}'] = []\n",
    "var_names_fmt = list(var_names_fmt.keys())\n",
    "if len(rec_bus_IDs) == 0:\n",
    "    rec_bus_IDs = list(np.unique([int(re.findall('\\d+', var_name)[0]) \\\n",
    "                                  for var_name in network_parameters['var_names']]))\n",
    "    rec_bus_list = 'buses_' + '-'.join(map(str, rec_bus_IDs))\n",
    "if not os.path.isdir(data_dir):\n",
    "    raise Exception(f'{data_dir}: no such directory')\n",
    "print(f'Loaded network from {best_checkpoint}.')\n",
    "print(f'Data directory is {data_dir}.')\n",
    "print(f'Variable names: {var_names_fmt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=False, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4781234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_H = OrderedDict([\n",
    "    ('G01', 5.00), ('G02', 4.33), ('G03', 4.47), ('G04', 3.57), ('G05', 4.33),\n",
    "    ('G06', 4.35), ('G07', 3.77), ('G08', 3.47), ('G09', 3.45), ('G10', 4.20)\n",
    "])\n",
    "\n",
    "\n",
    "generators_areas_map = {\n",
    "    'default': [\n",
    "        ['G02', 'G03'],\n",
    "        ['G04', 'G05', 'G06', 'G07'],\n",
    "        ['G08', 'G09', 'G10'],\n",
    "        ['G01']\n",
    "    ]\n",
    "}\n",
    "\n",
    "P_nom = {'G01': 10000e6, 'G02': 700e6, 'G03': 800e6, 'G04':  800e6, 'G05':  300e6,\n",
    "         'G06':   800e6, 'G07': 700e6, 'G08': 700e6, 'G09': 1000e6, 'G10': 1000e6}\n",
    "\n",
    "\n",
    "window_dur = 60\n",
    "window_step = 1\n",
    "\n",
    "var_names = network_parameters['var_names']\n",
    "data_mean = {var_name: x_train_mean[k] for k,var_name in enumerate(var_names)}\n",
    "data_std = {var_name: x_train_std[k] for k,var_name in enumerate(var_names)}\n",
    "\n",
    "if area_measure == 'inertia':\n",
    "    measure_units = 's'\n",
    "elif area_measure == 'energy':\n",
    "    measure_units = r'GW$\\cdot$s'\n",
    "elif area_measure == 'momentum':\n",
    "    measure_units = r'GW$\\cdot$s$^2$'\n",
    "    \n",
    "stoch_load_bus_list = 'stoch_load_bus_' + '-'.join(map(str, stoch_load_bus_IDs))\n",
    "rec_bus_list = 'buses_' + '-'.join(map(str, rec_bus_IDs))\n",
    "\n",
    "abbrv = {'inertia': 'H', 'energy': 'E', 'momentum': 'M'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779dba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_values = [\n",
    "    default_H,\n",
    "    OrderedDict([\n",
    "        ('G01', 5.00), ('G02', 3.3), ('G03', 3.3), ('G04', 3.57), ('G05', 4.33),\n",
    "        ('G06', 4.35), ('G07', 3.77), ('G08', 3.47), ('G09', 3.45), ('G10', 4.20)\n",
    "    ]),\n",
    "    OrderedDict([\n",
    "        ('G01', 5.00), ('G02', 5.8), ('G03', 5.8), ('G04', 3.57), ('G05', 4.33),\n",
    "        ('G06', 4.35), ('G07', 3.77), ('G08', 3.47), ('G09', 3.45), ('G10', 4.2)\n",
    "    ])\n",
    "]\n",
    "N_H = len(H_values)\n",
    "\n",
    "measure_exact = []\n",
    "\n",
    "data_normalized = []\n",
    "measure = []\n",
    "area_inertia = []\n",
    "\n",
    "for H in H_values:\n",
    "    data_file = data_dir + '/ieee39_PF_' + '_'.join(map(lambda h: f'{h:.3f}', H.values())) + '.h5'\n",
    "    _,_,v,_ = read_area_values(data_file, generators_areas_map['default'], P_nom, area_measure)\n",
    "    _,_,h,_ = read_area_values(data_file, generators_areas_map['default'], P_nom, 'inertia')\n",
    "    measure_exact.append(v[area_ID - 1])\n",
    "    area_inertia.append(h[area_ID - 1])\n",
    "\n",
    "    t, _, data_norm, data_sliding, _ = load_data_slide([data_file],\n",
    "                                                        var_names,\n",
    "                                                        data_mean,\n",
    "                                                        data_std,\n",
    "                                                        window_dur,\n",
    "                                                        window_step,\n",
    "                                                        add_omega_ref = False,\n",
    "                                                        verbose = True)\n",
    "    data_normalized.append(data_norm)\n",
    "    dt = np.diff(t[:2])[0]\n",
    "    time, HH, _ = predict(model, data_sliding, window_step)\n",
    "    measure.append(HH)\n",
    "measure_exact = np.array(measure_exact)\n",
    "area_inertia = np.array(area_inertia)\n",
    "measure_predicted = np.array(list(map(np.nanmean, measure)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_layers = len(model.layers)\n",
    "N_vars = len(var_names)\n",
    "submodels = [\n",
    "    keras.Model(inputs=model.inputs, outputs=[model.layers[i].output for i in range(i,i+N_vars)])\n",
    "     for i in range(N_vars, N_layers - N_vars - 3, N_vars)\n",
    "]\n",
    "for layer in model.layers[-4:]:\n",
    "    submodels.append(keras.Model(inputs=model.inputs, outputs=layer.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcdaf8c",
   "metadata": {},
   "source": [
    "## Continue from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab1c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_blocks = len(data_sliding[0][var_names[0]])\n",
    "inputs_to_network = []\n",
    "inputs_to_classifier = []\n",
    "outputs = []\n",
    "for data_slid in data_sliding:\n",
    "    for i in range(N_blocks):\n",
    "        x = {var_name: tf.constant(data_slid[var_name][i:i+1,:], dtype=tf.float32) for var_name in var_names}\n",
    "        inputs_to_network.append(np.concatenate([np.squeeze(data_slid[var_name][i:i+1,:]) for var_name in var_names]))\n",
    "        inputs_to_classifier.append(np.squeeze(submodels[-3].predict(x)))\n",
    "#         inputs_to_classifier.append(np.squeeze(submodels[-2].predict(x)))\n",
    "        outputs.append(np.squeeze(submodels[-1].predict(x)))\n",
    "inputs_to_network = np.array(inputs_to_network)\n",
    "inputs_to_classifier = np.array(inputs_to_classifier)\n",
    "outputs = np.array(outputs)\n",
    "scaled_inputs_to_network = StandardScaler().fit_transform(inputs_to_network)\n",
    "scaled_inputs_to_classifier = StandardScaler().fit_transform(inputs_to_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'kr'\n",
    "fig,ax = plt.subplots(2, 1, figsize=(10,5), sharex=False)\n",
    "j = 10\n",
    "for i in range(2):\n",
    "    ax[0].plot(inputs_to_network[i*N_blocks + j], col[i], lw=1)\n",
    "    ax[1].plot(inputs_to_classifier[i*N_blocks + j], col[i], lw=1, label=f'{outputs[i*N_blocks+j,0]:.2f}')\n",
    "ax[1].legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98de7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_to_classifier[i + j].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e0332",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_to_classifier[i*N_blocks + j].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df8b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_inputs_to_network = np.zeros(N_blocks)\n",
    "dst_inputs_to_classifier = np.zeros(N_blocks)\n",
    "for i in range(N_blocks):\n",
    "    alignment = dtw.dtw(inputs_to_network[i], inputs_to_network[i+N_blocks], keep_internals=False)\n",
    "    dst_inputs_to_network[i] = alignment.distance\n",
    "    alignment = dtw.dtw(inputs_to_classifier[i], inputs_to_classifier[i+N_blocks], keep_internals=False)\n",
    "    dst_inputs_to_classifier[i] = alignment.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1766d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(dst_inputs_to_network, [0.25,0.5,0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(dst_inputs_to_classifier, [0.25,0.5,0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "m = np.min([inputs_to_network[i].min(), inputs_to_network[i+N_blocks].min()])\n",
    "M = np.max([inputs_to_network[i].max(), inputs_to_network[i+N_blocks].max()])\n",
    "dst_inputs_to_network = np.sqrt((inputs_to_network[i] - inputs_to_network[i+N_blocks]) ** 2) / (M - m)\n",
    "m = np.min([inputs_to_classifier[i].min(), inputs_to_classifier[i+N_blocks].min()])\n",
    "M = np.max([inputs_to_classifier[i].max(), inputs_to_classifier[i+N_blocks].max()])\n",
    "dst_inputs_to_classifier = np.sqrt((inputs_to_classifier[i] - inputs_to_classifier[i+N_blocks]) ** 2) / (M - m)\n",
    "\n",
    "plt.plot(dst_inputs_to_network, 'k', lw=0.5)\n",
    "plt.plot(dst_inputs_to_classifier, 'r', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3122e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum((scaled_inputs_to_network[0] - scaled_inputs_to_network[N]) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum((scaled_inputs_to_classifier[0] - scaled_inputs_to_classifier[N]) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d023c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "pc_inputs_to_network = pca.fit_transform(scaled_inputs_to_network)\n",
    "pc_inputs_to_classifier = pca.fit_transform(scaled_inputs_to_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 2, figsize=(7,4))\n",
    "ax[0].plot(pc_inputs_to_network[:N,0], pc_inputs_to_network[:N,1], 'k.')\n",
    "ax[0].plot(pc_inputs_to_network[N:,0], pc_inputs_to_network[N:,1], 'r.')\n",
    "ax[1].plot(pc_inputs_to_classifier[:N,0], pc_inputs_to_classifier[:N,1], 'k.')\n",
    "ax[1].plot(pc_inputs_to_classifier[N:,0], pc_inputs_to_classifier[N:,1], 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_inputs_to_network = umap.UMAP().fit_transform(scaled_inputs_to_network)\n",
    "embedding_inputs_to_classifier = umap.UMAP().fit_transform(scaled_inputs_to_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4de92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(embedding_inputs_to_network[:N,0], embedding_inputs_to_network[:N,1],\n",
    "         'ko', ms=4, markerfacecolor='w')\n",
    "plt.plot(embedding_inputs_to_network[N:,0], embedding_inputs_to_network[N:,1],\n",
    "         'ks', ms=4)\n",
    "plt.plot(embedding_inputs_to_classifier[:N,0], embedding_inputs_to_classifier[:N,1],\n",
    "         'ro', ms=4, markerfacecolor='w')\n",
    "plt.plot(embedding_inputs_to_classifier[N:,0], embedding_inputs_to_classifier[N:,1],\n",
    "         'rs', ms=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf8180",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = outputs[0]\n",
    "rows = output[0].shape[2] + 1\n",
    "rows = 8\n",
    "fig,ax = plt.subplots(rows, 6, figsize=(16,rows), sharex=True)\n",
    "for j,key in enumerate(x.keys()):\n",
    "    for i,a in enumerate(ax[:,j]):\n",
    "        if i == 0:\n",
    "            a.plot(np.squeeze(x[key].numpy()), 'r', lw=1)\n",
    "        else:\n",
    "            a.plot(np.squeeze(output[j][0,:,i-1]), 'k', lw=1)\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])\n",
    "        for side in 'right','top':\n",
    "            a.spines[side].set_visible(False)\n",
    "        if i == rows:\n",
    "            break\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef2661",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = outputs[1]\n",
    "rows = output[0].shape[2] + 1\n",
    "rows = 8\n",
    "fig,ax = plt.subplots(rows, 6, figsize=(16,rows), sharex=False)\n",
    "for j,key in enumerate(x.keys()):\n",
    "    for i,a in enumerate(ax[:,j]):\n",
    "        if i == 0:\n",
    "            a.plot(np.squeeze(x[key].numpy()), 'r', lw=1)\n",
    "        else:\n",
    "            a.plot(np.squeeze(output[j][0,:,i-1]), 'k', lw=1)\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])\n",
    "        for side in 'right','top':\n",
    "            a.spines[side].set_visible(False)\n",
    "        if i == rows:\n",
    "            break\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94de17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = outputs[2]\n",
    "rows = output[0].shape[2] + 1\n",
    "rows = 8\n",
    "fig,ax = plt.subplots(rows, 6, figsize=(16,rows), sharex=False)\n",
    "for j,key in enumerate(x.keys()):\n",
    "    for i,a in enumerate(ax[:,j]):\n",
    "        if i == 0:\n",
    "            a.plot(np.squeeze(x[key].numpy()), 'r', lw=1)\n",
    "        else:\n",
    "            a.plot(np.squeeze(output[j][0,:,i-1]), 'k', lw=1)\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])\n",
    "        for side in 'right','top':\n",
    "            a.spines[side].set_visible(False)\n",
    "        if i == rows:\n",
    "            break\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1149c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = outputs[3]\n",
    "rows = output[0].shape[2] + 1\n",
    "rows = 8\n",
    "fig,ax = plt.subplots(rows, 6, figsize=(16,rows), sharex=False)\n",
    "for j,key in enumerate(x.keys()):\n",
    "    for i,a in enumerate(ax[:,j]):\n",
    "        if i == 0:\n",
    "            a.plot(np.squeeze(x[key].numpy()), 'r', lw=1)\n",
    "        else:\n",
    "            a.plot(np.squeeze(output[j][0,:,i-1]), 'k', lw=1)\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])\n",
    "        for side in 'right','top':\n",
    "            a.spines[side].set_visible(False)\n",
    "        if i == rows:\n",
    "            break\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a3c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = outputs[4]\n",
    "rows = output[0].shape[2] + 1\n",
    "rows = 8\n",
    "fig,ax = plt.subplots(rows, 6, figsize=(16,rows), sharex=False)\n",
    "for j,key in enumerate(x.keys()):\n",
    "    for i,a in enumerate(ax[:,j]):\n",
    "        if i == 0:\n",
    "            a.plot(np.squeeze(x[key].numpy()), 'r', lw=1)\n",
    "        else:\n",
    "            a.plot(np.squeeze(output[j][0,:,i-1]), 'k', lw=1)\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])\n",
    "        for side in 'right','top':\n",
    "            a.spines[side].set_visible(False)\n",
    "        if i == rows:\n",
    "            break\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2abb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = outputs[5]\n",
    "rows = output[0].shape[2] + 1\n",
    "rows = 8\n",
    "fig,ax = plt.subplots(rows, 6, figsize=(16,rows), sharex=False)\n",
    "for j,key in enumerate(x.keys()):\n",
    "    for i,a in enumerate(ax[:,j]):\n",
    "        if i == 0:\n",
    "            a.plot(np.squeeze(x[key].numpy()), 'r', lw=1)\n",
    "        else:\n",
    "            a.plot(np.squeeze(output[j][0,:,i-1]), 'k', lw=1)\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])\n",
    "        for side in 'right','top':\n",
    "            a.spines[side].set_visible(False)\n",
    "        if i == rows:\n",
    "            break\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc75f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 2, figsize=(10,3))\n",
    "for i,a in enumerate(ax):\n",
    "    a.plot(outputs[6+i][0], 'k', lw=1)\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "    for side in 'right','top':\n",
    "        a.spines[side].set_visible(False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 2, figsize=(10,3))\n",
    "for i,a in enumerate(ax):\n",
    "    a.plot(outputs[6+i][0], 'k', lw=1)\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "    for side in 'right','top':\n",
    "        a.spines[side].set_visible(False)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
