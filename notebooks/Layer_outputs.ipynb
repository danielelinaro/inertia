{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.signal import butter, filtfilt, lombscargle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from dlml.utils import collect_experiments\n",
    "from dlml.data import read_area_values, load_data_areas, load_data_files\n",
    "from dlml.nn import predict, DownSampling1D, SpectralPooling, MaxPooling1DWithArgmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf57d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_output_hist(Y, group_index, N_bins, cols=8, w=2, h=1.5, cmap=None, ax=None, labels=None):\n",
    "    N_trials, N_samples, N_filters = Y.shape\n",
    "    N_groups = len(group_index)\n",
    "    N = np.zeros((N_filters, N_groups, N_bins))\n",
    "    edges = np.zeros((N_filters, N_groups, N_bins+1))\n",
    "    for i in range(N_filters):\n",
    "        for j,jdx in enumerate(group_index):\n",
    "            N[i,j,:],edges[i,j,:] = np.histogram(Y[jdx, :, i], N_bins)\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('tab10', N_groups)\n",
    "    if ax is None:\n",
    "        rows = N_filters // cols\n",
    "        fig,ax = plt.subplots(rows, cols, figsize=(w*cols, h*rows))\n",
    "    else:\n",
    "        fig = None\n",
    "        N_filters = ax.size\n",
    "    ax = ax.flatten()\n",
    "    for i in range(N_filters):\n",
    "        for j in range(N_groups):\n",
    "            de = np.diff(edges[i, j, :])[0]\n",
    "            col = np.max([[0,0,0], cmap(j)[:3] - 1/3 * np.ones(3)], axis=0)\n",
    "            ax[i].bar(edges[i, j, :-1], N[i, j, :], width=de*0.8, align='edge',\n",
    "                     facecolor=cmap(j), edgecolor=col, linewidth=0.5, alpha=0.75)\n",
    "        xlim = [edges[i, :, 2:-3].min(), edges[i, j, 2:-3].max()]\n",
    "        ylim = ax[i].get_ylim()\n",
    "#         ax[i].set_xlim(xlim)\n",
    "#         ax[i].set_xticks(xlim)\n",
    "        if labels is not None:\n",
    "            ax[i].text(xlim[0], ylim[1], labels[i], fontsize=7, verticalalignment='top',\n",
    "                      horizontalalignment='left')\n",
    "        ax[i].set_xticklabels([])\n",
    "        ax[i].set_yticks(ax[i].get_ylim())\n",
    "        ax[i].set_yticklabels([])\n",
    "        for side in 'right','top':\n",
    "            ax[i].spines[side].set_visible(False)\n",
    "    if fig is not None:\n",
    "        fig.tight_layout()\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_output_ci(Y, group_index, cols=8, w=2, h=1.5, cmap=None, ax=None):\n",
    "    N_trials, N_samples, N_filters = Y.shape\n",
    "    N_groups = len(group_index)\n",
    "    mean = np.array([[Y[idx, :, i].mean(axis=0) for idx in group_index] for i in range(N_filters)])\n",
    "    ci = np.array([[1.96 * Y[idx, :, i].std(axis=0) / np.sqrt(idx.size) \\\n",
    "                    for idx in group_index] for i in range(N_filters)])\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('tab10', N_groups)\n",
    "    if ax is None:\n",
    "        rows = N_filters // cols\n",
    "        fig,ax = plt.subplots(rows, cols, figsize=(w*cols, h*rows), sharex=True)\n",
    "    else:\n",
    "        fig = None\n",
    "        N_filters = ax.size\n",
    "    ax = ax.flatten()\n",
    "    for i in range(N_filters):\n",
    "        for j in range(N_groups):\n",
    "            x = np.arange(mean.shape[-1])\n",
    "            ax[i].fill_between(x, mean[i, j, :] + ci[i, j, :], mean[i, j, :] - ci[i, j, :],\n",
    "                                 color=cmap(j), alpha=0.75)\n",
    "            ax[i].set_xticks(x[[0,-1]])\n",
    "            ax[i].set_xticklabels(x[[0,-1]]+1)\n",
    "            ax[i].set_yticks(ax[i].get_ylim())\n",
    "            ax[i].set_yticklabels([])\n",
    "            ax[i].grid(which='major', axis='y', ls=':', lw=0.5, color=[.6,.6,.6])\n",
    "            for side in 'right','top':\n",
    "                ax[i].spines[side].set_visible(False)\n",
    "    if fig is not None:\n",
    "        fig.tight_layout()\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36871b",
   "metadata": {},
   "source": [
    "#### Find the best experiment given a set of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b90255",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_ID = 1\n",
    "area_measure = 'momentum'\n",
    "stoch_load_bus_IDs = []\n",
    "rec_bus_IDs = [3]\n",
    "H_G1, D, DZA = None, None, None # 500, 2, 0\n",
    "additional_tags = ['ReLU_none', 'converted_from_PowerFactory', 'all_stoch_loads', 'data_subset']\n",
    "missing_tags = []\n",
    "use_FFT = False\n",
    "if use_FFT:\n",
    "    additional_tags.append('fft')\n",
    "else:\n",
    "    missing_tags.append('fft')\n",
    "pooling_type = 'argmax'\n",
    "if pooling_type is not None and pooling_type != '':\n",
    "    additional_tags.append(pooling_type + '_pooling')\n",
    "\n",
    "# training on frequency data, 2 output values\n",
    "# experiment_ID = '9ea493c789b542bf979c51a6031f4044'\n",
    "# training on frequency data, 4 output values\n",
    "# experiment_ID = 'f6d9a03f1cfe450288e9cb86da94235f'\n",
    "# training on time series data, 2 output values\n",
    "experiment_ID = '034a1edb0797475b985f0e1335dab383'\n",
    "# training on time series data, 4 output values\n",
    "# experiment_ID = 'b346a89d384c4db2ba4058a2c83c4f12'\n",
    "\n",
    "# training on time series data, 2 output values, with MaxPooling1DWithArgmax layer\n",
    "# experiment_ID = '9034f8bc4f874c938dfa5f1f9ee04e82'\n",
    "\n",
    "if experiment_ID is not None:\n",
    "    from comet_ml.api import API, APIExperiment\n",
    "    api = API(api_key = os.environ['COMET_API_KEY'])\n",
    "    experiment = api.get_experiment('danielelinaro', 'inertia', experiment_ID)\n",
    "    sys.stdout.write(f'Getting metrics for experiment {experiment_ID[:6]}... ')\n",
    "    sys.stdout.flush()\n",
    "    metrics = experiment.get_metrics()\n",
    "    sys.stdout.write('done.\\n')\n",
    "    val_loss = []\n",
    "    for m in metrics:\n",
    "        if m['metricName'] == 'val_loss':\n",
    "            val_loss.append(float(m['metricValue']))\n",
    "        elif m['metricName'] == 'mape_prediction':\n",
    "            MAPE = float(m['metricValue'])\n",
    "    val_loss = np.array(val_loss)\n",
    "else:\n",
    "    # find the best experiment that matches the set of tags above\n",
    "    experiments = collect_experiments(area_ID, area_measure=area_measure, D=D, DZA=DZA, \\\n",
    "                                      stoch_load_bus_IDs=stoch_load_bus_IDs, H_G1=H_G1, \\\n",
    "                                      rec_bus_IDs=rec_bus_IDs, additional_tags=additional_tags, \\\n",
    "                                      missing_tags=missing_tags, verbose=True)\n",
    "    experiment_IDs = list(experiments.keys())\n",
    "    experiment_ID = experiment_IDs[np.argmin([expt['val_loss'].min() for expt in experiments.values()])]\n",
    "    experiment_ID = experiment_IDs[np.argmin([expt['MAPE'] for expt in experiments.values()])]\n",
    "\n",
    "    MAPE = experiments[experiment_ID]['MAPE']\n",
    "    loss = experiments[experiment_ID]['loss']\n",
    "    val_loss = experiments[experiment_ID]['val_loss']\n",
    "    batch_loss = experiments[experiment_ID]['batch_loss']\n",
    "    tags = experiments[experiment_ID]['tags']\n",
    "print(f'Selected experiment is {experiment_ID[:6]} (val_loss = {val_loss.min():.4f}, MAPE = {MAPE:.4f}%).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0a7fe",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81320651",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_path = '../experiments/neural_network/'\n",
    "network_parameters = pickle.load(open(os.path.join(experiments_path, experiment_ID, 'parameters.pkl'), 'rb'))\n",
    "checkpoint_path = experiments_path + experiment_ID + '/checkpoints/'\n",
    "checkpoint_files = glob.glob(checkpoint_path + '*.h5')\n",
    "try:\n",
    "    epochs = [int(os.path.split(file)[-1].split('.')[1].split('-')[0]) for file in checkpoint_files]\n",
    "    best_checkpoint = checkpoint_files[epochs.index(np.argmin(val_loss) + 1)]\n",
    "except:\n",
    "    best_checkpoint = checkpoint_files[-1]\n",
    "try:\n",
    "    model = keras.models.load_model(best_checkpoint)\n",
    "except:\n",
    "    if pooling_type == 'downsample':\n",
    "        custom_objects = {'DownSampling1D': DownSampling1D}\n",
    "    elif pooling_type == 'spectral':\n",
    "        custom_objects = {'SpectralPooling': SpectralPooling}\n",
    "    elif pooling_type == 'argmax':\n",
    "        custom_objects = {'MaxPooling1DWithArgmax': MaxPooling1DWithArgmax}\n",
    "    with keras.utils.custom_object_scope(custom_objects):\n",
    "        model = keras.models.load_model(best_checkpoint)\n",
    "if pooling_type == 'argmax':\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, MaxPooling1DWithArgmax):\n",
    "            print(f'Setting store_argmax = True for layer \"{layer.name}\".')\n",
    "            layer.store_argmax = True\n",
    "x_train_mean = network_parameters['x_train_mean']\n",
    "x_train_std  = network_parameters['x_train_std']\n",
    "try:\n",
    "    x_train_min = network_parameters['x_train_min']\n",
    "    x_train_max = network_parameters['x_train_max']\n",
    "except:\n",
    "    pass\n",
    "var_names = network_parameters['var_names']\n",
    "print(f'Loaded network from {best_checkpoint}.')\n",
    "print(f'Variable names: {var_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e540f",
   "metadata": {},
   "source": [
    "#### Plot the model topology"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f804eafc",
   "metadata": {},
   "source": [
    "keras.utils.plot_model(model, show_shapes=False, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f8316",
   "metadata": {},
   "source": [
    "#### Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_fft = network_parameters['use_fft'] if 'use_fft' in network_parameters else False\n",
    "if use_fft:\n",
    "    raise Exception('This notebook must be used on a network that uses time-domain inputs')\n",
    "\n",
    "use_test_set = True\n",
    "with_compensators = False if use_test_set else True\n",
    "set_name = 'test'\n",
    "\n",
    "if use_test_set:\n",
    "    data_dir = os.path.join('..', network_parameters['data_dirs'][0].format(network_parameters['area_IDs'][0]))\n",
    "    data_files = sorted(glob.glob(data_dir + os.path.sep + f'*_{set_name}_set.h5'))\n",
    "    ret = load_data_areas({set_name: data_files}, network_parameters['var_names'],\n",
    "                            network_parameters['generators_areas_map'][:1],\n",
    "                            network_parameters['generators_Pnom'],\n",
    "                            network_parameters['area_measure'],\n",
    "                            trial_dur=network_parameters['trial_duration'],\n",
    "                            max_block_size=500,\n",
    "                            use_tf=False, add_omega_ref=True,\n",
    "                            use_fft=False)\n",
    "    t = ret[0]\n",
    "    X_raw = ret[1][set_name]\n",
    "    y = ret[2][set_name]\n",
    "    group_index = [np.where(y == mom)[0] for mom in np.unique(y)]\n",
    "\n",
    "else:\n",
    "    base_folder = network_parameters['data_dirs'][0]\n",
    "    generators_areas_map = network_parameters['generators_areas_map'].copy()\n",
    "    generators_areas_map[0].append('Comp11')\n",
    "    generators_Pnom = network_parameters['generators_Pnom'].copy()\n",
    "    generators_Pnom['Comp11'] = 100e6\n",
    "    while '{}' in base_folder:\n",
    "        base_folder,_ = os.path.split(base_folder)\n",
    "    data_files = []\n",
    "    group_index = []\n",
    "    for i,prefix in enumerate(('low','high')):\n",
    "        folder = os.path.join('..', base_folder, prefix + '_momentum_' + set_name)\n",
    "        if with_compensators and os.path.isdir(folder + '_comp'):\n",
    "            folder += '_comp'\n",
    "        print(f'Loading data from {folder}...')\n",
    "        files = sorted(glob.glob(folder + os.path.sep + '*.h5'))\n",
    "        group_index.append(np.arange(len(files)) + len(data_files))\n",
    "        data_files += files\n",
    "\n",
    "    ret = load_data_files(data_files,\n",
    "                          network_parameters['var_names'],\n",
    "                          generators_areas_map[:1],\n",
    "                          generators_Pnom,\n",
    "                          'momentum')\n",
    "\n",
    "    t = ret[0][:-1]\n",
    "    X_raw = ret[1][:, :, :-1]\n",
    "    y = ret[2]\n",
    "\n",
    "X = np.zeros(X_raw.shape)\n",
    "for i,(m,s) in enumerate(zip(x_train_mean, x_train_std)):\n",
    "    X[i,:,:] = (X_raw[i,:,:] - m) / s\n",
    "X_raw = X_raw.squeeze()\n",
    "X = X.squeeze()\n",
    "y = y.squeeze()\n",
    "dt = np.diff(t[:2])[0]\n",
    "N_samples = t.size\n",
    "Xf = fft(X)\n",
    "Xf = 2.0 / N_samples * np.abs(Xf[:, :N_samples//2])\n",
    "F = fftfreq(N_samples, dt)[:N_samples//2]\n",
    "n_mom_groups = len(group_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f46974",
   "metadata": {},
   "source": [
    "#### Predict the momentum using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "momentum = [y_pred[idx] for idx in group_index]\n",
    "mean_momentum = [m.mean() for m in momentum]\n",
    "stddev_momentum = [m.std() for m in momentum]\n",
    "print('Mean momentum:', mean_momentum)\n",
    "print(' Std momentum:', stddev_momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4a70d",
   "metadata": {},
   "source": [
    "### Plot the inputs and their FFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d827d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('tab10', n_mom_groups)\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "gs = GridSpec(1, 3, figure=fig, width_ratios=[2, 1, 3])\n",
    "ax = [fig.add_subplot(gs[0,0]), fig.add_subplot(gs[0,1]), fig.add_subplot(gs[0,2])]\n",
    "\n",
    "for j,jdx in enumerate(group_index):\n",
    "    for i in range(3):\n",
    "        ax[0].plot(t, X[jdx[i]], color=cmap(j), lw=1, alpha=0.8)\n",
    "    edges = np.linspace(-4, 4, 41)\n",
    "    n,edges = np.histogram(X[jdx], bins=edges, density=True)\n",
    "    de = np.diff(edges)[0]\n",
    "    ax[1].barh(edges[1:], n, height=de*0.8, facecolor=cmap(j), align='edge', alpha=0.8)\n",
    "    mean = Xf[jdx].mean(axis=0)\n",
    "    stddev = Xf[jdx].std(axis=0)\n",
    "    ci = 1.96 * stddev / np.sqrt(jdx.size)\n",
    "    ylim = [0, np.max((mean + ci)[F > 0.1]) * 1.1]\n",
    "    ax[2].fill_between(F, mean + ci, mean - ci, color=cmap(j),\n",
    "                       label=r'M = {:.3f} GW$\\cdot s^2$'.format(y[jdx].mean()))\n",
    "for a in ax:\n",
    "    for side in 'right','top':\n",
    "        a.spines[side].set_visible(False)\n",
    "    a.grid(which='major', axis='both', ls=':', lw=0.5, color=[.6,.6,.6])\n",
    "ax[0].set_xlabel('Time [s]')\n",
    "ax[0].set_ylabel(f'Normalized {var_names[0]}')\n",
    "ax[1].set_ylim(ax[0].get_ylim())\n",
    "ax[1].set_yticklabels([])\n",
    "ax[1].set_xlabel('PDF')\n",
    "ax[2].set_xscale('log')\n",
    "ax[2].set_xlabel('Frequency [Hz]')\n",
    "ax[2].set_ylabel('Power')\n",
    "ax[2].set_ylim(ylim)\n",
    "ax[2].legend(loc='upper right')\n",
    "fig.tight_layout(pad=0)\n",
    "fig.savefig(f'spectra_{n_mom_groups}_momentum_levels.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74371b1c",
   "metadata": {},
   "source": [
    "#### Make a model with as many outputs as there are convolutional and pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30332534",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "while 'conv' in model.layers[i].name or 'pool' in model.layers[i].name:\n",
    "    i += 1\n",
    "multi_output_model = keras.Model(inputs=model.inputs,\n",
    "                                 outputs=[layer.output for layer in model.layers[1:i]])\n",
    "multi_output = multi_output_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a0f94",
   "metadata": {},
   "source": [
    "#### Load the correlations files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_bands = 40\n",
    "sort_F = 1.0\n",
    "IDX = []\n",
    "for layer in multi_output_model.layers[1:]:\n",
    "    _, N_neurons, N_filters = layer.output.shape\n",
    "    correlations_file = f'correlations_{experiment_ID[:6]}_{N_bands}-bands_{N_filters}-filters' + \\\n",
    "        f'_{N_neurons}-neurons_1000-trials_8-butter_{layer.name}.npz'\n",
    "    print(f'Loading {correlations_file}...')\n",
    "    corr = np.load(os.path.join('..', 'experiments', 'neural_network', experiment_ID, correlations_file))\n",
    "    R = corr['R']\n",
    "    idx = corr['idx']\n",
    "    edges = corr['edges']\n",
    "    R_mean = [R[jdx].mean(axis=0) for jdx in idx]\n",
    "    edge = np.abs(corr['edges'] - sort_F).argmin()\n",
    "    IDX.append([np.argsort(rm[edge,:]) for rm in R_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b60123",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = 2, 1\n",
    "rows, cols = len(multi_output), 5\n",
    "fig,ax = plt.subplots(rows, cols, figsize=(cols*w, rows*h))\n",
    "for i in range(rows):\n",
    "    dst = np.abs(np.diff(np.array([multi_output[i][idx,:,:].mean(axis=(0,1)) \\\n",
    "                                   for idx in group_index]), axis=0)).squeeze()\n",
    "    idx = np.argsort(dst)[::-1]\n",
    "    tmp = multi_output[i][:, :, idx]\n",
    "    _,_ = plot_layer_output_hist(tmp, group_index, N_bins=16, ax=ax[i], labels=[f'#{j}' for j in idx])\n",
    "    ss = model.layers[i+1].name.split('_')\n",
    "    lbl = '_'.join([ss[2], ss[4]])\n",
    "    ax[i,0].set_ylabel(lbl)\n",
    "fig.tight_layout(pad=0.5)\n",
    "fig.savefig(f'layer_outputs_distr_{experiment_ID}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33077f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = 2, 1\n",
    "rows, cols = len(multi_output), 5\n",
    "fig,ax = plt.subplots(rows, cols, figsize=(cols*w, rows*h))\n",
    "for i in range(rows):\n",
    "    _,_ = plot_layer_output_ci(multi_output[i], group_index, ax=ax[i])\n",
    "    ss = model.layers[i+1].name.split('_')\n",
    "    lbl = '_'.join([ss[2], ss[4]])\n",
    "    ax[i,0].set_ylabel(lbl)\n",
    "for j in range(cols):\n",
    "    ax[-1,j].set_xlabel('Sample #')\n",
    "fig.tight_layout(pad=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
