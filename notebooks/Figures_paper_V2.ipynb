{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import shelve\n",
    "import numpy as np\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.optimize import curve_fit\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib.colors import LogNorm, FuncNorm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.ticker import FixedLocator, NullLocator, FixedFormatter\n",
    "from matplotlib.patches import Polygon\n",
    "import seaborn as sns\n",
    "\n",
    "fontsize = 7\n",
    "lw = 0.75\n",
    "COL_WIDTH = {1: 88/25.4, 2: 180/25.4} # column width in inches\n",
    "matplotlib.rc('font', **{'family': 'Arial', 'size': fontsize})\n",
    "matplotlib.rc('axes', **{'linewidth': 0.75, 'labelsize': fontsize})\n",
    "matplotlib.rc('xtick', **{'labelsize': fontsize})\n",
    "matplotlib.rc('ytick', **{'labelsize': fontsize})\n",
    "matplotlib.rc('xtick.major', **{'width': lw, 'size':3})\n",
    "matplotlib.rc('ytick.major', **{'width': lw, 'size':3})\n",
    "matplotlib.rc('ytick.minor', **{'width': lw, 'size':1.5})\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from dlml.data import load_data_files, load_data_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_axes(rows, cols, x_offset, y_offset, x_space, y_space, squeeze=True):\n",
    "    w = (1 - np.sum(x_offset) - x_space * (cols - 1)) / cols\n",
    "    h = (1 - np.sum(y_offset) - y_space * (rows - 1)) / rows\n",
    "    \n",
    "    ax = [[plt.axes([x_offset[0] + (w + x_space) * j,\n",
    "                     y_offset[0] + (h + y_space) * i,\n",
    "                     w, h]) for j in range(cols)] for i in range(rows-1, -1, -1)]\n",
    "    \n",
    "    for row in ax:\n",
    "        for a in row:\n",
    "            for side in 'right','top':\n",
    "                a.spines[side].set_visible(False)\n",
    "\n",
    "    if squeeze:\n",
    "        if rows == 1 and cols == 1:\n",
    "            return ax[0][0]\n",
    "        if rows == 1:\n",
    "            return ax[0]\n",
    "        if cols == 1:\n",
    "            return [a[0] for a in ax]\n",
    "        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlations(R, p, R_ctrl, p_ctrl, edges, idx, ax, sort_freq=1.0,\n",
    "                      vmin=None, vmax=None, legend_bbox=[0.4, -0.05]):\n",
    "    if p is not None:\n",
    "        R = R.copy()\n",
    "        R[p > 0.05] = np.nan\n",
    "    if R_ctrl is not None and p_ctrl is not None:\n",
    "        R_ctrl = R_ctrl.copy()\n",
    "        R_ctrl[p_ctrl > 0.05] = np.nan\n",
    "    rows, cols = ax.shape\n",
    "    if rows != len(idx):\n",
    "        raise Exception('Number of rows of ax does not match len(idx)')\n",
    "    R_mean = [np.nanmean(R[jdx], axis=0) for jdx in idx]\n",
    "    R_abs_mean = [np.mean(np.abs(r), axis=1) for r in R_mean]\n",
    "    if R_ctrl is not None:\n",
    "        R_ctrl_mean = [np.nanmean(R_ctrl[jdx], axis=0) for jdx in idx]\n",
    "        R_ctrl_abs_mean = [np.mean(np.abs(r), axis=1) for r in R_ctrl_mean]\n",
    "    else:\n",
    "        R_ctrl_mean = [None for _ in range(rows)]\n",
    "        R_ctrl_abs_mean = None\n",
    "    if np.isscalar(sort_freq):\n",
    "        sort_freq += np.zeros(rows)\n",
    "    edge = np.array([np.abs(edges - freq).argmin() for freq in sort_freq])\n",
    "    for i in range(rows):\n",
    "        kdx = np.argsort(R_mean[i][edge[i],:])\n",
    "        R_mean[i] = R_mean[i][:,kdx]\n",
    "        if R_ctrl is not None:\n",
    "            R_ctrl_mean[i] = R_ctrl_mean[i][:,kdx]\n",
    "\n",
    "    make_symmetric = False\n",
    "    if vmin is None:\n",
    "        vmin = min([r.min() for r in R_mean])\n",
    "        make_symmetric = True\n",
    "    if vmax is None:\n",
    "        vmax = max([r.max() for r in R_mean])\n",
    "        if make_symmetric:\n",
    "            if vmax > np.abs(vmin):\n",
    "                vmin = -vmax\n",
    "            else:\n",
    "                vmax = -vmin\n",
    "    print(f'Color bar bounds: ({vmin:.2f},{vmax:.2f}).')\n",
    "    ticks = np.linspace(vmin, vmax, 7)\n",
    "    ticklabels = [f'{tick:.2f}' for tick in ticks]\n",
    "\n",
    "    cmap = plt.get_cmap('bwr')\n",
    "    y = edges[:-1] + np.diff(edges) / 2\n",
    "    dfs = []\n",
    "    for i in range(rows):\n",
    "        for j,R in enumerate((R_mean[i], R_ctrl_mean[i])):\n",
    "            if R is not None:\n",
    "                x = np.arange(R.shape[-1])\n",
    "                im = ax[i][j].pcolormesh(x, y, R, vmin=vmin, vmax=vmax, shading='auto', cmap=cmap)\n",
    "                ax[i][j].set_xticks(np.linspace(0, x[-1], 3, dtype=np.int32))\n",
    "                X,Y = np.meshgrid(x,y)\n",
    "                dfs.append(pd.DataFrame(data={'Filter': X.flatten(),\n",
    "                                              'Frequency': Y.flatten(),\n",
    "                                              'R': R.flatten()}))\n",
    "        if cols > 1:\n",
    "            cbar = plt.colorbar(im, fraction=0.1, shrink=1, aspect=20, label='Correlation',\n",
    "                                orientation='vertical', ax=ax[i][1], ticks=ticks)\n",
    "            cbar.ax.set_yticklabels(ticklabels, fontsize=fontsize-1)\n",
    "            if R_ctrl_abs_mean is not None:\n",
    "                ax[i][-1].plot(R_abs_mean[i], y, 'r', lw=1, label='train.')\n",
    "                ax[i][-1].plot(R_ctrl_abs_mean[i], y, 'g--', lw=1, label='untrain.')\n",
    "                ax[i][-1].plot(R_abs_mean[i] - R_ctrl_abs_mean[i], y, 'k', lw=1, label='diff.')\n",
    "                ax[i][-1].legend(loc='lower left', bbox_to_anchor=legend_bbox,\n",
    "                                 frameon=False, fontsize=fontsize-1)\n",
    "                dfs.append(pd.DataFrame(data={'Frequency': y,\n",
    "                                              'R_trained': R_abs_mean[i],\n",
    "                                              'R_untrained': R_ctrl_abs_mean[i],\n",
    "                                              'Diff': R_abs_mean[i] - R_ctrl_abs_mean[i]}))\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            ax[i][j].set_ylim(edges[[0,-2]])\n",
    "            ax[i][j].set_yscale('log')\n",
    "\n",
    "    sns.despine()\n",
    "    return vmin, vmax, dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_output_hist(Y, group_index, N_bins, cols=8, w=2, h=1.5, cmap=None, ax=None, labels=None):\n",
    "    N_trials, N_samples, N_filters = Y.shape\n",
    "    N_groups = len(group_index)\n",
    "    N = np.zeros((N_filters, N_groups, N_bins))\n",
    "    edges = np.zeros((N_filters, N_groups, N_bins+1))\n",
    "    for i in range(N_filters):\n",
    "        for j,jdx in enumerate(group_index):\n",
    "            N[i,j,:],edges[i,j,:] = np.histogram(Y[jdx, :, i], N_bins)\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('tab10', N_groups)\n",
    "    if ax is None:\n",
    "        rows = N_filters // cols\n",
    "        fig,ax = plt.subplots(rows, cols, figsize=(w*cols, h*rows))\n",
    "    else:\n",
    "        fig = None\n",
    "        N_filters = ax.size\n",
    "    ax = ax.flatten()\n",
    "    for i in range(N_filters):\n",
    "        for j in range(N_groups):\n",
    "            de = np.diff(edges[i, j, :])[0]\n",
    "            col = np.max([[0,0,0], cmap(j)[:3] - 1/3 * np.ones(3)], axis=0)\n",
    "            ax[i].bar(edges[i, j, :-1], N[i, j, :], width=de*0.8, align='edge',\n",
    "                     facecolor=cmap(j), edgecolor=col, linewidth=0.5, alpha=0.85)\n",
    "        xlim = [edges[i, :, 2:-3].min(), edges[i, j, 2:-3].max()]\n",
    "        ylim = ax[i].get_ylim()\n",
    "        if labels is not None:\n",
    "            ax[i].text(xlim[0] - 0.1 * np.diff(xlim), ylim[1],\n",
    "                       labels[i], fontsize=fontsize-1, verticalalignment='top',\n",
    "                       horizontalalignment='left')\n",
    "        ax[i].set_xticklabels([])\n",
    "        ax[i].set_yticks(ax[i].get_ylim())\n",
    "        ax[i].set_yticklabels([])\n",
    "        for side in 'right','top':\n",
    "            ax[i].spines[side].set_visible(False)\n",
    "    if fig is not None:\n",
    "        fig.tight_layout()\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dfs_dict(keys):\n",
    "    return {key: pd.DataFrame() for key in keys}\n",
    "\n",
    "def save_dfs_dict(dfs, xls_file, index=False):\n",
    "    if not isinstance(index, list):\n",
    "        index = [index for _ in range(len(dfs))]\n",
    "    with pd.ExcelWriter(xls_file) as writer:\n",
    "        for (name,df),idx in zip(dfs.items(), index):\n",
    "            df.to_excel(writer, sheet_name='Panel_'+name, header=True, index=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, Xf = {}, {}, {}\n",
    "group_index, n_mom_groups = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = 'Vd_bus3'\n",
    "area_ID = 1\n",
    "if area_ID == 1:\n",
    "    subset = 8\n",
    "elif area_ID == 2:\n",
    "    subset = 2\n",
    "else:\n",
    "    raise Exception(f'Unknown area ID: {area_ID}.')\n",
    "var_names, area_IDs = [var_name], [area_ID]\n",
    "data_file = f'traces_hist_spectra_comp_grid_area_{area_ID}_{var_name}.npz'\n",
    "force = False\n",
    "set_name = 'training'\n",
    "if not os.path.isfile(data_file) or force:\n",
    "\n",
    "    data_dir = '../data/IEEE39/converted_from_PowerFactory/all_stoch_loads/' + \\\n",
    "        f'var_H_area_{area_ID}_comp_grid/coarse_H_comp{area_ID}1_0.1/diagonal'\n",
    "    data_files = sorted(glob.glob(data_dir + os.path.sep + f'*_{set_name}_set.h5'))\n",
    "    generators_areas_map = [['G02', 'G03', 'Comp11'],\n",
    "                            ['G04', 'G05', 'G06', 'G07', 'Comp21'],\n",
    "                            ['G08', 'G09', 'G10', 'Comp31'],\n",
    "                            ['G01']]\n",
    "    generators_Pnom = {'G01': 10e9, 'G02': 700e6, 'G03': 800e6, 'G04': 800e6, 'G05': 300e6,\n",
    "                       'G06': 800e6, 'G07': 700e6, 'G08': 700e6, 'G09': 1000e6, 'G10': 1000e6,\n",
    "                       'Comp11': 100e6, 'Comp21': 100e6, 'Comp31': 100e6}\n",
    "    area_measure = 'momentum'\n",
    "    ret = load_data_areas({set_name: data_files}, var_names,\n",
    "                          [generators_areas_map[i-1] for i in area_IDs],\n",
    "                          generators_Pnom,\n",
    "                          area_measure,\n",
    "                          trial_dur=60,\n",
    "                          max_block_size=1000,\n",
    "                          use_tf=False,\n",
    "                          add_omega_ref=True,\n",
    "                          use_fft=False)\n",
    "    t = ret[0]\n",
    "    X_raw = ret[1][set_name]\n",
    "    y[set_name] = ret[2][set_name]\n",
    "    group_index[set_name] = [np.where(y[set_name] == mom)[0] for mom in np.unique(y[set_name])]\n",
    "    n_mom_groups[set_name] = len(group_index[set_name])\n",
    "    X_mean, X_std = X_raw.mean(axis=(1,2)), X_raw.std(axis=(1,2))\n",
    "    X[set_name] = (X_raw - X_mean) / X_std\n",
    "    X[set_name] = X[set_name].squeeze()\n",
    "    y[set_name] = y[set_name].squeeze()\n",
    "\n",
    "    dt = np.diff(t[:2])[0]\n",
    "    N_samples = t.size\n",
    "    Xf[set_name] = fft(X[set_name])\n",
    "    Xf[set_name] = 2.0 / N_samples * np.abs(Xf[set_name][:, :N_samples//2])\n",
    "    F = fftfreq(N_samples, dt)[:N_samples//2]\n",
    "\n",
    "    data_dir = '../data/IEEE39/converted_from_PowerFactory/all_stoch_loads/' + \\\n",
    "        f'var_H_area_{area_ID}_comp_grid/subset_{subset}_H_comp{area_ID}1_0.1'\n",
    "    data_files = sorted(glob.glob(data_dir + os.path.sep + f'*_{set_name}_set.h5'))\n",
    "    ret = load_data_areas({set_name: data_files}, var_names,\n",
    "                          [generators_areas_map[i-1] for i in area_IDs],\n",
    "                          generators_Pnom,\n",
    "                          area_measure,\n",
    "                          trial_dur=60,\n",
    "                          max_block_size=1000,\n",
    "                          use_tf=False,\n",
    "                          add_omega_ref=True,\n",
    "                          use_fft=False)    \n",
    "    X_raw = ret[1][set_name]\n",
    "    y['var_G2_G3'] = ret[2][set_name]\n",
    "    group_index['var_G2_G3'] = [np.where(y['var_G2_G3'] == mom)[0] for mom in np.unique(y['var_G2_G3'])]\n",
    "    n_mom_groups['var_G2_G3'] = len(group_index['var_G2_G3'])\n",
    "    X['var_G2_G3'] = (X_raw - X_mean) / X_std\n",
    "    X['var_G2_G3'] = X['var_G2_G3'].squeeze()\n",
    "    y['var_G2_G3'] = y['var_G2_G3'].squeeze()\n",
    "    Xf['var_G2_G3'] = fft(X['var_G2_G3'])\n",
    "    Xf['var_G2_G3'] = 2.0 / N_samples * np.abs(Xf['var_G2_G3'][:, :N_samples//2])\n",
    "\n",
    "    np.savez_compressed(data_file, t=t, X=X, y=y, Xf=Xf, F=F,\n",
    "                        group_index=group_index, n_mom_groups=n_mom_groups)\n",
    "else:\n",
    "    data = np.load(data_file, allow_pickle=True)\n",
    "    t = data['t']\n",
    "    X = data['X'].item()\n",
    "    y = data['y'].item()\n",
    "    F = data['F']\n",
    "    Xf = data['Xf'].item()\n",
    "    group_index = data['group_index'].item()\n",
    "    n_mom_groups = data['n_mom_groups'].item()\n",
    "    \n",
    "mom = np.array([y[set_name][idx].mean() for idx in group_index[set_name]])\n",
    "std = np.array([X[set_name][idx,:].std() for idx in group_index[set_name]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplot_mosaic(\n",
    "    '''\n",
    "    aaaaaaaa\n",
    "    bbbbbccc\n",
    "    ddddeeee\n",
    "    ''',\n",
    "    figsize=(COL_WIDTH[1], 4)\n",
    ")\n",
    "cmap = plt.get_cmap('Set1')\n",
    "N_div_cmap = 10\n",
    "div_cmap = plt.get_cmap('bwr', N_div_cmap)\n",
    "\n",
    "###########################################\n",
    "momentum = lambda H, S, fn: 2 * H@S / fn * 1e-3\n",
    "\n",
    "N = 11, 11\n",
    "h_G2_0, h_G3_0 = 4.33, 4.47\n",
    "h_G2 = h_G2_0 + np.linspace(-1, 1, N[0])\n",
    "h_G3 = h_G3_0 + np.linspace(-1, 1, N[1])\n",
    "H_G2, H_G3 = np.meshgrid(*[h_G2, h_G3])\n",
    "\n",
    "M = np.zeros(N)\n",
    "S = np.array([700, 800])\n",
    "fn = 60\n",
    "for i in range(N[0]):\n",
    "    for j in range(N[1]):\n",
    "        H = np.array([H_G2[i,j], H_G3[i,j]])\n",
    "        M[i,j] = momentum(H, S, fn)\n",
    "        \n",
    "gray_cmap = plt.get_cmap('gray')\n",
    "cont = ax['a'].contourf(H_G2, H_G3, M, levels=100, cmap=gray_cmap, zorder=-1)\n",
    "cbar = plt.colorbar(cont, ax=ax['a'])\n",
    "white = [1,1,1]\n",
    "magenta = [1,0,1]\n",
    "green = [0,1,0]\n",
    "yellow = [1,1,0]\n",
    "blue = [0,.5,1]\n",
    "red = [1,.333,.333]\n",
    "orange = [1, .5, 0]\n",
    "gray = [.6, .6, .6]\n",
    "zord = 0\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax['a'].plot(H_G2[i,j], H_G3[i,j], 's', markersize=4, lw=1, color=div_cmap(i*2+j), zorder=zord)\n",
    "        ax['a'].plot(H_G2[-1-i,-1-j], H_G3[-1-i,-1-j], 's', markersize=4, lw=1,\n",
    "                     color=div_cmap(N_div_cmap-1-i*2-j), zorder=zord)\n",
    "        zord += 2\n",
    "for i in range(0, N[0], 2):\n",
    "    ax['a'].plot(H_G2[i,i], H_G3[i,i], 'o', color=cmap(i//2), markersize=3, lw=1, zorder=zord)\n",
    "    zord += 1\n",
    "ax['a'].plot(H_G2[:2,:2].mean(), H_G3[:2,:2].mean(), 'x', color=magenta, markersize=4,\n",
    "             markeredgewidth=1, zorder=zord)\n",
    "zord += 1\n",
    "ax['a'].plot(H_G2[-2:,-2:].mean(), H_G3[-2:,-2:].mean(), 'x', color=magenta, markersize=4,\n",
    "             markeredgewidth=1, zorder=zord)\n",
    "zord += 1\n",
    "ax['a'].scatter(H_G2[::2, ::2], H_G3[::2, ::2], s=5, c='w', edgecolors='k', lw=0.5, marker='o', zorder=zord)\n",
    "ax['a'].set_xlabel(r'$H_{G_2}$ [s]')\n",
    "ax['a'].set_ylabel(r'$H_{G_3}$ [s]')\n",
    "ax['a'].set_xlim([h_G2_0 - 1.1, h_G2_0 + 1.1])\n",
    "ax['a'].set_ylim([h_G3_0 - 1.1, h_G3_0 + 1.1])\n",
    "ax['a'].set_xticks([h_G2_0 - 1, h_G2_0, h_G2_0 + 1])\n",
    "ax['a'].set_yticks([h_G3_0 - 1, h_G3_0, h_G3_0 + 1])\n",
    "cbar.set_label(r'Momentum [GW$\\cdot$s$^2$]')\n",
    "cbar.set_ticks(np.r_[0.17 : 0.28 : 0.02])\n",
    "###########################################\n",
    "\n",
    "dfs = make_dfs_dict(('b','c','c-inset','d','e'))\n",
    "tend = 10\n",
    "jdx, = np.where(t < tend)\n",
    "dfs['b']['Time'] = t[jdx]\n",
    "dfs['d']['Frequency'] = F\n",
    "dfs['e']['Frequency'] = F\n",
    "for i,idx in enumerate(group_index[set_name]):\n",
    "    n,edges = np.histogram(X[set_name][idx,:], bins=50, density=True)\n",
    "    ax['b'].plot(t[jdx], X[set_name][idx[0]+1, jdx], lw=1, color=cmap(i))\n",
    "    key = 'M={:.4f}'.format(np.unique(y[set_name])[i])\n",
    "    dfs['b']['V_' + key] = X[set_name][idx[0]+1,jdx]\n",
    "    ax['c'].plot(n, edges[1:], lw=1, color=cmap(i))\n",
    "    dfs['c']['edges_' + key] = edges[1:]\n",
    "    dfs['c']['distr_' + key] = n\n",
    "    ax['d'].plot(F, 20 * np.log10(Xf[set_name][idx,:].mean(axis=0)), lw=1,\n",
    "               color=cmap(i), label=r'{:.3f} GW$\\cdot$s$^2$'.format(y[set_name][idx[0]+1]))\n",
    "    dfs['d']['Power_' + key] = 20 * np.log10(Xf[set_name][idx,:].mean(axis=0))\n",
    "\n",
    "for i,idx in enumerate(group_index['var_G2_G3']):\n",
    "    if i < 4:\n",
    "        ax['e'].plot(F, 20 * np.log10(Xf['var_G2_G3'][idx,:].mean(axis=0)), lw=1,\n",
    "                   color=div_cmap(i))\n",
    "    else:\n",
    "        ax['e'].plot(F, 20 * np.log10(Xf['var_G2_G3'][idx,:].mean(axis=0)), lw=1,\n",
    "                   color=div_cmap(N_div_cmap - (i - 3)))\n",
    "    key = 'M={:.4f}'.format(np.unique(y['var_G2_G3'])[i])\n",
    "    dfs['e']['Power_' + key] = 20 * np.log10(Xf['var_G2_G3'][idx,:].mean(axis=0))\n",
    "\n",
    "for key in 'bc':\n",
    "    ax[key].grid(which='major', axis='y', lw=0.5, ls=':', color=[.6,.6,.6])\n",
    "for key in 'de':\n",
    "    ax[key].grid(which='major', axis='x', lw=0.5, ls=':', color=[.6,.6,.6])\n",
    "for a in ax.values():\n",
    "    for side in 'right','top':\n",
    "        a.spines[side].set_visible(False)\n",
    "for key in 'bc':\n",
    "    ax[key].set_ylim([-3.5, 3.5])\n",
    "    ax[key].set_yticks(np.r_[-3 : 4 : 1.5])\n",
    "for key in 'de':\n",
    "    ax[key].set_ylim([-55, -10])\n",
    "    ax[key].set_yticks(np.r_[-50 : -9 : 10])\n",
    "    ax[key].set_xscale('log')\n",
    "    ax[key].set_xlabel('Frequency [Hz]')\n",
    "    ticks = np.array([0.1, 0.2, 0.5, 1, 2, 5, 10, 20])\n",
    "    ax[key].set_xlim(ticks[[0,-1]] + np.array([0,1]))\n",
    "    ax[key].xaxis.set_major_locator(FixedLocator(ticks))\n",
    "    ax[key].xaxis.set_minor_locator(NullLocator())\n",
    "    ax[key].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in ticks]))\n",
    "ax['c'].set_yticklabels([])\n",
    "ax['e'].set_yticklabels([])\n",
    "\n",
    "ax['b'].set_xlabel('Time [s]')\n",
    "ax['b'].set_ylabel('Norm. V')\n",
    "ax['c'].set_xlabel('Distr.')\n",
    "ax['d'].set_ylabel('Power [dB]')\n",
    "\n",
    "ticks = np.r_[0 : 10.5 : 2]\n",
    "ax['b'].set_xlim(ticks[[0,-1]])\n",
    "ax['b'].xaxis.set_major_locator(FixedLocator(ticks))\n",
    "ax['b'].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in ticks]))\n",
    "\n",
    "ticks = np.r_[0 : 0.51 : 0.25]\n",
    "ax['c'].set_xlim(ticks[[0,-1]])\n",
    "ax['c'].xaxis.set_major_locator(FixedLocator(ticks))\n",
    "ax['c'].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in ticks]))\n",
    "\n",
    "inset = fig.add_axes([0.85, 0.62, 0.13, 0.09])\n",
    "for i,(m,s) in enumerate(zip(mom,std)):\n",
    "    inset.plot(m, s, 'o', ms=2.5, color=cmap(i))\n",
    "dfs['c-inset'] = pd.DataFrame(data={'Momentum': mom, 'Stddev': std})\n",
    "for side in 'right','top':\n",
    "    inset.spines[side].set_visible(False)\n",
    "xlim = [mom.min()*0.95, mom.max()*1.05]\n",
    "ylim = [std.min()*0.95, std.max()*1.05]\n",
    "inset.set_xlim(xlim)\n",
    "inset.set_ylim(ylim)\n",
    "xticks = [mom.min(), mom.max()]\n",
    "yticks = [std.min(), std.max()]\n",
    "inset.xaxis.set_major_locator(FixedLocator(xticks))\n",
    "inset.xaxis.set_major_formatter(FixedFormatter([f'{tick:.2f}' for tick in xticks]))\n",
    "inset.yaxis.set_major_locator(FixedLocator(yticks))\n",
    "inset.yaxis.set_major_formatter(FixedFormatter([f'{tick:.2f}' for tick in yticks]))\n",
    "    \n",
    "trans = mtransforms.ScaledTranslation(-0.45, -0.05, fig.dpi_scale_trans)\n",
    "for label in 'abd':\n",
    "    ax[label].text(0.0, 1.0, label, transform=ax[label].transAxes + trans,\n",
    "                   fontsize=fontsize, fontweight='bold', va='bottom')\n",
    "trans = mtransforms.ScaledTranslation(-0.15, -0.05, fig.dpi_scale_trans)\n",
    "for label in 'ce':\n",
    "    ax[label].text(0.0, 1.0, label, transform=ax[label].transAxes + trans,\n",
    "                   fontsize=fontsize, fontweight='bold', va='bottom')\n",
    "\n",
    "fig.tight_layout(pad=0.2)\n",
    "plt.savefig('traces_hist_spectra_comp_grid.pdf')\n",
    "save_dfs_dict(dfs, 'figure_2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ID = '474d2016e33b441889ce8b17531487cb' # replaces '98475b819ecb4d569646d7e1467d7c9c'\n",
    "# experiment_ID = 'd0e4cb94211c4190828fd8cd856cdd94' # replaces 'ed79ae2784274401a9dba5f5ccee98d8'\n",
    "experiments_path = '../experiments/neural_network/'\n",
    "history = pickle.load(open(os.path.join(experiments_path, experiment_ID, 'history.pkl'), 'rb'))\n",
    "test_results = pickle.load(open(os.path.join(experiments_path, experiment_ID, 'test_results.pkl'), 'rb'))\n",
    "N_bands = 60\n",
    "filter_order = 6\n",
    "if experiment_ID == '474d2016e33b441889ce8b17531487cb': # replaces '98475b819ecb4d569646d7e1467d7c9c'\n",
    "    N_trials = 4000\n",
    "elif experiment_ID == 'd0e4cb94211c4190828fd8cd856cdd94': # replaces 'ed79ae2784274401a9dba5f5ccee98d8'\n",
    "    N_trials = 1000\n",
    "else:\n",
    "    raise Exception(f'Unknown number of trials for experiment {experiment_ID[:6]}')\n",
    "correlations_file = f'correlations_{experiment_ID[:6]}_{N_bands}-bands_64-filters_' + \\\n",
    "    f'36-neurons_{N_trials}-trials_{filter_order}-butter_Vd_bus3_pool_1_3.npz'\n",
    "correlations = np.load(os.path.join(experiments_path, experiment_ID, correlations_file))\n",
    "for key in correlations.files:\n",
    "    exec(f'{key} = correlations[\"{key}\"]')\n",
    "corr_idx = idx\n",
    "corr_edges = edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'var_G2_G3'\n",
    "\n",
    "fig = plt.figure(figsize=(COL_WIDTH[2], 4))\n",
    "\n",
    "offset = 0.07, 0.1\n",
    "border = 0.01, 0.02\n",
    "space = ((0.05, 0.1, 0.1), (0.075, 0.075, 0.13)), 0.17\n",
    "rows = 2\n",
    "h = (1 - offset[1] - border[1] - space[1] * (rows-1)) / rows\n",
    "w_rel = [\n",
    "    [3, 2, 3, 2],\n",
    "    [2, 3, 3, 2]\n",
    "]\n",
    "\n",
    "w_rel_sum = np.sum(w_rel, axis=1)\n",
    "cols = list(map(len, w_rel))\n",
    "w_total = [1 - offset[0] - border[0] - np.sum(sp) for sp in space[0]]\n",
    "w = []\n",
    "for i in range(rows):\n",
    "    w.append([])\n",
    "    for j in range(cols[i]):\n",
    "        w[-1].append(w_total[i] * w_rel[i][j] / w_rel_sum[i])\n",
    "\n",
    "labels = ['abcd','efgh']\n",
    "ax = {}\n",
    "for i in range(rows):\n",
    "    for j in range(cols[i]):\n",
    "        ax[labels[i][j]] = fig.add_axes([offset[0] + np.sum(space[0][i][:j]) + np.sum(w[i][:j]),\n",
    "                                         1 - border[1] - h * (i+1) - space[1] * i,\n",
    "                                         w[i][j],\n",
    "                                         h])\n",
    "\n",
    "dfs = make_dfs_dict([key for key in ax])\n",
    "# ############# Panel A #############\n",
    "cmap = plt.get_cmap('tab10')\n",
    "green, magenta = cmap(2), cmap(6)\n",
    "jdx, = np.where(t < tend)\n",
    "ax['a'].plot(t[jdx], X[key][:5, jdx].T, color=green, lw=0.5)\n",
    "ax['a'].plot(t[jdx], X[key][-10:-5, jdx].T, color=magenta, lw=0.5)\n",
    "dfs['a']['Time'] = t[jdx]\n",
    "M = 'M={:.4f}'.format(np.unique(y[set_name])[0])\n",
    "for i in range(5):\n",
    "    dfs['a'][f'V_{M}_{i+1}'] = X[key][i, jdx]\n",
    "M = 'M={:.4f}'.format(np.unique(y[set_name])[-1])\n",
    "for i in range(5):\n",
    "    dfs['a'][f'V_{M}_{i+1}'] = X[key][-1-i, jdx]\n",
    "\n",
    "# ############# Panel B #############\n",
    "idx = np.concatenate(group_index[key][:4])\n",
    "n,edges = np.histogram(X[key][idx,:], bins=50, density=True)\n",
    "ax['b'].plot(n, edges[1:], lw=1, color=green)\n",
    "M = 'M={:.4f}'.format(np.unique(y[set_name])[0])\n",
    "dfs['b']['edges_' + M] = edges[1:]\n",
    "dfs['b']['distr_' + M] = n\n",
    "m = Xf[key][idx,:].mean(axis=0)\n",
    "ci = 1.96 * Xf[key][idx,:].std(axis=0) / np.sqrt(idx.size)\n",
    "ax['e'].plot(20 * np.log10(m), F, color=green, lw=1)\n",
    "dfs['e']['Frequency'] = F\n",
    "dfs['e']['Power_' + M] = 20 * np.log10(m)\n",
    "idx = np.concatenate(group_index[key][4:])\n",
    "n,edges = np.histogram(X[key][idx,:], bins=50, density=True)\n",
    "ax['b'].plot(n, edges[1:], lw=1, color=magenta)\n",
    "M = 'M={:.4f}'.format(np.unique(y[set_name])[-1])\n",
    "dfs['b'][f'edges_{M}'] = edges[1:]\n",
    "dfs['b'][f'distr_{M}'] = n\n",
    "m = Xf[key][idx,:].mean(axis=0)\n",
    "ci = 1.96 * Xf[key][idx,:].std(axis=0) / np.sqrt(idx.size)\n",
    "ax['e'].plot(20 * np.log10(m), F, color=magenta, lw=1)\n",
    "dfs['e']['Power_' + M] = 20 * np.log10(m)\n",
    "ax['b'].set_xlim([0, 0.5])\n",
    "ticks = np.r_[0 : 0.51 : 0.25]\n",
    "ax['b'].xaxis.set_major_locator(FixedLocator(ticks))\n",
    "ax['b'].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in ticks]))\n",
    "ax['b'].set_yticklabels([])\n",
    "\n",
    "# ############# Panel C #############\n",
    "ax['c'].plot(history['loss'], 'k', lw=1, label='Training')\n",
    "ax['c'].plot(history['val_loss'], 'r', lw=1, label='Validation')\n",
    "ax['c'].legend(loc='upper right', frameon=False, fontsize=fontsize)\n",
    "dfs['c'] = pd.DataFrame(data={'Epoch': np.arange(len(history['loss']))+1,\n",
    "                              'Loss': history['loss'],\n",
    "                              'Validation_loss': history['val_loss']})\n",
    "\n",
    "# ############# Panel D #############\n",
    "target_values = np.unique(exact_momentum)\n",
    "df = pd.DataFrame(data={'exact': exact_momentum, 'pred': np.concatenate(pred_momentum)})\n",
    "df_ctrl = pd.DataFrame(data={'exact': exact_momentum, 'pred': np.concatenate(pred_momentum_ctrl)})\n",
    "sns.violinplot(x='exact', y='pred', data=df, cut=0, inner='quartile',\n",
    "               palette=[green, magenta], ax=ax['d'], linewidth=0.5)\n",
    "ax['d'].xaxis.set_major_locator(FixedLocator([0, 1]))\n",
    "ax['d'].xaxis.set_major_formatter(FixedFormatter([f'{tick:.2f}' for tick in target_values]))\n",
    "ax['d'].yaxis.set_major_locator(FixedLocator(target_values))\n",
    "ax['d'].yaxis.set_major_formatter(FixedFormatter([f'{tick:.2f}' for tick in target_values]))\n",
    "dfs['d'] = df\n",
    "\n",
    "# ############# Panels F, G ########\n",
    "_,_,corr_dfs = plot_correlations(R, p, R_ctrl, p_ctrl, corr_edges,\n",
    "                          [np.concatenate(corr_idx)], sort_freq=[1.1],\n",
    "                          ax=np.array([[ax['f'], ax['g'], ax['h']]]))\n",
    "ax['h'].plot(np.zeros(2), ax['h'].get_ylim(), ':', lw=1, color=[.6,.6,.6])\n",
    "ax['f'].set_title('Trained network', fontsize=fontsize)\n",
    "ax['g'].set_title('Untrained network', fontsize=fontsize)\n",
    "for df,k in zip(corr_dfs,'FGH'):\n",
    "    dfs[k] = df\n",
    "\n",
    "for label in 'ab':\n",
    "    ax[label].set_ylim([-3.5, 3.5])\n",
    "    ax[label].set_yticks(np.r_[-3 : 3.1])\n",
    "\n",
    "for label in 'abe':\n",
    "    ax[label].grid(which='major', axis='y', lw=0.5, ls=':', color=[.6,.6,.6])\n",
    "\n",
    "for a in ax.values():\n",
    "    for side in 'right','top':\n",
    "        a.spines[side].set_visible(False)\n",
    "\n",
    "for label in 'efg':\n",
    "    ax[label].set_yscale('log')\n",
    "    ax[label].set_ylim([0.05, 20])\n",
    "\n",
    "ax['e'].set_xlim([-10, -55])\n",
    "ax['e'].set_xticks(np.r_[-50 : -9 : 10])\n",
    "\n",
    "for label in 'efgh':\n",
    "    ticks = np.array([0.1, 0.2, 0.5, 1, 2, 5, 10, 20])\n",
    "    ax[label].set_ylim(ticks[[0,-1]])\n",
    "    ax[label].yaxis.set_major_locator(FixedLocator(ticks))\n",
    "    ax[label].yaxis.set_minor_locator(NullLocator())\n",
    "    ax[label].yaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in ticks]))\n",
    "\n",
    "ax['a'].set_xlabel('Time [s]')\n",
    "ax['a'].set_ylabel('Norm. V')\n",
    "ax['b'].set_xlabel('Distribution')\n",
    "ax['c'].set_xlabel('Epoch')\n",
    "ax['c'].set_ylabel('Loss')\n",
    "ax['d'].set_xlabel(r'Exact M [GW$\\cdot$s$^2$]')\n",
    "ax['d'].set_ylabel(r'Predicted M [GW$\\cdot$s$^2$]')\n",
    "ax['e'].set_xlabel('Power [dB]')\n",
    "ax['e'].set_ylabel('Frequency [Hz]')\n",
    "ax['f'].set_xlabel('Filter #')\n",
    "ax['g'].set_xlabel('Filter #')\n",
    "ax['h'].set_xlabel('Correlation')\n",
    "ax['f'].set_xticklabels([1, 32, 64])\n",
    "ax['g'].set_xticklabels([1, 32, 64])\n",
    "\n",
    "trans = mtransforms.ScaledTranslation(-0.2, -0.05, fig.dpi_scale_trans)\n",
    "ax['b'].text(0.0, 1.0, 'b', transform=ax['b'].transAxes + trans, fontsize=fontsize,\n",
    "             fontweight='bold', va='bottom')\n",
    "trans = mtransforms.ScaledTranslation(-0.55, -0.05, fig.dpi_scale_trans)\n",
    "ax['c'].text(0.0, 1.0, 'c', transform=ax['c'].transAxes + trans, fontsize=fontsize,\n",
    "             fontweight='bold', va='bottom')\n",
    "trans = mtransforms.ScaledTranslation(-0.4, -0.05, fig.dpi_scale_trans)\n",
    "for label in 'adefgh':\n",
    "    ax[label].text(0.0, 1.0, label, transform=ax[label].transAxes + trans,\n",
    "                   fontsize=fontsize, fontweight='bold', va='bottom')\n",
    "\n",
    "plt.savefig(f'low_high_momentum_{experiment_ID[:6]}.pdf')\n",
    "save_dfs_dict(dfs, 'figure_3.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(os.path.join(experiments_path, experiment_ID, 'stopband_momentum_estimation.npz'))\n",
    "for key in data.files:\n",
    "    exec(f'{key} = data[\"{key}\"]')\n",
    "N_bands = len(bands)\n",
    "Xfm = np.zeros((len(group_index), F.size))\n",
    "Xfci = np.zeros((len(group_index), F.size))\n",
    "for i,idx in enumerate(group_index):\n",
    "    Xfm[i] = Xf[idx].mean(axis=0)\n",
    "    Xfci[i] = 1.96 * Xf[idx].std(axis=0) / np.sqrt(idx.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplot_mosaic(\n",
    "    '''\n",
    "    aaaa.\n",
    "    bbbb.\n",
    "    ''',\n",
    "    figsize=(COL_WIDTH[1], 4)\n",
    ")\n",
    "\n",
    "dfs = make_dfs_dict('ab')\n",
    "cmap2 = plt.get_cmap('Set1')\n",
    "y_m = [y[idx].mean() for idx in group_index]\n",
    "y_s = [y[idx].std() for idx in group_index]\n",
    "y_pred_m = np.array([[pred[jdx].mean() for jdx in group_index] for pred in y_pred])\n",
    "y_pred_s = np.array([[pred[jdx].std() for jdx in group_index] for pred in y_pred])\n",
    "add_band = False\n",
    "last_band = N_bands + 1 if add_band else N_bands + 1\n",
    "dfs['a']['Exact'] = np.concatenate((y_m, y_s))\n",
    "dfs['a'].index = 'Mean low momentum', 'Mean high momentum', 'Stddev low momentum', 'Stddev high momentum'\n",
    "for i in range(last_band):\n",
    "    if i == 0:\n",
    "        lbl = 'Broadband'\n",
    "    else:\n",
    "        lbl = f'({bands[i-1][0]}-{bands[i-1][1]:g}) Hz'\n",
    "    dfs['a'][lbl] = np.concatenate((y_pred_m[i,:], y_pred_s[i,:]))\n",
    "    ax['a'].plot(y_m, y_pred_m[i], color=cmap2(i), lw=1, label=lbl)\n",
    "    for j in range(len(group_index)):\n",
    "        ax['a'].plot(y_m[j] + np.zeros(2),\n",
    "                   y_pred_m[i,j] + y_pred_s[i,j] * np.array([-1,1]),\n",
    "                   color=cmap2(i), lw=1)\n",
    "        ax['a'].plot(y_m[j] + y_s[j] * np.array([-1,1]),\n",
    "                   y_pred_m[i,j] + np.zeros(2),\n",
    "                   color=cmap2(i), lw=1)\n",
    "        ax['a'].plot(y_m[j], y_pred_m[i,j], 'o', color=cmap2(i),\n",
    "                   markerfacecolor='w', markersize=4, markeredgewidth=1)\n",
    "for side in 'right','top':\n",
    "    ax['a'].spines[side].set_visible(False)\n",
    "ax['a'].legend(loc='center left', bbox_to_anchor=[0.875, 0.5], frameon=False, fontsize=fontsize-1)\n",
    "ax['a'].set_xlabel(r'Exact M [GW$\\cdot$s$^2$]')\n",
    "ax['a'].set_ylabel(r'Predicted M [GW$\\cdot$s$^2$]')\n",
    "ax['a'].set_xlim(y_m + np.diff(y_m) * np.array([-1/10, 1/5]))\n",
    "ax['a'].set_ylim(y_m + np.diff(y_m) / 3 * np.array([-1,1]))\n",
    "ax['a'].xaxis.set_major_locator(FixedLocator(y_m))\n",
    "ax['a'].xaxis.set_major_formatter(FixedFormatter([f'{tick:.2f}' for tick in y_m]))\n",
    "ax['a'].yaxis.set_major_locator(FixedLocator(y_m))\n",
    "ax['a'].yaxis.set_major_formatter(FixedFormatter([f'{tick:.2f}' for tick in y_m]))\n",
    "\n",
    "axr = ax['b'].twinx()\n",
    "dfs['b']['Frequency'] = F\n",
    "for i,(m,ci,col) in enumerate(zip(Xfm, Xfci, (green,magenta))):\n",
    "    ax['b'].plot(F, 20*np.log10(m), color=col,\n",
    "                 label=r'M = {:.2f} GW$\\cdot$s$^2$'.format(y[group_index[i]].mean()))\n",
    "    k = 'M={:.4f}'.format(y[group_index[i]].mean())\n",
    "    dfs['b']['Power_' + k] = 20*np.log10(m)\n",
    "ax['b'].legend(loc='lower left', frameon=False, fontsize=fontsize-1, bbox_to_anchor=[0.0, 0.13])\n",
    "axr.plot(ax['b'].get_xlim(), scores[0] + np.zeros(2), '--', color=cmap2(0), lw=2)\n",
    "for i,band in enumerate(bands):\n",
    "    if i >= last_band-1:\n",
    "        break\n",
    "    axr.axvline(band[0], color=[.6,.6,.6], ls=':', lw=0.5)\n",
    "    axr.plot(band, scores[i+1] + np.zeros(2), color=cmap2(i+1), lw=2)\n",
    "ax['b'].set_xlabel('Frequency [Hz]')\n",
    "ax['b'].set_ylabel('Power [dB]')\n",
    "axr.set_ylabel(r'R$^2$ score')\n",
    "ax['b'].set_xscale('log')\n",
    "\n",
    "ticks = np.array([0.1, 0.2, 0.5, 1, 2, 5, 10, 20])\n",
    "ax['b'].set_xlim(ticks[[0,-1]] + np.array([0,1]))\n",
    "ax['b'].xaxis.set_major_locator(FixedLocator(ticks))\n",
    "ax['b'].xaxis.set_minor_locator(NullLocator())\n",
    "ax['b'].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in ticks]))\n",
    "axr.set_ylim((-0.25, 1.05))\n",
    "axr.set_yticks(np.r_[-0.2 : 1.05 : 0.2])\n",
    "\n",
    "trans = mtransforms.ScaledTranslation(-0.45, -0.05, fig.dpi_scale_trans)\n",
    "for label in 'ab':\n",
    "    ax[label].text(0.0, 1.0, label, transform=ax[label].transAxes + trans,\n",
    "                   fontsize=fontsize, fontweight='bold', va='bottom')\n",
    "\n",
    "fig.tight_layout(pad=0.2)\n",
    "if add_band:\n",
    "    fig.savefig(f'stopband_momentum_estimation_{experiment_ID}_{last_band}.pdf')\n",
    "else:\n",
    "    fig.savefig(f'stopband_momentum_estimation_{experiment_ID[:6]}.pdf')\n",
    "save_dfs_dict(dfs, 'figure_4.xlsx', index=[True,False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_IDs = '474d2016e33b441889ce8b17531487cb', 'c6f72abb5e364c4cb7770250e135bd73'\n",
    "experiments_path = '../experiments/neural_network/'\n",
    "\n",
    "data, correlations = {}, {}\n",
    "N_bands = 60\n",
    "filter_order = 6\n",
    "for experiment_ID,N_trials in zip(experiment_IDs, (4000,4000)):\n",
    "    key = experiment_ID[:6]\n",
    "    tmp = np.load(os.path.join(experiments_path, experiment_ID, f'variable_inertia_{key}.npz'),\n",
    "                  allow_pickle=True)\n",
    "    data[key] = {}\n",
    "    for fname in tmp.files:\n",
    "        try:\n",
    "            exec(f'data[\"{key}\"][\"{fname}\"] = tmp[\"{fname}\"].item()')\n",
    "        except:\n",
    "            exec(f'data[\"{key}\"][\"{fname}\"] = tmp[\"{fname}\"]')\n",
    "    correlations_file = f'correlations_{key}_{N_bands}-bands_64-filters_' + \\\n",
    "        f'36-neurons_{N_trials}-trials_{filter_order}-butter_Vd_bus3_pool_1_3.npz'\n",
    "    correlations[key] = np.load(os.path.join(experiments_path, experiment_ID, correlations_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(COL_WIDTH[2], 4))\n",
    "\n",
    "cols = 2\n",
    "offset = np.array([[0.075, 0.1], [0.14, 0.03]])\n",
    "space = {\n",
    "    'ab': 0.125,\n",
    "    'bc': 0.05,\n",
    "    'lr': 0.1,\n",
    "    'de': 0.125,\n",
    "    'ef': 0.035\n",
    "}\n",
    "width = {'a': 0.4}\n",
    "height = {'a': 0.4}\n",
    "width['b'] = (width['a'] - space['bc']) / 2\n",
    "width['c'] = width['b']\n",
    "height['b'] = 1 - np.sum(offset[:,1]) - height['a'] - space['ab']\n",
    "height['c'] = height['b']\n",
    "width['d'] = 1 - np.sum(offset[:,0]) - space['lr'] - width['a'] + 0.1\n",
    "width['e'], width['f'] = width['d'] - 0.06, width['d']\n",
    "height['d'] = (1 - np.sum(offset[:,1]) - space['de'] - space['ef']) / 2.5\n",
    "height['e'] = (1 - np.sum(offset[:,1]) - space['de'] - space['ef'] - height['d']) / 2\n",
    "height['f'] = height['e']\n",
    "\n",
    "ax = {\n",
    "    'a': plt.axes([offset[0,0],\n",
    "                   offset[0,1] + space['ab'] + height['b'],\n",
    "                   width['a'], height['a']]),\n",
    "    'b': plt.axes([offset[0,0],\n",
    "                   offset[0,1],\n",
    "                   width['b'],\n",
    "                   height['b']]),\n",
    "    'c': plt.axes([offset[0,0] + space['bc'] + width['b'],\n",
    "                   offset[0,1],\n",
    "                   width['c'], height['c']]),\n",
    "    'd': plt.axes([offset[0,0] + width['a'] + space['lr'],\n",
    "                   offset[0,1] + 2*height['e'] + space['ef'] + space['de'],\n",
    "                   width['d'], height['d']]),\n",
    "    'e': plt.axes([offset[0,0] + width['a'] + space['lr'],\n",
    "                   offset[0,1] + height['f'] + space['ef'],\n",
    "                   width['e'], height['e']]),\n",
    "    'f': plt.axes([offset[0,0] + width['a'] + space['lr'],\n",
    "                   offset[0,1],\n",
    "                   width['f'], height['f']]),\n",
    "}\n",
    "\n",
    "dfs = make_dfs_dict('abcd')\n",
    "\n",
    "def cmap(i):\n",
    "    p = sns.color_palette('colorblind')\n",
    "    return [p[0], p[1], p[2], p[4]][i]\n",
    "\n",
    "################### Panels A, B and C ###################\n",
    "key = experiment_IDs[0][:6]\n",
    "xticks = {'a': np.array([0.1, 0.2, 0.5, 1, 2, 5, 10, 20]),\n",
    "          'b': np.array([0.4, 0.5, 0.7, 1, 1.5]),\n",
    "          'c': np.array([8, 10, 12, 15])}\n",
    "yticks = {'a': np.r_[-60 : -5 : 10],\n",
    "          'b': np.r_[-30 : -9 : 5],\n",
    "          'c': np.r_[-56 : -44 : 3]}\n",
    "dfs['a']['Frequency'] = F\n",
    "for a in 'abc':\n",
    "    n = 0\n",
    "    for i,(k,v) in enumerate(data[key]['Xf'].items()):\n",
    "        for j in range(data[key]['n_mom_groups'][k]):\n",
    "            lbl = r'{:.3f} GW$\\cdot$s$^2$'.format(data[key]['ym'][k][j])\n",
    "            if k == 'var_G2_G3':\n",
    "                lbl += ' (GEN)'\n",
    "            elif k == 'var_Comp11':\n",
    "                lbl += ' (COMP)'\n",
    "            m = v[data[key]['group_index'][k][j], :].mean(axis=0)\n",
    "            s = v[data[key]['group_index'][k][j], :].std(axis=0)\n",
    "            ci = 1.96 * s / np.sqrt(data[key]['group_index'][k][j].size)\n",
    "            ax[a].fill_between(data[key]['F'],\n",
    "                               20*np.log10(m + ci),\n",
    "                               20*np.log10(m - ci),\n",
    "                               color=cmap(n), facecolor=cmap(n), edgecolor=cmap(n),\n",
    "                               alpha=0.5, label=lbl)\n",
    "            \n",
    "            n += 1\n",
    "            M = 'M={:.4f}{}'.format(data[key]['ym'][k][j], ' ' + lbl.split(' ')[-1] if k == 'var_G2_G3' else '')\n",
    "            dfs['a']['Mean_power_' + M] = m\n",
    "            dfs['a']['CI_power_' + M] = ci\n",
    "    ax[a].set_xscale('log')\n",
    "    ax[a].set_xlabel('Frequency [Hz]')\n",
    "    for side in 'right','top':\n",
    "        ax[a].spines[side].set_visible(False)\n",
    "    ax[a].set_xlim(xticks[a][[0,-1]] + np.array([0,1]))\n",
    "    ax[a].xaxis.set_major_locator(FixedLocator(xticks[a]))\n",
    "    ax[a].xaxis.set_minor_locator(NullLocator())\n",
    "    ax[a].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in xticks[a]]))\n",
    "    ax[a].yaxis.set_major_locator(FixedLocator(yticks[a]))\n",
    "    ax[a].yaxis.set_minor_locator(NullLocator())\n",
    "    ax[a].yaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in yticks[a]]))\n",
    "\n",
    "ax['a'].legend(loc='lower left', frameon=False, fontsize=6)\n",
    "ax['a'].set_ylabel('Power [dB]')\n",
    "ax['b'].set_ylabel('Power [dB]')\n",
    "ax['a'].set_ylim([-60, -10])\n",
    "ax['b'].set_xlim([0.4, 1.5])\n",
    "ax['c'].set_xlim([8, 15])\n",
    "ax['b'].set_ylim([-31, -9])\n",
    "ax['c'].set_ylim([-57, -46])\n",
    "\n",
    "################### Panel D ###################\n",
    "key = experiment_IDs[0][:6]\n",
    "ax['d'].plot(data[key]['ym']['test'], data[key]['ym']['test'], 'k--', lw=2, markerfacecolor='w')\n",
    "m = 'so'\n",
    "markers = {ID[:6]: m[i] for i,ID in enumerate(experiment_IDs)}\n",
    "lw, ms = 1.5, 6\n",
    "for ID in experiment_IDs:\n",
    "    n = 0\n",
    "    key = ID[:6]\n",
    "    ym = data[key]['ym']\n",
    "    ym_pred = data[key]['ym_pred']\n",
    "    ys_pred = data[key]['ys_pred']\n",
    "    for cond in ym:\n",
    "        for i in range(len(ym[cond])):\n",
    "            ax['d'].plot(ym[cond][i] + np.zeros(2),\n",
    "                         ym_pred[cond][i] + ys_pred[cond][i] * np.array([-1,1]),\n",
    "                         color=cmap(n), linewidth=lw)\n",
    "            ax['d'].plot(ym[cond][i], ym_pred[cond][i], markers[key], color=cmap(n), markersize=ms,\n",
    "                     markerfacecolor='w', markeredgewidth=lw)\n",
    "            n += 1\n",
    "dfs['b'] = pd.DataFrame(data={'Exact_M': ym['test'] + ym['var_G2_G3'] + ym['var_Comp11'],\n",
    "                        'Predicted_M_mean': ym_pred['test'] + ym_pred['var_G2_G3'] + ym_pred['var_Comp11'],\n",
    "                        'Predicted_M_stddev': ys_pred['test'] + ys_pred['var_G2_G3'] + ys_pred['var_Comp11']},\n",
    "                  index=['Low momentum', 'High momentum', 'Varying H_G2+H_G3', 'Varying H_Comp11'])\n",
    "ax['d'].plot(1, 1, 'k'+m[0], markersize=ms, markerfacecolor='w',\n",
    "             markeredgewidth=lw, label='without var. comp. in training')\n",
    "ax['d'].plot(1, 1, 'k'+m[1], markersize=ms, markerfacecolor='w',\n",
    "             markeredgewidth=lw, label='with var. comp. in training')\n",
    "ax['d'].legend(loc='lower right', frameon=False, fontsize=fontsize-1, bbox_to_anchor=[1, -0.05])\n",
    "ax['d'].set_xlabel(r'Exact M [GW$\\cdot$s$^2$]')\n",
    "ax['d'].set_ylabel(r'Predicted M [GW$\\cdot$s$^2$]')\n",
    "ym = ym['test']\n",
    "ticks = [ym[0], 0.197, ym[1]]\n",
    "ax['d'].set_xlim(ym + np.diff(ym) * np.array([-1/10, 1/10]))\n",
    "ax['d'].set_ylim(ym + np.diff(ym) * np.array([-1/10, 1/10]))\n",
    "ax['d'].xaxis.set_major_locator(FixedLocator(ticks))\n",
    "ax['d'].xaxis.set_major_formatter(FixedFormatter([f'{tick:.3f}' for tick in ticks]))\n",
    "ax['d'].yaxis.set_major_locator(FixedLocator(ticks))\n",
    "ax['d'].yaxis.set_major_formatter(FixedFormatter([f'{tick:.3f}' for tick in ticks]))\n",
    "\n",
    "################### Panels E and F ###################\n",
    "key = experiment_IDs[1][:6]\n",
    "_,_,df = plot_correlations(correlations[key]['R'],\n",
    "                                 correlations[key]['p'],\n",
    "                                 None,\n",
    "                                 None,\n",
    "                                 correlations[key]['edges'],\n",
    "                                 [np.concatenate(correlations[key]['idx'])],\n",
    "                                 sort_freq=[1.5],\n",
    "                                 ax=np.array([[ax['e']]]))\n",
    "dfs['c'] = df[0]\n",
    "_,_,df = plot_correlations(correlations[key]['R'],\n",
    "                                 correlations[key]['p'],\n",
    "                                 None,\n",
    "                                 None,\n",
    "                                 correlations[key]['edges'],\n",
    "                                 [np.concatenate(correlations[key]['idx'])],\n",
    "                                 sort_freq=[10],\n",
    "                                 ax=np.array([[ax['f'], ax['f']]]))\n",
    "dfs['d'] = df[0]\n",
    "for lbl in 'ef':\n",
    "    ax[lbl].set_ylim(xticks['a'][[0,-1]] + np.array([0,1]))\n",
    "    ax[lbl].yaxis.set_major_locator(FixedLocator(xticks['a']))\n",
    "    ax[lbl].yaxis.set_minor_locator(NullLocator())\n",
    "    ax[lbl].yaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in xticks['a']]))\n",
    "    ax[lbl].set_ylabel('Frequency [Hz]')\n",
    "    ax[lbl].set_xticks([1, 32, 64])\n",
    "ax['e'].set_xticklabels([])\n",
    "ax['f'].set_xlabel('Filter #')\n",
    "\n",
    "for lbl in ax:\n",
    "    for side in 'right','top':\n",
    "        ax[lbl].spines[side].set_visible(False)\n",
    "\n",
    "trans = mtransforms.ScaledTranslation(-0.4, -0.04, fig.dpi_scale_trans)\n",
    "ax['a'].text(0.0, 1.0, 'a', transform=ax['a'].transAxes + trans,\n",
    "             fontsize=fontsize, fontweight='bold', va='bottom')\n",
    "trans = mtransforms.ScaledTranslation(-0.45, -0.04, fig.dpi_scale_trans)\n",
    "for key,lbl in zip('def', 'bcd'):\n",
    "    ax[key].text(0.0, 1.0, lbl, transform=ax[key].transAxes + trans,\n",
    "                 fontsize=fontsize, fontweight='bold', va='bottom')\n",
    "\n",
    "pdf_file = f'variable_inertia_{experiment_IDs[0][:6]}_{experiment_IDs[1][:6]}.pdf'\n",
    "fig.savefig(pdf_file)\n",
    "save_dfs_dict(dfs, 'figure_5.xlsx', index=[i==1 for i in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ID = 'f64bde90cab54d1ea770bb21f33c3ed1'\n",
    "experiments_path = '../experiments/neural_network/'\n",
    "history = pickle.load(open(os.path.join(experiments_path, experiment_ID, 'history.pkl'), 'rb'))\n",
    "test_results = pickle.load(open(os.path.join(experiments_path, experiment_ID, 'test_results.pkl'), 'rb'))\n",
    "print('MAPE on the test set: {:.2f}%.'.format(test_results['mape_prediction'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2, 1, figsize=(2.5,3))\n",
    "dfs = {'top': pd.DataFrame(data={'Epoch': np.arange(len(history['loss']))+1,\n",
    "                                 'Loss': history['loss'],\n",
    "                                 'Validation_loss': history['val_loss']})}\n",
    "ax[0].plot(history['loss'], 'k', lw=1, label='Training')\n",
    "ax[0].plot(history['val_loss'], 'r', lw=1, label='Validation')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend(loc='upper right', frameon=False)\n",
    "\n",
    "y_test,y_pred = test_results['y_test'].squeeze(), test_results['y_prediction'].squeeze()\n",
    "if experiment_ID[:6] == 'a40658':\n",
    "    limits = [0.17, 0.3]\n",
    "    ticks = np.r_[0.18 : 0.31 : 0.03]\n",
    "elif experiment_ID[:6] == 'f64bde':\n",
    "    limits = [0.17, 0.28]\n",
    "    ticks = np.r_[0.18 : 0.31 : 0.03]\n",
    "else:\n",
    "    raise Exception('set limits and ticks')\n",
    "\n",
    "dfs['bottom'] = pd.DataFrame(columns=('Exact_M', 'Predicted_M_mean', 'Predicted_M_stddev'))\n",
    "ax[1].plot(limits, limits, '--', lw=1, color=[.6,.6,.6])\n",
    "for i,y in enumerate(np.unique(y_test)):\n",
    "    idx = y_test == y\n",
    "    m = y_pred[idx].mean()\n",
    "    s = y_pred[idx].std()\n",
    "    ax[1].plot(y+np.zeros(2), m+s*np.array([-1,1]), 'k', lw=1)\n",
    "    ax[1].plot(y, m, 'ko', markersize=4, markerfacecolor='w', markeredgewidth=1)\n",
    "    dfs['bottom'].loc[i] = y, m, s\n",
    "ax[1].set_xlabel(r'Exact M [GW$\\cdot$s$^2$]')\n",
    "ax[1].set_ylabel(r'Predicted M [GW$\\cdot$s$^2$]')\n",
    "ax[1].set_xticks(ticks)\n",
    "ax[1].set_yticks(ticks)\n",
    "ax[1].set_xlim(limits)\n",
    "ax[1].set_ylim(limits)\n",
    "\n",
    "for a in ax:\n",
    "    for side in 'right','top':\n",
    "        a.spines[side].set_visible(False)\n",
    "\n",
    "fig.tight_layout(pad=0.1)\n",
    "fig.savefig(f'training_{experiment_ID[:6]}.pdf')\n",
    "save_dfs_dict(dfs, 'figure_6.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_IDs = 'a40658acee3c4e419c0ee34d0c59f4df', 'f64bde90cab54d1ea770bb21f33c3ed1'\n",
    "experiments_path = '../experiments/neural_network/'\n",
    "dbs = [shelve.open(os.path.join(experiments_path, ID, ID[:6]+'_Pfrac=0.1.out'), flag='r') \n",
    "       for ID in experiment_IDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplot_mosaic(\n",
    "    '''\n",
    "    a\n",
    "    b\n",
    "    c\n",
    "    d\n",
    "    ''',\n",
    "    figsize=(COL_WIDTH[1], 4)\n",
    ")\n",
    "\n",
    "ylim = [\n",
    "    [0.17, 0.27],\n",
    "    [0.17, 0.27],\n",
    "    [0.20, 0.25],\n",
    "    [0.20, 0.25]\n",
    "]\n",
    "yticks = [\n",
    "    np.r_[0.18 : 0.27 : 0.04],\n",
    "    np.r_[0.18 : 0.27 : 0.04],\n",
    "    np.r_[0.20 : 0.26 : 0.025],\n",
    "    np.r_[0.20 : 0.26 : 0.025],\n",
    "]\n",
    "nextch = lambda ch: chr(ord(ch) + 1)\n",
    "ithch = lambda n,start='a': chr(ord(start)+n)\n",
    "\n",
    "dfs = make_dfs_dict(ax.keys())\n",
    "\n",
    "col = [.6+np.zeros(3), np.zeros(3)]\n",
    "col = [[.2,.8,.4], np.zeros(3)]\n",
    "magenta = [1,0,1]\n",
    "\n",
    "comp = 'fixed', 'var'\n",
    "for i,db in enumerate(dbs):\n",
    "    for j,expt in enumerate(db['experiments'][:4]):\n",
    "        J = ithch(j)\n",
    "        time = expt['prediction_time']\n",
    "        prediction = np.squeeze(expt['prediction'])\n",
    "        exact = expt['exact']\n",
    "        mean_prediction = expt['mean_prediction']\n",
    "        dfs[J]['Time'] = time/60\n",
    "        dfs[J][f'M_{comp[i]}_comp'] = prediction\n",
    "        N_blocks = len(expt['H_values'])\n",
    "        block_dur = np.ceil(expt['data_time'][-1]) / N_blocks\n",
    "        area_measure = 'M'\n",
    "        measure_units = r'GW$\\cdot$s$^2$'\n",
    "        for k in range(N_blocks):\n",
    "            t0,t1 = block_dur*k, block_dur*(k+1)\n",
    "            idx, = np.where((time >= t0) & (time < t1) & np.logical_not(np.isnan(prediction)))\n",
    "            n,x = np.histogram(prediction[idx], bins=10, density=True)\n",
    "            ax[J].plot(np.array([t0, t1])/60, mean_prediction[k] + np.zeros(2), color=col[i], lw=1)\n",
    "            if i == 1:\n",
    "                ax[J].plot(np.array([t0, t1])/60, exact[k] + np.zeros(2), '--', color=magenta, lw=1)\n",
    "        ax[J].plot(time/60, prediction, color=col[i], lw=0.75)\n",
    "        if i == 0:\n",
    "            ax[J].grid(which='major', axis='y', lw=0.5, ls=':', color=[.6,.6,.6])\n",
    "            ax[J].set_ylim(ylim[j])\n",
    "            ax[J].set_xticks(np.r_[0 : 181 : 30])\n",
    "            ax[J].set_yticks(yticks[j])\n",
    "            ax[J].set_ylabel(f'{area_measure.capitalize()} [{measure_units}]')\n",
    "\n",
    "ax['d'].set_xlabel('Time [min]')\n",
    "for i in 'ab':\n",
    "    ax[i].set_xticklabels([])\n",
    "ax['c'].set_xticklabels([])\n",
    "trans = mtransforms.ScaledTranslation(-0.5, -0.05, fig.dpi_scale_trans)\n",
    "for lbl in 'abcd':\n",
    "    ax[lbl].text(0.0, 1.0, lbl, transform=ax[lbl].transAxes + trans,\n",
    "                 fontsize=fontsize, fontweight='bold', va='bottom')\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout(pad=0.2)\n",
    "pdf_file = f'area_momentum_estimation_grid_{experiment_IDs[0][:6]}_{experiment_IDs[1][:6]}.pdf'\n",
    "fig.savefig(pdf_file)\n",
    "save_dfs_dict(dfs, 'figure_7.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buses = 3,\n",
    "N_buses = len(buses)\n",
    "var_names = [f'Vd_bus{bus}' for bus in buses]\n",
    "area_measure = 'momentum'\n",
    "max_block_size = 100\n",
    "cutoff = 0.1\n",
    "filename = f'spectra_compensators_buses={\"-\".join(map(str,buses))}_cutoff={cutoff:.02f}_blocks={max_block_size}'\n",
    "data = np.load(filename + '.npz')\n",
    "for key in data.files:\n",
    "    globals()[key] = data[key]\n",
    "min_fft = Xfm.min(axis=(1,2))\n",
    "max_fft = Xfm.max(axis=(1,2))\n",
    "N_H = len(H_comp)\n",
    "step = Xfm.shape[1] // H_comp.size\n",
    "\n",
    "n = Xfm.shape[1]\n",
    "peaks = np.zeros((n,2))\n",
    "var_idx = 0\n",
    "for i in range(n):\n",
    "    x = 20*np.log10(Xfm[var_idx, i, F < 1.5])\n",
    "    locs,_ = find_peaks(x, height=10, prominence=1, distance=5)\n",
    "    peaks[i,:] = F[locs[1:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(COL_WIDTH[1], 5))\n",
    "x_offset = [0.175, 0.19]\n",
    "y_offset = [0.075, 0.03]\n",
    "x_space = 0.1\n",
    "y_space = 0.05\n",
    "\n",
    "H_comp_sub = [1.0, 3.0, 6.0]\n",
    "rows = len(H_comp_sub)\n",
    "height = [0.15, 0.15]\n",
    "n = 2.2\n",
    "w = 1 - np.sum(x_offset)\n",
    "h = (1 - np.sum(height) - y_space*len(height) - np.sum(y_offset) - y_space * (rows - 1)) / rows\n",
    "\n",
    "ax = [plt.axes([x_offset[0], 1 - y_offset[1] - height[0], w, height[0]])]\n",
    "for i in range(rows-1, -1, -1):\n",
    "    ax.append(plt.axes([x_offset[0],\n",
    "                        y_offset[0] + height[1] + y_space*1.7 + (h + y_space/2) * i,\n",
    "                        w, h]))\n",
    "ax.append(plt.axes([x_offset[0], y_offset[0], w, height[1]]))\n",
    "\n",
    "dfs = make_dfs_dict('abc')\n",
    "\n",
    "xticks = np.array([0.1, 0.2, 0.5, 1, 2, 5, 10, 20])\n",
    "var_idx = 0\n",
    "\n",
    "reds = plt.get_cmap('Reds', N_H+4)\n",
    "blues = plt.get_cmap('Blues', N_H+4)\n",
    "dfs['a']['F'] = F\n",
    "for i in range(1, N_H):\n",
    "    lbl = 'Low momentum' if i == N_H - 1 else None\n",
    "    ax[0].plot(F, 20*np.log10(Xfm[var_idx, i*step, :]), color=reds(i+2), lw=2, label=lbl)\n",
    "    lbl = 'High momentum' if i == N_H - 1 else None\n",
    "    ax[0].plot(F, 20*np.log10(Xfm[var_idx, step - 1 + i*step, :]), color=blues(i+2), lw=1, label=lbl)\n",
    "    dfs['a'][f'Low_M_H_comp={H_comp[i]:.1f}'] = 20*np.log10(Xfm[var_idx, i*step, :])\n",
    "    dfs['a'][f'High_M_H_comp={H_comp[i]:.1f}'] = 20*np.log10(Xfm[var_idx, step - 1 + i*step, :])\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_yticks(np.r_[-20:25:10])\n",
    "ax[0].set_ylabel('Power [dB]')\n",
    "ax[0].grid(which='major', axis='both', ls=':', lw=0.5, color=[.6,.6,.6])\n",
    "ax[0].set_xlim(xticks[[0,-1]] + np.array([0,1]))\n",
    "ax[0].xaxis.set_major_locator(FixedLocator(xticks))\n",
    "ax[0].xaxis.set_minor_locator(NullLocator())\n",
    "ax[0].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in xticks]))\n",
    "ax[0].legend(loc='lower left', frameon=False, fontsize=fontsize-1, bbox_to_anchor=[0,-0.05])\n",
    "ax[0].arrow(17, 7, -10, 7, shape='left', width=0.2, head_width=2, head_length=1, fc='k', ec='k')\n",
    "ax[0].text(10, 27, 'Increasing H comp', fontsize=fontsize-2, va='top', ha='center', rotation=-15)\n",
    "_forward = lambda x: 20 * np.log10(x)\n",
    "_inverse = lambda y: 10 ** (y/20)\n",
    "cmap = plt.cm.inferno\n",
    "cbar_ticks = np.r_[-20 : 35 : 10]\n",
    "for i,H in enumerate(H_comp_sub):\n",
    "    j = np.where(H_comp == H)[0][0]\n",
    "    idx = IDX[j]\n",
    "    norm = FuncNorm((_forward, _inverse), vmin=_inverse(cbar_ticks[0]), vmax=_inverse(cbar_ticks[-1]))\n",
    "    X,Y = np.meshgrid(F, ym[idx])\n",
    "    ylim = [Y.min(), Y.max()]\n",
    "    ax[i+1].imshow(Xfm[var_idx, idx, :], origin='lower', aspect='auto',\n",
    "                   extent=[X.min(), X.max(), Y.min(), Y.max()], cmap=cmap, norm=norm)\n",
    "    dfs['b'][f'F_H_comp={H:.1f}'] = X.flatten()\n",
    "    dfs['b'][f'M_H_comp={H:.1f}'] = Y.flatten()\n",
    "    dfs['b'][f'Power_H_comp={H:.1f}'] = Xfm[var_idx, idx, :].flatten()\n",
    "    col = [[1,1,1], [0,.75,1]]\n",
    "    for k in range(2):\n",
    "        jdx = np.argsort(ym[idx])\n",
    "        xdata = peaks[idx[jdx],k]\n",
    "        ydata = ym[idx[jdx]]\n",
    "        func = lambda x,a,b: a*x**b\n",
    "        popt,pcov = curve_fit(func, ydata, xdata)\n",
    "        ax[i+1].plot(func(ydata, *popt), ydata, '--', color=col[k], lw=1)\n",
    "\n",
    "    dx = np.log(F[-1] - cutoff) / 40\n",
    "    dy = np.diff(ylim)[0] / 10\n",
    "    xy = np.array([\n",
    "        [Fpeak[var_idx,j], ylim[0] + dy],\n",
    "        [np.exp(np.log(Fpeak[var_idx,j]) - dx), ylim[0]],\n",
    "        [np.exp(np.log(Fpeak[var_idx,j]) + dx), ylim[0]]\n",
    "    ])\n",
    "    triangle = Polygon(xy, fc='w', ec='w')\n",
    "    ax[i+1].add_patch(triangle)\n",
    "\n",
    "    ax[i+1].set_xlim([cutoff, F[-1]])\n",
    "    ax[i+1].set_xscale('log')\n",
    "    ax[i+1].set_xlim(xticks[[0,-1]] + np.array([0,1]))\n",
    "    ax[i+1].xaxis.set_major_locator(FixedLocator(xticks))\n",
    "    ax[i+1].xaxis.set_minor_locator(NullLocator())\n",
    "    if i == rows-1:\n",
    "        ax[i+1].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in xticks]))\n",
    "    else:\n",
    "        ax[i+1].xaxis.set_major_formatter(FixedFormatter([]))\n",
    "\n",
    "    yticks = np.linspace(ylim[0], ylim[1], 3)\n",
    "    ax[i+1].text(xticks[-1]-3, ylim[1] - 0.1*np.diff(ylim), 'H = {:.1f} s'.format(H),\n",
    "               fontsize=fontsize, color='w', verticalalignment='top', horizontalalignment='right')\n",
    "    ax[i+1].set_ylim(ylim)\n",
    "    ax[i+1].yaxis.set_major_locator(FixedLocator(yticks))\n",
    "    ax[i+1].yaxis.set_major_formatter(FixedFormatter([f'{tick:.2f}' for tick in yticks]))\n",
    "    ax[i+1].set_ylabel(r'M [GW$\\cdot$s$^2$]')\n",
    "\n",
    "cax = plt.axes([1 - x_offset[1] + 0.02, y_offset[0] + height[1] + y_space*1.7, w/20, h])\n",
    "mappable = matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "cbar = fig.colorbar(mappable, cax=cax, label='Power [dB]')\n",
    "cbar.ax.set_yticks(_inverse(cbar_ticks))\n",
    "cbar.ax.set_yticklabels([f'{tick:.0f}' for tick in cbar_ticks])\n",
    "\n",
    "ax[-2].set_xlabel('Frequency [Hz]')\n",
    "\n",
    "ax[-1].plot(H_comp[1:], Fpeak[var_idx,1:], 'ko-', lw=1, markersize=4, markerfacecolor='w')\n",
    "dfs['c'] = pd.DataFrame(data={'Compensator_H': H_comp[1:], 'Frequency_peak': Fpeak[var_idx,1:]})\n",
    "ax[-1].set_xlim([0.5, 6.5])\n",
    "ax[-1].set_xticks(np.r_[1:7])\n",
    "ax[-1].set_ylim([3, 16])\n",
    "ax[-1].set_yticks(np.r_[5 : 20 : 5])\n",
    "ax[-1].set_xlabel('Compensator inertia [s]')\n",
    "ax[-1].set_ylabel('Frequency peak [Hz]')\n",
    "ax[-1].grid(which='major', axis='y', ls=':', lw=0.5, color=[.6,.6,.6])\n",
    "for side in 'right','top':\n",
    "    for a in ax:\n",
    "        a.spines[side].set_visible(False)\n",
    "\n",
    "trans = mtransforms.ScaledTranslation(-0.5, 0, fig.dpi_scale_trans)\n",
    "ax[0].text(0.0, 1.0, 'a', transform=ax[0].transAxes + trans, fontsize=fontsize, fontweight='bold', va='bottom')\n",
    "ax[-1].text(0.0, 1.0, 'c', transform=ax[-1].transAxes + trans, fontsize=fontsize, fontweight='bold', va='bottom')\n",
    "trans = mtransforms.ScaledTranslation(-0.5, 0.03, fig.dpi_scale_trans)\n",
    "ax[1].text(0.0, 1.0, 'b', transform=ax[1].transAxes + trans, fontsize=fontsize, fontweight='bold', va='bottom')\n",
    "\n",
    "fig.savefig(f'spectra_compensators_{var_names[var_idx]}.pdf')\n",
    "save_dfs_dict(dfs, 'figure_8.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full grid without varying compensator's inertia\n",
    "experiment_ID = 'f64bde90cab54d1ea770bb21f33c3ed1'\n",
    "# full grid with variable compensator's inertia\n",
    "experiment_ID = 'a40658acee3c4e419c0ee34d0c59f4df'\n",
    "experiments_path = '../experiments/neural_network/'\n",
    "Pfrac = 0.1\n",
    "shelf_name = os.path.join(experiments_path,\n",
    "                          experiment_ID,\n",
    "                          experiment_ID[:6] + f'_Pfrac={Pfrac:.1f}.out')\n",
    "db = shelve.open(shelf_name, flag='r')\n",
    "print('The shelf `{}` contains the following experiments:'.format(os.path.basename(shelf_name)))\n",
    "for i,expt in enumerate(db['experiments']):\n",
    "    print('[{}] > {}'.format(i, expt['description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTRL_expt = db['experiments'][0]\n",
    "expt = db['experiments'][7]\n",
    "descr = expt['description']\n",
    "\n",
    "pdf_file = 'prediction'\n",
    "if descr == 'momentum steps varying only generators inertia, G3 type=2':\n",
    "    pdf_file += '_G3_type=2'\n",
    "elif descr == 'momentum steps varying only generators inertia, split G3 type=2':\n",
    "    pdf_file += '_split_G3_type=2_Pfrac={:.1f}'.format(Pfrac)\n",
    "elif descr == 'momentum steps varying only generators inertia, split G3 type=6':\n",
    "    pdf_file += '_split_G3_type=6_Pfrac={:.1f}'.format(Pfrac)\n",
    "elif descr == 'momentum steps varying inertia of G2 and VSG3, a VSG that replaces G3':\n",
    "    pdf_file += '_VSG'\n",
    "elif descr == 'momentum steps varying inertia of G2, G3 and VSG3':\n",
    "    pdf_file += '_split_VSG_Pfrac={:.1f}'.format(Pfrac)\n",
    "else:\n",
    "    raise Exception('Unknown condition to consider')\n",
    "\n",
    "var_name = 'Vd_bus3'\n",
    "pdf_file += f'_{var_name}_{experiment_ID[:6]}.pdf'\n",
    "time = expt['data_time']\n",
    "dt = np.diff(time[:2])[0]\n",
    "X = expt['data'][var_name]\n",
    "\n",
    "time_wo_VSG = CTRL_expt['data_time']\n",
    "X_wo_VSG = CTRL_expt['data'][var_name]\n",
    "\n",
    "prediction_time = expt['prediction_time']\n",
    "prediction = np.squeeze(expt['prediction'])\n",
    "exact = expt['exact']\n",
    "mean_prediction = expt['mean_prediction']\n",
    "N_blocks = len(expt['H_values'])\n",
    "block_dur = np.ceil(expt['data_time'][-1]) / N_blocks\n",
    "area_measure = 'M'\n",
    "measure_units = r'GW$\\cdot$s$^2$'\n",
    "\n",
    "offset = np.array([[0.175, 0.1], [0.01, 0.04]])\n",
    "yspace_lower, yspace = 0.115, 0.025\n",
    "h_lower = 0.25\n",
    "w = 1 - np.sum(offset[:,0])\n",
    "h = (1 - np.sum(offset[:,1]) - yspace_lower - yspace*2 - h_lower) / 3\n",
    "\n",
    "fig = plt.figure(figsize=(COL_WIDTH[1], 4))\n",
    "ax = {\n",
    "    'a': plt.axes([offset[0,0], offset[0,1] + h_lower + yspace_lower + 2*(h + yspace), w, h]),\n",
    "    'b': plt.axes([offset[0,0], offset[0,1] + h_lower + yspace_lower + h + yspace, w, h]),\n",
    "    'c': plt.axes([offset[0,0], offset[0,1] + h_lower + yspace_lower, w, h]),\n",
    "    'd': plt.axes([offset[0,0], offset[0,1], w, h_lower]),\n",
    "}\n",
    "\n",
    "dfs = make_dfs_dict('ab')\n",
    "dur = 60\n",
    "N_samples = int(dur / dt)\n",
    "\n",
    "lbl = 'abc'\n",
    "xticks = np.array([0.3, 0.5, 1, 2, 5, 10])\n",
    "\n",
    "def plot_panel(time, t0, t1, X, N_samples, axx, color):\n",
    "    idx, = np.where((time >= t0) & (time < t1))\n",
    "    t = time[idx]\n",
    "    N_trials = t.size // N_samples\n",
    "    Y = np.reshape(X[idx][:N_trials*N_samples], (N_trials,-1))\n",
    "    Yf = fft(Y)\n",
    "    Yf = 2.0 / N_samples * np.abs(Yf[:, :N_samples//2])\n",
    "    F = fftfreq(N_samples, dt)[:N_samples//2]\n",
    "    Ydb = 20*np.log10(Yf.mean(axis=0))\n",
    "    axx.semilogx(F, Ydb, color, lw=1)\n",
    "    if color == 'k':\n",
    "        jdx, = np.where((F>0.5) & (F<2))\n",
    "        peaks,_ = find_peaks(Ydb[jdx], height=10, distance=10, prominence=3)\n",
    "        dx = np.log(F[-1]) / 100\n",
    "        dy = np.diff(axx.get_ylim())[0] / 50\n",
    "        offset = 2\n",
    "        for peak,fc in zip(peaks[:3], 'kkw'):\n",
    "            Fpeak,Ydbpeak = F[jdx[peak]], Ydb[jdx[peak]]\n",
    "            xy = np.array([\n",
    "                [Fpeak, Ydbpeak + offset],\n",
    "                [np.exp(np.log(Fpeak) - dx), Ydbpeak + offset + dy],\n",
    "                [np.exp(np.log(Fpeak) + dx), Ydbpeak + offset + dy]\n",
    "            ])\n",
    "            triangle = Polygon(xy, fc=fc, ec='k', lw=0.75)\n",
    "            axx.add_patch(triangle)\n",
    "            axx.text(Fpeak, Ydbpeak+offset+dy, f'{Fpeak:.2f} Hz',\n",
    "                     fontsize=fontsize-2, ha='center', va='bottom')\n",
    "    return F,Ydb\n",
    "    \n",
    "for k in range(N_blocks):\n",
    "    t0,t1 = block_dur*k, block_dur*(k+1)\n",
    "    \n",
    "    F,Ydb = plot_panel(time_wo_VSG, t0, t1, X_wo_VSG, N_samples, ax[lbl[k]], 'r')\n",
    "    if k == 0: dfs['a']['Frequency'] = F\n",
    "    dfs['a'][f'Power_without_VSG_M={exact[k]:.4f}'] = Ydb\n",
    "    _,Ydb = plot_panel(time, t0, t1, X, N_samples, ax[lbl[k]], 'k')\n",
    "    dfs['a'][f'Power_with_VSG_M={exact[k]:.4f}'] = Ydb\n",
    "    ax[lbl[k]].text(10, 22.5, r'M = {:.4f} GW$\\cdot$s$^2$'.format(expt['exact'][k]),\n",
    "                    fontsize=fontsize, va='center', ha='right')\n",
    "    \n",
    "    idx, = np.where((prediction_time >= t0) & (prediction_time < t1) & np.logical_not(np.isnan(prediction)))\n",
    "    n,x = np.histogram(prediction[idx], bins=10, density=True)\n",
    "    ax['d'].plot(np.array([t0, t1])/60, mean_prediction[k] + np.zeros(2), color='k', lw=1)\n",
    "    ax['d'].plot(np.array([t0, t1])/60, exact[k] + np.zeros(2), '--', color='m', lw=1)\n",
    "\n",
    "dfs['b'] = pd.DataFrame(data={'Time': prediction_time/60, 'Predicted_M': prediction})\n",
    "ax['d'].plot(prediction_time/60, prediction, color='k', lw=0.75)\n",
    "ax['d'].set_xticks(np.r_[0 : 181 : 30])\n",
    "ax['d'].set_xlabel('Time [min]')\n",
    "ax['d'].set_ylabel(r'M [GW$\\cdot$s$^2$]')\n",
    "ax['d'].grid(which='major', axis='y', lw=0.5, ls=':', color=[.6,.6,.6])\n",
    "\n",
    "for lbl in 'abcd':\n",
    "    for side in 'right','top':\n",
    "        ax[lbl].spines[side].set_visible(False)\n",
    "    if lbl != 'd':\n",
    "        ax[lbl].xaxis.set_minor_locator(NullLocator())\n",
    "        ax[lbl].set_xlim(xticks[[0,-1]] + np.array([0,1]))\n",
    "        ax[lbl].set_ylim([-16,31])\n",
    "        ax[lbl].set_yticks(np.r_[-15 : 31 : 15])\n",
    "        if lbl == 'c':\n",
    "            ax[lbl].xaxis.set_major_locator(FixedLocator(xticks))\n",
    "            ax[lbl].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in xticks]))\n",
    "            ax[lbl].set_xlabel('Frequency [Hz]')\n",
    "        else:\n",
    "            ax[lbl].set_xticks([])\n",
    "            ax[lbl].spines['bottom'].set_visible(False)\n",
    "        ax[lbl].grid(which='major', axis='y', lw=0.5, ls=':', color=[.6,.6,.6])\n",
    "ax['b'].set_ylabel('Power [dB]')\n",
    "\n",
    "trans = mtransforms.ScaledTranslation(-0.5, 0, fig.dpi_scale_trans)\n",
    "ax['a'].text(0.0, 1.0, 'a', transform=ax['a'].transAxes + trans, fontsize=fontsize,\n",
    "             fontweight='bold', va='bottom')\n",
    "ax['d'].text(0.0, 1.0, 'b', transform=ax['d'].transAxes + trans, fontsize=fontsize,\n",
    "             fontweight='bold', va='bottom')\n",
    "\n",
    "fig.savefig(pdf_file)\n",
    "save_dfs_dict(dfs, 'figure_9.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10\n",
    "Effect of varying the damping coefficient on the estimation of momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full grid with variable compensator's inertia\n",
    "experiment_ID = 'a40658acee3c4e419c0ee34d0c59f4df'\n",
    "experiments_path = '../experiments/neural_network'\n",
    "network_pars = pickle.load(open(os.path.join(experiments_path, experiment_ID, 'parameters.pkl'), 'rb'))\n",
    "area_ID = network_pars['area_IDs'][0]\n",
    "data_dir = network_pars['data_dirs'][0].format(area_ID)\n",
    "\n",
    "damping_data_file = 'var_D_area_1.npz'\n",
    "force = False\n",
    "if force or not os.path.isfile(damping_data_file):\n",
    "    data_files = sorted(glob.glob(os.path.join('..', data_dir, 'ieee39_*_D*_*.h5')))\n",
    "    D = np.array([float(re.findall('D=\\d.\\d', d)[0].split('=')[1]) for d in data_files])\n",
    "    window_dur = 60,\n",
    "    window_step = window_dur\n",
    "    var_names = network_pars['var_names']\n",
    "    x_train_mean = network_pars['x_train_mean']\n",
    "    x_train_std = network_pars['x_train_std']\n",
    "    data_mean = {var_name: x_train_mean[k] for k,var_name in enumerate(var_names)}\n",
    "    data_std = {var_name: x_train_std[k] for k,var_name in enumerate(var_names)}\n",
    "    Xf = []\n",
    "    for data_file in data_files:\n",
    "        t, _, _, X_slide, _ = load_data_slide([data_file], var_names, data_mean, data_std,\n",
    "                                                   window_dur, window_step, add_omega_ref=False,\n",
    "                                                   verbose=True)\n",
    "        dt = np.diff(t[:2])[0]\n",
    "        Xf.append({})\n",
    "        for var_name in var_names:\n",
    "            N_samples = X_slide[var_name].shape[1]\n",
    "            tmp = fft(X_slide[var_name])\n",
    "            tmp = 2.0 / N_samples * np.abs(tmp[:, :N_samples//2])\n",
    "            Xf[-1][var_name] = tmp.mean(axis=0)\n",
    "            F = fftfreq(N_samples, dt)[:N_samples//2]\n",
    "    data = {'F': F, 'Xf': Xf, 'd': D, 'window_dur': window_dur, 'window_step': window_step,\n",
    "            'var_names': var_names, 'x_train_mean': x_train_mean, 'x_train_std': x_train_std}\n",
    "    np.savez_compressed(damping_data_file, **data)\n",
    "else:\n",
    "    data = np.load(damping_data_file, allow_pickle=True)\n",
    "    D, F, Xf = data['D'][:-1], data['F'], data['Xf'][:-1]\n",
    "    var_names = data['var_names']\n",
    "\n",
    "db = shelve.open(os.path.join(experiments_path, experiment_ID,\n",
    "                              experiment_ID[:6]+'_Pfrac=0.1_KAD=2.0_KW=10.0.out'))\n",
    "experiments = db['experiments'][9:9+len(D)]\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for expt in experiments:\n",
    "    damping = float(re.findall('\\d.\\d', expt['description'])[-1])\n",
    "    M = expt['exact'][0]\n",
    "    M_pred = np.squeeze(expt['prediction'])\n",
    "    idx = np.where(np.logical_not(np.isnan(M_pred)))[0][0]\n",
    "    df = pd.DataFrame(data={'D': damping, 'M': M, 'M_pred': M_pred[idx::60]})\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "M = df.M.unique()[0]\n",
    "df['MAPE'] = (df['M'] - df['M_pred']).abs() / df['M'] * 100\n",
    "df_mean = df.groupby('D').mean()\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [[.8,.8,.8] for i in range(len(D))]\n",
    "offset = np.array([[0.15,0.1], [0.12,0.05]])\n",
    "space = [0.1, 0.1]\n",
    "w = 1 - np.sum(offset[:,0])\n",
    "h = 0.26\n",
    "h_inset = 1 - np.sum(offset[:,1]) - 2*h - 2*space[1]\n",
    "w_inset = (1 - np.sum(offset[:,0]) - space[0]) / 2\n",
    "fig = plt.figure(figsize=(COL_WIDTH[1], 4))\n",
    "ax = [\n",
    "    plt.axes([offset[0,0], 1-offset[1,1]-h, w, h]),\n",
    "    plt.axes([offset[0,0], offset[0,1], w, h]),\n",
    "    plt.axes([offset[0,0], offset[0,1] + h + space[1]/1.4, w_inset, h_inset]),\n",
    "    plt.axes([offset[0,0]+w_inset+space[0], offset[0,1] + h + space[1]/1.4, w_inset, h_inset])\n",
    "]\n",
    "\n",
    "dfs = make_dfs_dict('ab')\n",
    "xlim = [-0.5, len(D)-0.5]\n",
    "ax[0].plot(xlim, [M, M], 'r', lw=2, zorder=-1)\n",
    "sns.violinplot(x='D', y='M_pred', data=df, cut=0, inner='quartile',\n",
    "               palette=palette, ax=ax[0], linewidth=1, zorder=0)\n",
    "dfs['a'] = df\n",
    "twin = ax[0].twinx()\n",
    "twin.plot(df_mean['MAPE'], 'ko', ms=7, markerfacecolor='w', markeredgewidth=1.5)\n",
    "ax[0].set_xlim(xlim)\n",
    "ax[0].set_xticklabels(range(len(D)))\n",
    "ax[0].set_yticks(np.r_[0.2 : 0.25 : 0.01])\n",
    "ax[0].set_xlabel('Damping')\n",
    "ax[0].set_ylabel(r'M [GW$\\cdot$s$^2$]')\n",
    "ax[0].set_ylim([0.2, 0.24])\n",
    "ax[0].set_yticks(np.linspace(0.2, 0.24, 5))\n",
    "twin.set_ylim([1, 4.2])\n",
    "twin.set_yticks(np.linspace(1, 4.2, 5))\n",
    "twin.set_ylabel('MAPE [%]')\n",
    "\n",
    "xlim = [[1e-2,20], [1e-2,1e-1], [0.4,1.5]]\n",
    "ylim = [[-60,0], [-20,0], [-30,-10]]\n",
    "var_name = 'Vd_bus3'\n",
    "cmap = plt.get_cmap('viridis', len(Xf))\n",
    "dfs['b']['F'] = F\n",
    "for d,xf in zip(D, Xf):\n",
    "    dfs['b'][f'Power_D={d:.1f}'] = 20*np.log10(xf[var_name])\n",
    "for i,a in enumerate(ax[1:]):\n",
    "    for k,xf in enumerate(Xf):\n",
    "        a.plot(F, 20*np.log10(xf[var_name]), color=cmap(k), lw=1, label=f'D={k:.1f}')\n",
    "    a.set_xscale('log')\n",
    "    a.set_xlim(xlim[i])\n",
    "    a.set_ylim(ylim[i])\n",
    "    a.set_yticks(np.r_[ylim[i][0] : ylim[i][1]+5 : 20 if i == 0 else 10])\n",
    "    a.grid(which='major', axis='x', lw=0.5, ls=':', color=[.6,.6,.6])\n",
    "ax[1].set_xlabel('Frequency [Hz]')\n",
    "ax[1].set_ylabel('Power [dB]')\n",
    "ax[1].legend(loc='lower left', bbox_to_anchor=[-0.02, -0.08], frameon=False)\n",
    "xticks = np.array([0.01, 0.03, 0.1, 0.3, 1, 3, 10, 20])\n",
    "ax[1].xaxis.set_major_locator(FixedLocator(xticks))\n",
    "ax[1].xaxis.set_minor_locator(NullLocator())\n",
    "ax[1].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in xticks]))\n",
    "ax[2].set_ylabel('Power [dB]')\n",
    "ax[2].xaxis.set_major_locator(FixedLocator(xticks[:3]))\n",
    "ax[2].xaxis.set_minor_locator(NullLocator())\n",
    "ax[2].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in xticks[:3]]))\n",
    "xticks = np.logspace(np.log10(xlim[2][0]), np.log10(xlim[2][1]), 4)\n",
    "ax[3].xaxis.set_major_locator(FixedLocator(xticks))\n",
    "ax[3].xaxis.set_minor_locator(NullLocator())\n",
    "ax[3].xaxis.set_major_formatter(FixedFormatter([f'{tick:.1f}' for tick in xticks]))\n",
    "\n",
    "for side in 'right','top':\n",
    "    for a in ax[1:]:\n",
    "        a.spines[side].set_visible(False)\n",
    "        \n",
    "trans = mtransforms.ScaledTranslation(-0.45, 0, fig.dpi_scale_trans)\n",
    "ax[0].text(0.0, 1.0, 'a', transform=ax[0].transAxes+trans, fontsize=fontsize,\n",
    "           fontweight='bold', va='bottom')\n",
    "ax[2].text(0.0, 1.0, 'b', transform=ax[2].transAxes+trans, fontsize=fontsize,\n",
    "           fontweight='bold', va='bottom')\n",
    "\n",
    "fig.savefig('var_D_area_1.pdf')\n",
    "save_dfs_dict(dfs, 'figure_10.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
