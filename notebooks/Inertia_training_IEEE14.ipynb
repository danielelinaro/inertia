{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "from time import strftime, localtime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from deep_utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's fix the seed of the RNG, for reproducibility purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/dev/random', 'rb') as fid:\n",
    "    seed = int.from_bytes(fid.read(4), 'little')\n",
    "tf.random.set_seed(seed)\n",
    "print('Seed: {}'.format(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_IDs = [1, 2]\n",
    "n_generators = len(generator_IDs)\n",
    "data_folders = [f'../data/var_H_G{gen_id}/IEEE14_D=2_DZA=60.0/' for gen_id in generator_IDs]\n",
    "var_names = [lbl for gen_id in generator_IDs for lbl in (f'omega_G{gen_id}',f'Pe_G{gen_id}')]\n",
    "n_vars = len(var_names)\n",
    "inertia = {key: np.arange(2,11) + i/3 for i,key in enumerate(('training', 'test', 'validation'))}\n",
    "time, x, y = load_data(data_folders, generator_IDs, inertia, var_names, max_block_size = 300)\n",
    "x['train'] = x.pop('training')\n",
    "y['train'] = y.pop('training')\n",
    "N_vars, N_training_traces, N_samples = x['train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mean = np.mean(x['train'], axis=(1,2))\n",
    "x_train_std = np.std(x['train'], axis=(1,2))\n",
    "for key in x:\n",
    "    x[key] = tf.constant([(x[key][i].numpy() - m) / s for i,(m,s) in enumerate(zip(x_train_mean, x_train_std))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_values = np.array([2,10])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 2 * N_vars))\n",
    "gs = fig.add_gridspec(N_vars, 3)\n",
    "ax = []\n",
    "for i in range(N_vars):\n",
    "    ax.append(fig.add_subplot(gs[i,:2]))\n",
    "    ax.append(fig.add_subplot(gs[i,2]))\n",
    "\n",
    "idx_min, _ = np.where(y['train'] == H_values.min())\n",
    "idx_max, _ = np.where(y['train'] == H_values.max())\n",
    "\n",
    "xlim_distr = None\n",
    "if xlim_distr is not None:\n",
    "    lim = xlim_distr\n",
    "else:\n",
    "    lim = 0\n",
    "\n",
    "for i in range(N_vars):\n",
    "    j = i * 2\n",
    "    ax[j].plot(time, x['train'].numpy()[i, idx_min[:5], :].T, 'k', lw=1)\n",
    "    ax[j].plot(time, x['train'].numpy()[i, idx_max[:5], :].T, 'r', lw=1)\n",
    "\n",
    "    n,edges = np.histogram(np.ndarray.flatten(x['train'][i,:,:].numpy()[idx_min,:]), \\\n",
    "                           bins=100, range=[-4,4], density=True)\n",
    "    lim = n.max() if n.max() > lim else lim\n",
    "    ax[j + 1].plot(n, edges[:-1], 'k', linewidth=1.2, label=f'H={H_values[0]:g}')\n",
    "    n,edges = np.histogram(np.ndarray.flatten(x['train'][i,:,:].numpy()[idx_max,:]), \\\n",
    "                           bins=100, range=[-4,4], density=True)\n",
    "    lim = n.max() if n.max() > lim else lim\n",
    "    ax[j + 1].plot(n, edges[:-1], 'r', linewidth=1.2, label=f'H={H_values[1]:g}')\n",
    "    ax[j + 1].set_yticklabels([])\n",
    "\n",
    "for a in ax:\n",
    "    for side in 'right', 'top':\n",
    "        a.spines[side].set_visible(False)\n",
    "\n",
    "for i in range(N_vars):\n",
    "    j = i * 2\n",
    "    ax[j + 1].plot([0, lim*1.05], [0,0], '--', lw=1, color=[.6,.6,.6])\n",
    "    ax[j].get_shared_x_axes().join(ax[0], ax[j])\n",
    "    ax[j+1].get_shared_x_axes().join(ax[1], ax[j+1])\n",
    "    ax[j+1].get_shared_y_axes().join(ax[j], ax[j+1])\n",
    "    ax[j].set_ylabel(var_names[i].replace('_',' '))\n",
    "ax[(N_vars - 1) * 2].set_xlabel('Time [s]')\n",
    "ax[(N_vars - 1) * 2 + 1].set_xlabel('Fraction')\n",
    "ax[1].legend(loc='upper right')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n",
    "The basic network topology used here is taken from the following paper:\n",
    "\n",
    "George, D., & Huerta, E. A. (2018). Deep neural networks to enable real-time multimessenger astrophysics. Physical Review D, 97(4), 044039. http://doi.org/10.1103/PhysRevD.97.044039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing_pipeline_1D(N_samples, N_units, kernel_size, activation_fun, activation_loc, input_name):\n",
    "    if activation_fun is not None:\n",
    "        if activation_fun.lower() not in ('relu',):\n",
    "            raise Exception(f'Unknown activation function {activation_fun}')\n",
    "        if activation_loc is None:\n",
    "            raise Exception(f'Must specify activation function location')\n",
    "        elif activation_loc.lower() not in ('after_conv', 'after_pooling'):\n",
    "            raise Exception('activation_loc must be one of \"after_conv\" or \"after_pooling\"')\n",
    "    inp = keras.Input(shape=(N_samples, 1), name=input_name)\n",
    "    for N_conv,N_pooling,sz in zip(N_units['conv'], N_units['pooling'], kernel_size):\n",
    "        try:\n",
    "            L = layers.Conv1D(N_conv, sz, activation=None)(L)\n",
    "        except:\n",
    "            L = layers.Conv1D(N_conv, sz, activation=None)(inp)\n",
    "        if activation_fun is not None:\n",
    "            if activation_loc.lower() == 'after_conv':\n",
    "                L = layers.ReLU()(L)\n",
    "                L = layers.MaxPooling1D(N_pooling)(L)\n",
    "            else:\n",
    "                L = layers.MaxPooling1D(N_pooling)(L)\n",
    "                L = layers.ReLU()(L)\n",
    "        else:\n",
    "            L = layers.MaxPooling1D(N_pooling)(L)\n",
    "    return inp, L\n",
    "\n",
    "\n",
    "def make_preprocessing_pipeline_2D(N_samples, N_units, kernel_size, activation_fun, activation_loc, input_name):\n",
    "    if activation_fun is not None:\n",
    "        if activation_fun.lower() not in ('relu',):\n",
    "            raise Exception(f'Unknown activation function {activation_fun}')\n",
    "        if activation_loc is None:\n",
    "            raise Exception(f'Must specify activation function location')\n",
    "        elif activation_loc.lower() not in ('after_conv', 'after_pooling'):\n",
    "            raise Exception('activation_loc must be one of \"after_conv\" or \"after_pooling\"')\n",
    "    inp = keras.Input(shape=(N_samples, 2, 1), name=input_name)\n",
    "    for N_conv,N_pooling,sz in zip(N_units['conv'], N_units['pooling'], kernel_size):\n",
    "        try:\n",
    "            L = layers.Conv2D(N_conv, [sz, 2], padding='same', activation=None)(L)\n",
    "        except:\n",
    "            L = layers.Conv2D(N_conv, [sz, 2], padding='same', activation=None)(inp)\n",
    "        if activation_fun is not None:\n",
    "            if activation_loc.lower() == 'after_conv':\n",
    "                L = layers.ReLU()(L)\n",
    "                L = layers.MaxPooling2D([N_pooling, 1])(L)\n",
    "            else:\n",
    "                L = layers.MaxPooling2D([N_pooling, 1])(L)\n",
    "                L = layers.ReLU()(L)\n",
    "        else:\n",
    "            L = layers.MaxPooling2D([N_pooling, 1])(L)\n",
    "    return inp, L\n",
    "\n",
    "# how many dimensions the input data should have\n",
    "N_dims = 1\n",
    "# whether to have a deeper network (2) or not (1)\n",
    "depth_level = 1\n",
    "# a dropout coefficient of 0 means no Dropout layer\n",
    "dropout_coeff = 0\n",
    "# learning rate of the Adam optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "N_units = {}\n",
    "\n",
    "if depth_level == 1:\n",
    "    N_units['conv'] = [16, 32, 64]\n",
    "    N_units['dense'] = [64]\n",
    "elif depth_level == 2:\n",
    "    N_units['conv'] = [64, 128, 256, 512]\n",
    "    N_units['dense'] = [128, 64]\n",
    "\n",
    "N_units['pooling'] = [4 for _ in range(len(N_units['conv']))]\n",
    "kernel_size = [5 for _ in range(len(N_units['conv']))]\n",
    "\n",
    "if N_dims == 1:\n",
    "    inputs = []\n",
    "    L = []\n",
    "    for var_name in var_names:\n",
    "        inp,lyr = make_preprocessing_pipeline_1D(N_samples, N_units, kernel_size, \\\n",
    "                                                 'relu', 'after_conv', var_name)\n",
    "        inputs.append(inp)\n",
    "        L.append(lyr)\n",
    "else:\n",
    "    inputs,L = make_preprocessing_pipeline_2D(N_samples, N_units, kernel_size, \\\n",
    "                                              'relu', 'after_conv', '_'.join(var_names))\n",
    "\n",
    "if isinstance(L, list):\n",
    "    L = layers.concatenate(L)\n",
    "L = layers.Flatten()(L)\n",
    "for n in N_units['dense']:\n",
    "    L = layers.Dense(n, activation='relu')(L)\n",
    "if dropout_coeff > 0:\n",
    "    L = layers.Dropout(dropout_coeff)(L)\n",
    "output = layers.Dense(y['train'].shape[1])(L)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = strftime('%Y%m%d-%H%M%S', localtime())\n",
    "path = 'experiments/' + ts\n",
    "checkpoint_path = path + '/checkpoints'\n",
    "os.makedirs(checkpoint_path)\n",
    "keras.utils.plot_model(model, show_shapes=True, dpi=96)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('model.json','w') as fid:\n",
    "    fid.write(model.to_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N_dims == 2:\n",
    "    for key in x:\n",
    "        x[key] = tf.transpose(x[key], perm=(1,2,0))\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path + \\\n",
    "                                                 '/weights.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "                                                 save_weights_only=False,\n",
    "                                                 save_best_only=True,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 verbose=0)\n",
    "\n",
    "N_epochs = 5\n",
    "batch_size = 128\n",
    "N_batches = np.ceil(N_training_traces / batch_size)\n",
    "steps_per_epoch = np.max([N_batches, 100])\n",
    "\n",
    "if N_dims == 1:\n",
    "    x_train = {name: x['train'][i] for i,name in enumerate(var_names)}\n",
    "    x_validation = {name: x['validation'][i] for i,name in enumerate(var_names)}\n",
    "else:\n",
    "    x_train = x['train']\n",
    "    x_validation = x['validation']\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y['train'],\n",
    "                    epochs = N_epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    steps_per_epoch = steps_per_epoch,\n",
    "                    validation_data = (x_validation, y['validation']),\n",
    "                    verbose = 1,\n",
    "                    callbacks = [cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the loss as a function of the epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.r_[0 : len(history.history['loss'])] + 1\n",
    "plt.plot(epochs, history.history['loss'], 'k', label='Training')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best model based on the validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_files = glob.glob(checkpoint_path + '/*.h5')\n",
    "val_loss = [float(file[:-3].split('-')[-1]) for file in checkpoint_files]\n",
    "best_checkpoint = checkpoint_files[np.argmin(val_loss)]\n",
    "best_model = tf.keras.models.load_model(best_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the network prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N_dims == 1:\n",
    "    y_prediction = best_model.predict({name: x['test'][i] for i,name in enumerate(var_names)})\n",
    "else:\n",
    "    y_prediction = best_model.predict(x['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Mean Absolute Percentage Error on the CNN prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.squeeze(y['test'].numpy())\n",
    "mape_prediction = tf.keras.losses.mean_absolute_percentage_error(y_test.T, y_prediction.T).numpy()\n",
    "for generator_ID, mape in zip(generator_IDs, mape_prediction):\n",
    "    print(f'MAPE on CNN prediction for generator {generator_ID} ... {mape:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results obtained with the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = y['test'].shape[0] // n_generators\n",
    "fig,ax = plt.subplots(1, n_generators, figsize=(4 * n_generators,4))\n",
    "if n_generators == 1:\n",
    "    ax = [ax]\n",
    "y_max = np.max(y['train'], axis=0)\n",
    "y_min = np.min(y['train'], axis=0)\n",
    "for i in range(n_generators):\n",
    "    limits = [y_min[i], y_max[i]+1]\n",
    "    ax[i].plot(limits, limits, 'g--')\n",
    "    idx = np.arange(i * block_size, (i+1) * block_size)\n",
    "    ax[i].plot(y['test'][i * block_size : (i+1) * block_size, i], \\\n",
    "               y_prediction[i * block_size : (i+1) * block_size, i], 'o', \\\n",
    "               color=[1,.7,1], markersize=4, markerfacecolor='w', markeredgewidth=1)\n",
    "    for j in range(int(limits[0]), int(limits[1])):\n",
    "        idx, = np.where(np.abs(y['test'][i * block_size : (i+1) * block_size, i] - (j + 1/3)) < 1e-3)\n",
    "        m = np.mean(y_prediction[idx + i * block_size,i])\n",
    "        s = np.std(y_prediction[idx + i * block_size,i])\n",
    "        ax[i].plot(j+1/3 + np.zeros(2), m + s * np.array([-1,1]), 'm-', linewidth=2)\n",
    "        ax[i].plot(j+1/3, m, 'ms', markersize=8, markerfacecolor='w', \\\n",
    "                markeredgewidth=2)\n",
    "    ax[i].axis([1.8, limits[1], 1.8, limits[1]])\n",
    "    ax[i].set_xlabel('Expected value')\n",
    "    ax[i].set_title(f'Generator {generator_IDs[i]}')\n",
    "ax[0].set_ylabel('Predicted value')\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
