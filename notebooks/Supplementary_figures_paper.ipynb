{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import shelve\n",
    "import numpy as np\n",
    "from scipy.fft import fft, fftfreq\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib.colors import LogNorm, FuncNorm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.ticker import FixedLocator, NullLocator, FixedFormatter\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "fontsize = 7\n",
    "lw = 0.75\n",
    "\n",
    "matplotlib.rc('font', **{'family': 'Times New Roman', 'size': fontsize})\n",
    "matplotlib.rc('axes', **{'linewidth': 0.75, 'labelsize': fontsize})\n",
    "matplotlib.rc('xtick', **{'labelsize': fontsize})\n",
    "matplotlib.rc('ytick', **{'labelsize': fontsize})\n",
    "matplotlib.rc('xtick.major', **{'width': lw, 'size':3})\n",
    "matplotlib.rc('ytick.major', **{'width': lw, 'size':3})\n",
    "matplotlib.rc('ytick.minor', **{'width': lw, 'size':1.5})\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from dlml.data import load_data_areas, load_data_slide\n",
    "from dlml.utils import collect_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe2654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlations(R_mean, R_ctrl_mean, edges, ax, sort_freq=1.0,\n",
    "                      vmin=None, vmax=None, legend_bbox=[0.4, -0.05]):\n",
    "    edge = np.abs(edges - sort_freq).argmin()\n",
    "    kdx = np.argsort(R_mean[edge,:])\n",
    "    R_mean = R_mean[:,kdx]\n",
    "    R_ctrl_mean = R_ctrl_mean[:,kdx]\n",
    "\n",
    "    make_symmetric = False\n",
    "    if vmin is None:\n",
    "        vmin = min([r.min() for r in R_mean])\n",
    "        make_symmetric = True\n",
    "    if vmax is None:\n",
    "        vmax = max([r.max() for r in R_mean])\n",
    "        if make_symmetric:\n",
    "            if vmax > np.abs(vmin):\n",
    "                vmin = -vmax\n",
    "            else:\n",
    "                vmax = -vmin\n",
    "    print(f'Color bar bounds: ({vmin:.2f},{vmax:.2f}).')\n",
    "    ticks = np.linspace(vmin, vmax, 7)\n",
    "    ticklabels = [f'{tick:.2f}' for tick in ticks]\n",
    "\n",
    "    cmap = plt.get_cmap('bwr')\n",
    "    x = np.arange(R_mean.shape[-1])\n",
    "    y = edges[:-1] + np.diff(edges) / 2\n",
    "    im = ax[0].pcolormesh(x, y, R_mean, vmin=vmin, vmax=vmax, shading='auto', cmap=cmap)\n",
    "    i = 0\n",
    "    ax[i].set_xticks(np.linspace(0, x[-1], 3, dtype=np.int32))\n",
    "    if len(ax) == 3:\n",
    "        i += 1\n",
    "        ax[i].pcolormesh(x, y, R_ctrl_mean, vmin=vmin, vmax=vmax, shading='auto', cmap=cmap)\n",
    "        ax[i].set_xticks(np.linspace(0, x[-1], 3, dtype=np.int32))\n",
    "    cbar = plt.colorbar(im, fraction=0.1, shrink=1, aspect=20, label='Correlation',\n",
    "                        orientation='vertical', ax=ax[i], ticks=ticks)\n",
    "    cbar.ax.set_yticklabels(ticklabels, fontsize=fontsize-1)\n",
    "    R_abs_mean = np.mean(np.abs(R_mean), axis=1)\n",
    "    R_ctrl_abs_mean = np.mean(np.abs(R_ctrl_mean), axis=1)\n",
    "    ax[i+1].plot(R_abs_mean, y, 'r', lw=1, label='Tr.')\n",
    "    ax[i+1].plot(R_ctrl_abs_mean, y, 'g--', lw=1, label='Untr.')\n",
    "    ax[i+1].plot(R_abs_mean - R_ctrl_abs_mean, y, 'k', lw=1, label='Diff.')\n",
    "    ax[i+1].legend(loc='lower left', bbox_to_anchor=legend_bbox, frameon=False, fontsize=fontsize-1)\n",
    "\n",
    "    for a in ax:\n",
    "        a.set_ylim(edges[[0,-2]])\n",
    "        a.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34246e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subplots(N, edges, norm_std, F, Xf, exact_momentum, pred_momentum, pred_momentum_ctrl,\n",
    "                  MAPE, R_mean, R_ctrl_mean, corr_edges, ax):\n",
    "    target_values = np.unique(exact_momentum)\n",
    "    \n",
    "    tab10 = plt.get_cmap('tab10')\n",
    "    green, magenta = tab10(2), tab10(6)\n",
    "    cmap = lambda i: (green, magenta)[i%2]\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[0].plot(edges[i][1:], N[i], lw=1, color=cmap(i))\n",
    "        ax[1].plot(F, 20*np.log10(Xf[i]), lw=1, color=cmap(i))\n",
    "\n",
    "    ax[0].grid(which='major', axis='x', lw=0.5, ls=':', color=[.6,.6,.6])\n",
    "    ax[0].set_xlim([-4.5, 4.5])\n",
    "    ax[0].set_xticks(np.r_[-4 : 4.5 : 2])\n",
    "    ax[0].set_ylabel('PDF')\n",
    "    ax[0].set_xlabel('Normalized V')\n",
    "    ticks = np.r_[0 : 0.61 : 0.2]\n",
    "    ax[0].set_ylim(ticks[[0,-1]])\n",
    "    ax[0].yaxis.set_major_locator(FixedLocator(ticks))\n",
    "    ax[0].yaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in ticks]))\n",
    "    for i in range(2):\n",
    "        ax[0].text(-4, 0.6-i*0.1*np.diff(ax[0].get_ylim()), f'STD={norm_std[i]:.2f}', color=cmap(i),\n",
    "                   fontsize=fontsize-1, va='top')\n",
    "\n",
    "    ax[1].set_ylim([-55, -10])\n",
    "    ax[1].set_yticks(np.r_[-50 : -9 : 10])\n",
    "    ax[1].set_xscale('log')\n",
    "    ax[1].set_xlabel('Frequency [Hz]')\n",
    "    ax[1].set_ylabel('Power [dB]')\n",
    "    f_ticks = np.array([0.1, 0.2, 0.5, 1, 2, 5, 10, 20])\n",
    "    ax[1].set_xlim(f_ticks[[0,-1]] + np.array([0,1]))\n",
    "    ax[1].xaxis.set_major_locator(FixedLocator(f_ticks))\n",
    "    ax[1].xaxis.set_minor_locator(NullLocator())\n",
    "    ax[1].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in f_ticks]))\n",
    "    ax[1].grid(which='major', axis='y', lw=0.5, ls=':', color=[.6,.6,.6])\n",
    "    for i,v in enumerate(target_values):\n",
    "        ax[1].text(0.12, -48-i*0.1*np.diff(ax[1].get_ylim()),\n",
    "                   r'M={:.2f} GW$\\cdot$s$^2$'.format(v),\n",
    "                   color=cmap(i), fontsize=fontsize-1)\n",
    "\n",
    "    df = pd.DataFrame(data={'Exact': exact_momentum, 'Pred': np.concatenate(pred_momentum)})\n",
    "    df_ctrl = pd.DataFrame(data={'Exact': exact_momentum, 'Pred': np.concatenate(pred_momentum_ctrl)})\n",
    "    sns.violinplot(x='Exact', y='Pred', data=df, cut=0, inner='quartile',\n",
    "                   palette=[cmap(0), cmap(1)], ax=ax[2], linewidth=0.5)\n",
    "    ax[2].xaxis.set_major_locator(FixedLocator([0, 1]))\n",
    "    ax[2].xaxis.set_major_formatter(FixedFormatter([f'{tick:.2f}' for tick in target_values]))\n",
    "    ax[2].yaxis.set_major_locator(FixedLocator(target_values))\n",
    "    ax[2].yaxis.set_major_formatter(FixedFormatter([f'{tick:.2f}' for tick in target_values]))\n",
    "    ax[2].text(np.mean(ax[2].get_xlim()), target_values.mean(), f'MAPE={MAPE:.1f}%',\n",
    "               ha='center', va='center', fontsize=fontsize-1)\n",
    "\n",
    "    plot_correlations(R_mean, R_ctrl_mean, corr_edges, sort_freq=[1.1], ax=ax[3:])\n",
    "    for i in (3,4):\n",
    "        ax[i].set_ylim(f_ticks[[0,-1]] + np.array([0,1]))\n",
    "        ax[i].yaxis.set_major_locator(FixedLocator(f_ticks))\n",
    "        ax[i].yaxis.set_minor_locator(NullLocator())\n",
    "        if i == 3:\n",
    "            ax[i].yaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in f_ticks]))\n",
    "        else:\n",
    "            ax[i].set_yticklabels([])\n",
    "    ax[3].set_ylabel('Frequency [Hz]')\n",
    "    ax[3].set_xlabel('Filter #')\n",
    "    ax[4].set_xlabel('Correlation')\n",
    "\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b975fe0a",
   "metadata": {},
   "source": [
    "## Figure S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_experiment_IDs = False\n",
    "\n",
    "if find_best_experiment_IDs:\n",
    "    from dlml.utils import collect_experiments\n",
    "    area_measure = 'momentum'\n",
    "    stoch_load_bus_IDs = []\n",
    "    rec_bus_IDs = [3]\n",
    "    H_G1, D, DZA = None, None, None # 500, 2, 0\n",
    "    best_experiment_IDs = {1: {'Vd_bus3': '474d2016e33b441889ce8b17531487cb',\n",
    "                               'Vq_bus3': '617188cbcaef4816a8853081fd303ac1'}}\n",
    "    for area_ID in (1,2):\n",
    "        if area_ID not in best_experiment_IDs:\n",
    "            best_experiment_IDs[area_ID] = {}\n",
    "        for var_name in (f'Vd_bus{rec_bus_IDs[0]}', f'Vq_bus{rec_bus_IDs[0]}'):\n",
    "            if var_name not in best_experiment_IDs[area_ID]:\n",
    "                additional_tags = ['ReLU_none', 'converted_from_PowerFactory', 'all_stoch_loads',\n",
    "                                   var_name.split('_')[0]]\n",
    "                expts = collect_experiments(area_ID, area_measure=area_measure, D=D, DZA=DZA, \\\n",
    "                                            stoch_load_bus_IDs=stoch_load_bus_IDs, H_G1=H_G1, \\\n",
    "                                            rec_bus_IDs=rec_bus_IDs, additional_tags=additional_tags, \\\n",
    "                                            verbose=False)\n",
    "                if expts is None or len(expts) == 0:\n",
    "                    continue\n",
    "                expt_IDs = list(expts.keys())\n",
    "                expt_ID = expt_IDs[np.argmin([expt['val_loss'].min() for expt in expts.values()])]\n",
    "                MAPE = expts[expt_ID]['MAPE']\n",
    "                loss = expts[expt_ID]['loss']\n",
    "                val_loss = expts[expt_ID]['val_loss']\n",
    "                batch_loss = expts[expt_ID]['batch_loss']\n",
    "                tags = expts[expt_ID]['tags']\n",
    "                best_experiment_IDs[area_ID][var_name] = expt_ID\n",
    "                print(f'The best experiment is {expt_ID[:6]} ' + \\\n",
    "                      f'(val_loss = {val_loss.min():.4f}, ' + \\\n",
    "                      f'MAPE = {MAPE:.4f}%).')\n",
    "else:\n",
    "    best_experiment_IDs = {1: {'Vd_bus3': '474d2016e33b441889ce8b17531487cb',\n",
    "                               'Vq_bus3': '617188cbcaef4816a8853081fd303ac1'}, #'4c8bfb605ed74ea2935f3c3ce416b52d'\n",
    "                           2: {'Vd_bus3': 'fb6e5dd5df00455fb12c97d0daf77d84',\n",
    "                               'Vq_bus3': 'f19eac813b484b8c8708ebde2cd5b2c0'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e25044",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4\n",
    "fig,ax = plt.subplots(rows, 5, width_ratios=[1, 1.8, 1, 2.2, 1], figsize=(16.5/2.54, 3.75/2.54*rows))\n",
    "\n",
    "force = False\n",
    "k = 0\n",
    "for area_ID in (1,2):\n",
    "    for var_name in ('Vd_bus3','Vq_bus3'):\n",
    "        experiment_ID = best_experiment_IDs[area_ID][var_name]\n",
    "        data_file = f'hist_spectra_acc_corr_area_{area_ID}_{var_name}_{experiment_ID[:6]}.npz'\n",
    "        if force or not os.path.isfile(data_file):\n",
    "            set_name = 'training'\n",
    "            data_dir = '/home/daniele/Research/deep-power/data/IEEE39/converted_from_PowerFactory/' + \\\n",
    "                f'all_stoch_loads/var_H_area_{area_ID}_comp_grid/subset_2'\n",
    "            data_files = sorted(glob.glob(os.path.join(data_dir, '*' + set_name + '*.h5')))\n",
    "            generators_areas_map = [['G02','G03','Comp11'],\n",
    "                             ['G04','G05','G06','G07','Comp21'],\n",
    "                             ['G08','G09', 'G10','Comp31'],\n",
    "                             ['G01']]\n",
    "            generators_Pnom = {'G01': 10000e6, 'G02': 700e6, 'G03': 800e6, 'G04': 800e6, 'G05': 300e6,\n",
    "                        'G06': 800e6, 'G07': 700e6, 'G08': 700e6, 'G09': 1000e6, 'G10': 1000e6,\n",
    "                        'Comp11': 100e6, 'Comp21': 100e6, 'Comp31': 100e6}\n",
    "\n",
    "            ret = load_data_areas({set_name: data_files}, [var_name],\n",
    "                                  [generators_areas_map[ID-1] for ID in [area_ID]],\n",
    "                                  generators_Pnom,\n",
    "                                  area_measure='momentum',\n",
    "                                  trial_dur=60,\n",
    "                                  max_block_size=10000,\n",
    "                                  use_tf=False,\n",
    "                                  add_omega_ref=True,\n",
    "                                  use_fft=False)\n",
    "\n",
    "            t = ret[0]\n",
    "            X_raw = ret[1][set_name]\n",
    "            y = ret[2][set_name]\n",
    "            group_index = [np.where(y == mom)[0] for mom in np.unique(y)]\n",
    "            n_mom_groups = len(group_index)\n",
    "            X_mean, X_std = X_raw.mean(axis=(1,2)), X_raw.std(axis=(1,2))\n",
    "            X = (X_raw - X_mean) / X_std\n",
    "            X = X.squeeze()\n",
    "            y = y.squeeze()\n",
    "            norm_std = [X[idx].std() for idx in group_index]\n",
    "\n",
    "            dt = np.diff(t[:2])[0]\n",
    "            N_samples = t.size\n",
    "            Xf = fft(X)\n",
    "            Xf = 2.0 / N_samples * np.abs(Xf[:, :N_samples//2])\n",
    "            F = fftfreq(N_samples, dt)[:N_samples//2]\n",
    "\n",
    "            Xf = [Xf[idx,:].mean(axis=0) for idx in group_index]\n",
    "            N,edges = zip(*(np.histogram(X[idx,:], bins=50, density=True) for idx in group_index))\n",
    "\n",
    "            experiments_path = '../experiments/neural_network/'\n",
    "            test_results = pickle.load(open(os.path.join(experiments_path, experiment_ID,\n",
    "                                                         'test_results.pkl'), 'rb'))\n",
    "            MAPE = test_results['mape_prediction'][0]\n",
    "            N_bands = 60\n",
    "            filter_order = 6\n",
    "            if experiment_ID == '474d2016e33b441889ce8b17531487cb':\n",
    "                N_trials = 4000\n",
    "            else:\n",
    "                N_trials = 1000\n",
    "            correlations_file = f'correlations_{experiment_ID[:6]}_{N_bands}-bands_64-filters_' + \\\n",
    "                f'36-neurons_{N_trials}-trials_{filter_order}-butter_{var_name}_pool_1_3.npz'\n",
    "            correlations = np.load(os.path.join(experiments_path, experiment_ID, correlations_file))\n",
    "            R, R_ctrl = correlations['R'], correlations['R_ctrl']\n",
    "            R[correlations['p'] > 0.05] = np.nan\n",
    "            R_ctrl[correlations['p_ctrl'] > 0.05] = np.nan\n",
    "            R_mean = np.nanmean(R, axis=0)\n",
    "            R_ctrl_mean = np.nanmean(R_ctrl, axis=0)\n",
    "\n",
    "            np.savez_compressed(data_file, N=N, edges=edges, F=F, Xf=Xf, norm_std=norm_std,\n",
    "                                exact_momentum=correlations['exact_momentum'],\n",
    "                                pred_momentum=correlations['pred_momentum'],\n",
    "                                pred_momentum_ctrl=correlations['pred_momentum_ctrl'], MAPE=MAPE,\n",
    "                                R_mean=R_mean, R_ctrl_mean=R_ctrl_mean, corr_edges=correlations['edges'])\n",
    "\n",
    "        data = np.load(data_file)\n",
    "        for key in data.files:\n",
    "            exec(f'{key} = data[\"{key}\"]')\n",
    "\n",
    "        plot_subplots(N, edges, norm_std, F, Xf, exact_momentum, pred_momentum, pred_momentum_ctrl,\n",
    "              MAPE, R_mean, R_ctrl_mean, corr_edges, ax[k,:])\n",
    "        ax[k,2].set_title('Area {}, {}'.format(area_ID, var_name.replace('_',' @ ')), fontsize=fontsize+1)\n",
    "        xl,yl = ax[k,3].get_xlim(), ax[k,3].get_ylim()\n",
    "        x,y = xl[0] + np.diff(xl)/20, np.logspace(np.log10(yl[0]), np.log10(yl[1]), 20)[1]\n",
    "        ax[k,3].text(x, y, experiment_ID[:6], fontsize=fontsize+1, color='k')\n",
    "        k += 1\n",
    "\n",
    "only_first = True\n",
    "if only_first:\n",
    "    trans = mtransforms.ScaledTranslation(-0.4, -0.05, fig.dpi_scale_trans)\n",
    "else:\n",
    "    trans = mtransforms.ScaledTranslation(-0.3, -0.05, fig.dpi_scale_trans)\n",
    "for i,label in enumerate('ABCD'):\n",
    "    if only_first:\n",
    "        ax[i,0].text(0.0, 1.0, label, transform=ax[i,0].transAxes + trans, fontsize=fontsize+2, va='bottom')\n",
    "    else:\n",
    "        for j in range(5):\n",
    "            ax[i,j].text(0.0, 1.0, label+str(j+1), transform=ax[i,j].transAxes + trans,\n",
    "                         fontsize=fontsize, va='bottom')\n",
    "fig.tight_layout(pad=0.3)\n",
    "plt.savefig('hist_spectra_acc_corr_supp.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0b29f",
   "metadata": {},
   "source": [
    "## Figure S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8616608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ID = '474d2016e33b441889ce8b17531487cb'\n",
    "N_bands = 20, 40, 60\n",
    "rows = len(N_bands)\n",
    "filter_order = 6\n",
    "N_trials = 4000\n",
    "experiments_path = '../experiments/neural_network'\n",
    "var_name = 'Vd_bus3'\n",
    "\n",
    "offset = np.array([[0.11, 0.1], [0.07, 0.025]])\n",
    "space = [0.12, 0.05]\n",
    "rows, cols = 3, 3\n",
    "height = (1 - offset[:,1].sum() - space[1]*(rows-1)) / rows\n",
    "width = (1 - offset[:,0].sum() - space[0]*(cols-1)) / (cols+2)\n",
    "fig = plt.figure(figsize=(9/2.54, 3/2.54*rows))\n",
    "ax = []\n",
    "for i in range(rows):\n",
    "    ax.append([])\n",
    "    for j in range(cols):\n",
    "        x = offset[0,0] + j*(space[0] + width*2)\n",
    "        if j == 1:\n",
    "            x -= space[0]/1.5\n",
    "        y = 1 - offset[1,1] - (i+1)*height - i*space[1]\n",
    "        if j == 0:\n",
    "            w = 2 * width\n",
    "        elif j == 1:\n",
    "            w = 2.1 * width\n",
    "        else:\n",
    "            w = width\n",
    "        h = height\n",
    "        a = plt.axes([x, y, w, h])\n",
    "        ax[i].append(a)\n",
    "ax = np.array(ax)\n",
    "\n",
    "for i,N in enumerate(N_bands):\n",
    "    correlations_file = f'correlations_{experiment_ID[:6]}_{N}-bands_64-filters_' + \\\n",
    "        f'36-neurons_{N_trials}-trials_{filter_order}-butter_{var_name}_pool_1_3'\n",
    "    if not os.path.isfile(os.path.join(experiments_path, experiment_ID, correlations_file + '_small.npz')):\n",
    "        correlations = np.load(os.path.join(experiments_path, experiment_ID, correlations_file + '.npz'))\n",
    "        R, R_ctrl = correlations['R'], correlations['R_ctrl']\n",
    "        R[correlations['p'] > 0.05] = np.nan\n",
    "        R_ctrl[correlations['p_ctrl'] > 0.05] = np.nan\n",
    "        R_mean = np.nanmean(R, axis=0)\n",
    "        R_ctrl_mean = np.nanmean(R_ctrl, axis=0)\n",
    "        data = {'R_mean': R_mean, 'R_ctrl_mean': R_ctrl_mean}\n",
    "        for k in 'edges', 'exact_momentum', 'pred_momentum', 'pred_momentum_ctrl':\n",
    "            data[k] = correlations[k]\n",
    "        np.savez_compressed(os.path.join(experiments_path, experiment_ID,\n",
    "                                         correlations_file + '_small.npz'), **data)\n",
    "    data = np.load(os.path.join(experiments_path, experiment_ID, correlations_file + '_small.npz'))\n",
    "    plot_correlations(data['R_mean'], data['R_ctrl_mean'], data['edges'], sort_freq=[1.1], ax=ax[i,:])\n",
    "    f_ticks = np.array([0.1, 0.2, 0.5, 1, 2, 5, 10, 20])\n",
    "    for j in range(3):\n",
    "        ax[i,j].set_ylim(f_ticks[[0,-1]] + np.array([0,1]))\n",
    "        ax[i,j].yaxis.set_major_locator(FixedLocator(f_ticks))\n",
    "        ax[i,j].yaxis.set_minor_locator(NullLocator())\n",
    "        if j != 1:\n",
    "            ax[i,j].yaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in f_ticks]))\n",
    "        else:\n",
    "            ax[i,j].set_yticklabels([])\n",
    "        if i != rows-1:\n",
    "            ax[i,j].set_xticklabels([])\n",
    "    ax[i,0].set_ylabel('Frequency [Hz]')\n",
    "    ax[i,1].text(60, 10, f'# bands: {N}', fontsize=fontsize+1, ha='right')\n",
    "\n",
    "ax[-1,-1].set_xlabel('Correlation')\n",
    "ax[-1,0].set_xlabel('Filter #')\n",
    "ax[-1,1].set_xlabel('Filter #')\n",
    "sns.despine()\n",
    "\n",
    "trans = mtransforms.ScaledTranslation(-0.35, -0.05, fig.dpi_scale_trans)\n",
    "for i,label in enumerate('ABC'):\n",
    "    ax[i,0].text(0.0, 1.0, label, transform=ax[i,0].transAxes + trans, fontsize=fontsize+2, va='bottom')\n",
    "        \n",
    "plt.savefig('corrs_vs_Nbands_supp.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0bf40",
   "metadata": {},
   "source": [
    "## Figure S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b037f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function computes the output size of a preprocessing pipeline given\n",
    "# its kernel size(s), kernel stride(s) and number of units in the pooling layer\n",
    "def compute_output_size(N_inputs, kernel_size, kernel_stride, pool_size, verbose=False):\n",
    "    f = lambda n_inp, k_sz, k_strd, pool_sz: ((n_inp - k_sz) // k_strd + 1) // pool_sz\n",
    "    n_char = int(np.ceil(np.log10(N_inputs)))\n",
    "    fmt = '{{:{0}d}} -> {{:{0}d}}'.format(n_char)\n",
    "    for i,(ksz,kstr,ps) in enumerate(zip(kernel_size,kernel_stride,pool_size)):\n",
    "        N_outputs = f(N_inputs, ksz, kstr, ps)\n",
    "        if verbose: print(fmt.format(N_inputs, N_outputs))\n",
    "        N_inputs = N_outputs\n",
    "    return N_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54829f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dur = 60  # [s]\n",
    "srate = 40      # [Hz]\n",
    "N_samples = trial_dur * srate\n",
    "units = {'conv': [16, 32, 64], 'pool': [4, 4, 4], 'dense': [64]}\n",
    "kernel = {'sizes': [5, 5, 5], 'strides': [1, 1, 1]}\n",
    "dfs = []\n",
    "\n",
    "kernel_sizes = 3, 5, 7, 9\n",
    "kernel_strides = 1, 2\n",
    "pool_sizes = 2, 4, 6, 8\n",
    "\n",
    "metrics = 'MAPE','val_loss','loss','batch_loss'\n",
    "time_interval = [datetime(2023, 4, 4), datetime(2023, 4, 7)]\n",
    "output_size = {}\n",
    "for kernel_size in kernel_sizes:\n",
    "    kernel['sizes'] = [kernel_size for _ in range(len(kernel['sizes']))]\n",
    "    output_size[kernel_size] = {}\n",
    "    for kernel_stride in kernel_strides:\n",
    "        kernel['strides'] = [kernel_stride for _ in range(len(kernel['strides']))]\n",
    "        output_size[kernel_size][kernel_stride] = {}\n",
    "        for pool_size in pool_sizes:\n",
    "            units['pool'] = [pool_size for _ in range(len(units['pool']))]\n",
    "            out_sz = compute_output_size(N_samples, kernel['sizes'], kernel['strides'], units['pool'])\n",
    "            output_size[kernel_size][kernel_stride][pool_size] = out_sz\n",
    "            if out_sz == 0:\n",
    "                print('Combination (kernel_size,kernel_stride,pool_size) = ({},{},{}) is unfeasible'.\n",
    "                     format(kernel_size, kernel_stride, pool_size))\n",
    "                continue\n",
    "            additional_tags = ['ReLU_none', 'converted_from_PowerFactory', 'all_stoch_loads',\n",
    "                               'data_subset', 'Vd', 'trial_dur_{}'.format(trial_dur)]\n",
    "            for k,v in units.items():\n",
    "                tag = 'N_' + k + '_units_' + '_'.join(map(str, v))\n",
    "                additional_tags.append(tag)\n",
    "            for k,v in kernel.items():\n",
    "                tag = 'kernel_' + k + '_' + '_'.join(map(str, v))\n",
    "                additional_tags.append(tag)\n",
    "            experiments = collect_experiments(area_IDs=[1],\n",
    "                                              area_measure='momentum',\n",
    "                                              rec_bus_IDs=[3],\n",
    "                                              additional_tags=additional_tags,\n",
    "                                              time_interval=time_interval,\n",
    "                                              full_metrics=False, verbose_level=0)\n",
    "            if experiments is None:\n",
    "                print('No experiments with (pool_size,kernel_size,kernel_stride) = ({},{},{})'.\n",
    "                     format(pool_size, kernel_size, kernel_stride))\n",
    "                continue\n",
    "            experiment_IDs = list(experiments.keys())\n",
    "            metric_values = np.array([[experiments[ID][metric] for ID in experiment_IDs]\n",
    "                                      for metric in metrics]).T\n",
    "            rows,_ = np.where(np.isnan(metric_values))\n",
    "            idx = np.setdiff1d(np.arange(metric_values.shape[0]), np.unique(rows))\n",
    "            n = idx.size\n",
    "            data = {'pool_size':     [pool_size     for _ in range(n)],\n",
    "                    'kernel_size':   [kernel_size   for _ in range(n)],\n",
    "                    'kernel_stride': [kernel_stride for _ in range(n)],\n",
    "                    'output_size':   [out_sz        for _ in range(n)]}\n",
    "            for i,metric in enumerate(metrics):\n",
    "                data[metric] = metric_values[idx,i]\n",
    "            df = pd.DataFrame(data=data, index=[experiment_IDs[i] for i in idx])\n",
    "            dfs.append(df)\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 1\n",
    "cols = 2\n",
    "\n",
    "fig,ax = plt.subplots(rows, cols, figsize=(6.5*cols/2.54, 5*rows/2.54), sharex=True, squeeze=False)\n",
    "\n",
    "for i in range(cols):\n",
    "    stride = i+1\n",
    "    sns.violinplot(data=df[df.kernel_stride==stride], x='kernel_size', y='val_loss',\n",
    "                   hue='pool_size', palette='Set2', cut=0, inner='quartile', scale='count',\n",
    "                   linewidth=0.75, ax=ax[0,i])\n",
    "    if rows > 1:\n",
    "        sns.violinplot(data=df[df.kernel_stride==stride], x='kernel_size', y='MAPE',\n",
    "                       hue='pool_size', palette='Set2', cut=0, inner='quartile', scale='count',\n",
    "                       linewidth=0.75, ax=ax[1,i])\n",
    "    ax[0,i].set_title(f'Stride = {i+1}', fontsize=fontsize+2)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "ax[0,0].set_ylabel('Validation loss')\n",
    "if rows == 1:\n",
    "    for i in range(cols):\n",
    "        ax[0,i].set_xlabel('Kernel size')\n",
    "    if cols == 2:\n",
    "        ax[0,1].legend_.set_visible(False)\n",
    "        ax[0,1].set(ylabel=None)\n",
    "    lgnd = ax[0,0].legend_\n",
    "else:\n",
    "    for i in range(cols):\n",
    "        ax[0,i].legend_.set_visible(False)\n",
    "        ax[0,i].set(xlabel=None)\n",
    "        ax[1,i].set_xlabel('Kernel size')\n",
    "    if cols == 2:\n",
    "        for i in range(rows):\n",
    "            ax[i,1].set(ylabel=None)\n",
    "        ax[1,1].legend_.set_visible(False)\n",
    "    lgnd = ax[1,0].legend_\n",
    "    ax[1,0].set_ylabel('MAPE [%]')\n",
    "lgnd.set_frame_on(False)\n",
    "lgnd.set_title('Pool size')\n",
    "lgnd.set_bbox_to_anchor((0.85, 0.5))\n",
    "\n",
    "if rows == 1:\n",
    "    ticks = np.logspace(np.log10(0.002), np.log10(0.02), 7)\n",
    "else:\n",
    "    ticks = np.logspace(np.log10(0.002), np.log10(0.02), 4)\n",
    "for i in range(cols):\n",
    "    ax[0,i].set_yscale('log')\n",
    "    ax[0,i].set_ylim(ticks[[0,-1]] + np.array([-0.0001, 0.001]))\n",
    "    ax[0,i].yaxis.set_major_locator(FixedLocator(ticks))\n",
    "    ax[0,i].yaxis.set_minor_locator(NullLocator())\n",
    "    if i == 0:\n",
    "        ax[0,i].yaxis.set_major_formatter(FixedFormatter([f'{tick:.3f}' for tick in ticks]))\n",
    "    else:\n",
    "        ax[0,i].set_yticklabels([])\n",
    "\n",
    "if rows > 1:\n",
    "    ticks = np.logspace(np.log10(0.5), np.log10(9), 4)\n",
    "    for i in range(cols):\n",
    "        ax[1,i].set_yscale('log')\n",
    "        ax[1,i].set_ylim(ticks[[0,-1]] + np.array([-0.05, 0.5]))\n",
    "        ax[1,i].yaxis.set_major_locator(FixedLocator(ticks))\n",
    "        if i == 0:\n",
    "            ax[1,i].yaxis.set_major_formatter(FixedFormatter([f'{tick:.0f}' for tick in ticks]))\n",
    "        else:\n",
    "            ax[1,i].set_yticklabels([])\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        ax[i,j].grid(which='major', axis='y', ls=':', lw=0.5, color=[.6,.6,.6])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(f'hyperpars_opt_supp.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a254c",
   "metadata": {},
   "source": [
    "### Table S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hyperpars_table.tex', 'w') as fid:\n",
    "    fid.write('\\\\begin{table}[tb!]\\n')\n",
    "    fid.write('\\\\begin{center}\\n')\n",
    "    fid.write('\\\\caption{Hyperparameters impact on \\\\ac{cnn} accuracy\\n')\n",
    "    fid.write('\\\\label{tab:hyperpars}}\\n')\n",
    "    fid.write('\\\\begin{tabular}{ccccc}\\n')\n",
    "    fid.write('\\\\toprule\\n')\n",
    "    fid.write('\\\\thead{Kernel size} & \\\\thead{Kernel stride} & \\\\thead{Pooling size} & \\n')\n",
    "    fid.write('\\\\thead{Output size} & \\\\thead{Median val. loss (n=10)} \\\\\\\\\\n')\n",
    "    fid.write('\\\\midrule\\n')\n",
    "    for j,psz in enumerate(pool_sizes):\n",
    "        for i,ksz in enumerate(kernel_sizes):\n",
    "            for kstrd in kernel_strides:\n",
    "                out_sz = output_size[ksz][kstrd][psz]\n",
    "                if out_sz > 0:\n",
    "                    idx, = np.where((df.kernel_size == ksz) &\n",
    "                                    (df.kernel_stride == kstrd) &\n",
    "                                    (df.pool_size == psz))\n",
    "                    val_loss = np.median(df.iloc[idx,df.columns.get_loc('val_loss')].to_numpy())\n",
    "                    if psz == 4 and ksz == 5 and kstrd == 1:\n",
    "                        fid.write('\\\\blue{{{}}} & \\\\blue{{{}}} & \\\\blue{{{}}} & \\\\blue{{{:4d}}} & \\\\blue{{{:7.5f}}} \\\\\\\\\\n'.\n",
    "                                  format(ksz, kstrd, psz, out_sz, val_loss))\n",
    "                    elif psz == 6 and ksz in (3,5) and kstrd == 1:\n",
    "                        fid.write('\\\\green{{{}}} & \\\\green{{{}}} & \\\\green{{{}}} & \\\\green{{{:4d}}} & \\\\green{{{:7.5f}}} \\\\\\\\\\n'.\n",
    "                                  format(ksz, kstrd, psz, out_sz, val_loss))\n",
    "                    else:\n",
    "                        fid.write('{} & {} & {} & {:4d} & {:7.5f} \\\\\\\\\\n'.\n",
    "                                  format(ksz, kstrd, psz, out_sz, val_loss))\n",
    "    fid.write('\\\\bottomrule\\n')\n",
    "    fid.write('\\\\end{tabular}\\n')\n",
    "    fid.write('\\\\end{center}\\n')\n",
    "    fid.write('\\\\end{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17dbea5",
   "metadata": {},
   "source": [
    "## Figure S4\n",
    "Not sure, maybe it'll be just for a reviewer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2997c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full grid without varying compensator's inertia\n",
    "# experiment_ID = 'f64bde90cab54d1ea770bb21f33c3ed1'\n",
    "# full grid with variable compensator's inertia\n",
    "experiment_ID = 'a40658acee3c4e419c0ee34d0c59f4df'\n",
    "datafile = 'wide_grid_test_' + experiment_ID[:6]\n",
    "df = pd.read_parquet(datafile + '.parquet.gz')\n",
    "\n",
    "#####\n",
    "fig,ax = plt.subplots(1, 1, figsize=(3,2.5))\n",
    "x = [3.33, 5.33, 5.33, 3.33, 3.33]\n",
    "y = [3.47, 3.47, 5.47, 5.47, 3.47]\n",
    "ax.plot(x, y, '-', color=.7+np.zeros(3), lw=1)\n",
    "ax.fill_between(x[:2], y[:2], y[2:4], fc=.8+np.zeros(3), alpha=0.5)\n",
    "\n",
    "coeff = 200\n",
    "for sz in (0.05,0.1,0.2,0.5):\n",
    "    ax.scatter(0, 0, s=sz*coeff, c='k', label=f'{int(sz*100)}%')\n",
    "im = ax.scatter(df['H_G02'], df['H_G03'], s=df['MAPE']*coeff, c=df['MAPE'],\n",
    "           cmap='RdYlBu_r', vmin=0.0, vmax=0.6)\n",
    "ax.legend(loc='best', frameon=False, bbox_to_anchor=(1.3,0.1))\n",
    "\n",
    "cbar = plt.colorbar(im, shrink=0.8)\n",
    "cbar.set_label('MAPE [%]')\n",
    "ticks = np.r_[0 : 0.65 : 0.1]\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.set_ticklabels([f'{tick*100:.0f}' for tick in ticks])\n",
    "\n",
    "ax.set_xlim([1.2,10.8])\n",
    "ax.set_ylim([1.2,10.8])\n",
    "ticks = np.arange(2, 11, 2)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xlabel(r'$H_{G2}$ [s]')\n",
    "ax.set_ylabel(r'$H_{G3}$ [s]')\n",
    "ax.text(1.5, 10.75, experiment_ID[:6], fontsize=fontsize+2)\n",
    "sns.despine()\n",
    "fig.tight_layout(pad=0.1)\n",
    "fig.savefig(datafile + '.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298581bc",
   "metadata": {},
   "source": [
    "## Figure S5\n",
    "Effect of varying the damping coefficient on the estimation of momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf048f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full grid without varying compensator's inertia\n",
    "# experiment_ID = 'f64bde90cab54d1ea770bb21f33c3ed1'\n",
    "# full grid with variable compensator's inertia\n",
    "experiment_ID = 'a40658acee3c4e419c0ee34d0c59f4df'\n",
    "experiments_path = '../experiments/neural_network'\n",
    "network_pars = pickle.load(open(os.path.join(experiments_path, experiment_ID, 'parameters.pkl'), 'rb'))\n",
    "area_ID = network_pars['area_IDs'][0]\n",
    "data_dir = network_pars['data_dirs'][0].format(area_ID)\n",
    "\n",
    "damping_data_file = 'var_D_area_1.npz'\n",
    "force = False\n",
    "if force or not os.path.isfile(damping_data_file):\n",
    "    data_files = sorted(glob.glob(os.path.join('..', data_dir, 'ieee39_*_D*_*.h5')))\n",
    "    D = np.array([float(re.findall('D=\\d.\\d', d)[0].split('=')[1]) for d in data_files])\n",
    "    window_dur = 60,\n",
    "    window_step = window_dur\n",
    "    var_names = network_pars['var_names']\n",
    "    x_train_mean = network_pars['x_train_mean']\n",
    "    x_train_std = network_pars['x_train_std']\n",
    "    data_mean = {var_name: x_train_mean[k] for k,var_name in enumerate(var_names)}\n",
    "    data_std = {var_name: x_train_std[k] for k,var_name in enumerate(var_names)}\n",
    "    Xf = []\n",
    "    for data_file in data_files:\n",
    "        t, _, _, X_slide, _ = load_data_slide([data_file], var_names, data_mean, data_std,\n",
    "                                                   window_dur, window_step, add_omega_ref=False,\n",
    "                                                   verbose=True)\n",
    "        dt = np.diff(t[:2])[0]\n",
    "        Xf.append({})\n",
    "        for var_name in var_names:\n",
    "            N_samples = X_slide[var_name].shape[1]\n",
    "            tmp = fft(X_slide[var_name])\n",
    "            tmp = 2.0 / N_samples * np.abs(tmp[:, :N_samples//2])\n",
    "            Xf[-1][var_name] = tmp.mean(axis=0)\n",
    "            F = fftfreq(N_samples, dt)[:N_samples//2]\n",
    "    data = {'F': F, 'Xf': Xf, 'D': D, 'window_dur': window_dur, 'window_step': window_step,\n",
    "            'var_names': var_names, 'x_train_mean': x_train_mean, 'x_train_std': x_train_std}\n",
    "    np.savez_compressed(damping_data_file, **data)\n",
    "else:\n",
    "    data = np.load(damping_data_file, allow_pickle=True)\n",
    "    D,F,Xf = data['D'], data['F'], data['Xf']\n",
    "    var_names = data['var_names']\n",
    "\n",
    "db = shelve.open(os.path.join(experiments_path, experiment_ID,\n",
    "                              experiment_ID[:6]+'_Pfrac=0.1_KAD=2.0_KW=10.0.out'))\n",
    "experiments = db['experiments'][9:9+len(D)]\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for expt in experiments:\n",
    "    damping = float(re.findall('\\d.\\d', expt['description'])[-1])\n",
    "    M = expt['exact'][0]\n",
    "    M_pred = np.squeeze(expt['prediction'])\n",
    "    idx = np.where(np.logical_not(np.isnan(M_pred)))[0][0]\n",
    "    df = pd.DataFrame(data={'D': damping, 'M': M, 'M_pred': M_pred[idx::60]})\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "M = df.M.unique()[0]\n",
    "df['MAPE'] = (df['M'] - df['M_pred']).abs() / df['M'] * 100\n",
    "df_mean = df.groupby('D').mean()\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bb0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [[.8,.8,.8] for i in range(len(D))]\n",
    "offset = np.array([[0.12,0.1], [0.1,0.05]])\n",
    "space = [0.1, 0.09]\n",
    "w = 1 - np.sum(offset[:,0])\n",
    "h = 0.26\n",
    "h_inset = 1 - np.sum(offset[:,1]) - 2*h - 2*space[1]\n",
    "w_inset = (1 - np.sum(offset[:,0]) - space[0]) / 2\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "ax = [\n",
    "    plt.axes([offset[0,0], 1-offset[1,1]-h, w, h]),\n",
    "    plt.axes([offset[0,0], offset[0,1], w, h]),\n",
    "    plt.axes([offset[0,0], offset[0,1] + h + space[1]/1.7, w_inset, h_inset]),\n",
    "    plt.axes([offset[0,0]+w_inset+space[0], offset[0,1] + h + space[1]/1.7, w_inset, h_inset])\n",
    "]\n",
    "\n",
    "xlim = [-0.5, len(D)-0.5]\n",
    "ax[0].plot(xlim, [M, M], 'r', lw=2, zorder=-1)\n",
    "sns.violinplot(x='D', y='M_pred', data=df, cut=0, inner='quartile',\n",
    "               palette=palette, ax=ax[0], linewidth=1, zorder=0)\n",
    "twin = ax[0].twinx()\n",
    "twin.plot(df_mean['MAPE'], 'ko', ms=7, markerfacecolor='w', markeredgewidth=1.5)\n",
    "ax[0].set_xlim(xlim)\n",
    "ax[0].set_yticks(np.r_[0.2 : 0.25 : 0.01])\n",
    "ax[0].set_xlabel('Damping')\n",
    "ax[0].set_ylabel(r'M [GW$\\cdot$s$^2$]')\n",
    "ax[0].set_ylim([0.2, 0.24])\n",
    "ax[0].set_yticks(np.linspace(0.2, 0.24, 5))\n",
    "twin.set_ylim([1, 4.2])\n",
    "twin.set_yticks(np.linspace(1, 4.2, 5))\n",
    "twin.set_ylabel('MAPE [%]')\n",
    "# ax[0].grid(which='major', axis='y', ls=':', lw=0.5, color=[.6,.6,.6])\n",
    "\n",
    "xlim = [[1e-2,20], [1e-2,1e-1], [0.4,1.5]]\n",
    "ylim = [[-60,0], [-20,0], [-30,-10]]\n",
    "var_name = 'Vd_bus3'\n",
    "cmap = plt.get_cmap('viridis', len(Xf))\n",
    "for i,a in enumerate(ax[1:]):\n",
    "    for k,xf in enumerate(Xf):\n",
    "        a.plot(F, 20*np.log10(xf[var_name]), color=cmap(k), lw=1, label=f'D={k:.1f}')\n",
    "    a.set_xscale('log')\n",
    "    a.set_xlim(xlim[i])\n",
    "    a.set_ylim(ylim[i])\n",
    "    a.set_yticks(np.r_[ylim[i][0] : ylim[i][1]+5 : 20 if i == 0 else 10])\n",
    "    a.grid(which='major', axis='x', lw=0.5, ls=':', color=[.6,.6,.6])\n",
    "ax[1].set_xlabel('Frequency [Hz]')\n",
    "ax[1].set_ylabel('Power [dB]')\n",
    "ax[1].legend(loc='lower left', frameon=False)\n",
    "xticks = np.array([0.01, 0.03, 0.1, 0.3, 1, 3, 10, 20])\n",
    "ax[1].xaxis.set_major_locator(FixedLocator(xticks))\n",
    "ax[1].xaxis.set_minor_locator(NullLocator())\n",
    "ax[1].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in xticks]))\n",
    "ax[2].set_ylabel('Power [dB]')\n",
    "ax[2].xaxis.set_major_locator(FixedLocator(xticks[:3]))\n",
    "ax[2].xaxis.set_minor_locator(NullLocator())\n",
    "ax[2].xaxis.set_major_formatter(FixedFormatter([f'{tick:g}' for tick in xticks[:3]]))\n",
    "xticks = np.logspace(np.log10(xlim[2][0]), np.log10(xlim[2][1]), 4)\n",
    "ax[3].xaxis.set_major_locator(FixedLocator(xticks))\n",
    "ax[3].xaxis.set_minor_locator(NullLocator())\n",
    "ax[3].xaxis.set_major_formatter(FixedFormatter([f'{tick:.1f}' for tick in xticks]))\n",
    "\n",
    "for side in 'right','top':\n",
    "    for a in ax[1:]:\n",
    "        a.spines[side].set_visible(False)\n",
    "        \n",
    "trans = mtransforms.ScaledTranslation(-0.45, 0, fig.dpi_scale_trans)\n",
    "ax[0].text(0.0, 1.0, 'A', transform=ax[0].transAxes+trans, fontsize=10, va='bottom')\n",
    "ax[2].text(0.0, 1.0, 'B', transform=ax[2].transAxes+trans, fontsize=10, va='bottom')\n",
    "\n",
    "fig.savefig('var_D_area_1.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
