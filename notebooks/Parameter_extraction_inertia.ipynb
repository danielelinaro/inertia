{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "from time import strftime, localtime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from deep_utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's fix the seed of the RNG, for reproducibility purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/dev/random', 'rb') as fid:\n",
    "    seed = int.from_bytes(fid.read(4), 'little')\n",
    "tf.random.set_seed(seed)\n",
    "print('Seed: {}'.format(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/IEEE14_D=2_DZA=60.0/'\n",
    "var_names = ('omega_G1', 'Pe_G1')\n",
    "n_vars = len(var_names)\n",
    "inertia = {key: np.arange(2,11) + i/3 for i,key in enumerate(('training', 'test', 'validation'))}\n",
    "time, x, y = load_data(data_folder, inertia, var_names)\n",
    "x['train'] = x.pop('training')\n",
    "y['train'] = y.pop('training')\n",
    "N_vars, N_training_traces, N_samples = x['train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mean = np.mean(x['train'], axis=(1,2))\n",
    "x_train_std = np.std(x['train'], axis=(1,2))\n",
    "for key in x:\n",
    "    x[key] = tf.constant([(x[key][i].numpy() - m) / s for i,(m,s) in enumerate(zip(x_train_mean, x_train_std))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2, 1, figsize=(8,8))\n",
    "for i,a in enumerate(ax):\n",
    "    a.plot(time, tf.transpose(x['train'][i, :10, :]), 'k')\n",
    "    a.plot(time, tf.transpose(x['train'][i, -10:, :]), 'r')\n",
    "ax[0].set_ylabel(r'$\\omega_{\\mathrm{G}_1}$ [Hz]')\n",
    "ax[1].set_ylabel(r'$Pe_{\\mathrm{G}_1}$ [Hz]')\n",
    "ax[1].set_xlabel('Time [s]');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "idx_2, _ = np.where(y['train'] == 2)\n",
    "idx_10, _ = np.where(y['train'] == 10)\n",
    "\n",
    "for i,a in enumerate(ax):\n",
    "    n,edges = np.histogram(np.ndarray.flatten(x['train'][i,:,:].numpy()[idx_2,:]), \\\n",
    "                           bins=100, range=[-4,4], density=True)\n",
    "    lim = n.max()\n",
    "    a.plot(edges[:-1], n, 'k', linewidth=1.2, label='H=2')\n",
    "    n,edges = np.histogram(np.ndarray.flatten(x['train'][i,:,:].numpy()[idx_10,:]), \\\n",
    "                           bins=100, range=[-4,4], density=True)\n",
    "    lim = n.max() if n.max() > lim else lim\n",
    "    a.plot(edges[:-1], n, 'r', linewidth=1.2, label='H=10')\n",
    "    a.plot([0,0], [0, lim*1.05], '--', lw=1, color=[.6,.6,.6])\n",
    "ax[0].set_xlabel(r'$\\omega_{\\mathrm{G}_1}$ [Hz]')\n",
    "ax[1].set_xlabel(r'$Pe_{\\mathrm{G}_1}$ [Hz]')\n",
    "ax[0].set_ylabel('Fraction')\n",
    "ax[1].legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n",
    "The basic network topology used here is taken from the following paper:\n",
    "\n",
    "George, D., & Huerta, E. A. (2018). Deep neural networks to enable real-time multimessenger astrophysics. Physical Review D, 97(4), 044039. http://doi.org/10.1103/PhysRevD.97.044039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing_pipeline_1D(N_samples, N_units, kernel_size, input_name):\n",
    "    inp = keras.Input(shape=(N_samples, 1), name=input_name)\n",
    "    for N_conv,N_pooling,sz in zip(N_units['conv'], N_units['pooling'], kernel_size):\n",
    "        try:\n",
    "            conv = layers.Conv1D(N_conv, sz, activation=None)(relu)\n",
    "        except:\n",
    "            conv = layers.Conv1D(N_conv, sz, activation=None)(inp)\n",
    "        pool = layers.MaxPooling1D(N_pooling)(conv)\n",
    "        relu = layers.ReLU()(pool)\n",
    "    return inp,relu\n",
    "\n",
    "\n",
    "def make_preprocessing_pipeline_2D(N_samples, N_units, kernel_size, input_name):\n",
    "    inp = keras.Input(shape=(N_samples, 2, 1), name=input_name)\n",
    "    for N_conv,N_pooling,sz in zip(N_units['conv'], N_units['pooling'], kernel_size):\n",
    "        try:\n",
    "            conv = layers.Conv2D(N_conv, [sz, 2], padding='same', activation=None)(relu)\n",
    "        except:\n",
    "            conv = layers.Conv2D(N_conv, [sz, 2], padding='same', activation=None)(inp)\n",
    "        pool = layers.MaxPooling2D([N_pooling, 1])(conv)\n",
    "        relu = layers.ReLU()(pool)\n",
    "    return inp,relu\n",
    "\n",
    "# how many dimensions the input data should have\n",
    "N_dims = 1\n",
    "# whether to have a deeper network (2) or not (1)\n",
    "depth_level = 1\n",
    "# a dropout coefficient of 0 means no Dropout layer\n",
    "dropout_coeff = 0\n",
    "# learning rate of the Adam optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "N_units = {}\n",
    "\n",
    "if depth_level == 1:\n",
    "    N_units['conv'] = [16, 32, 64]\n",
    "    N_units['dense'] = [64]\n",
    "elif depth_level == 2:\n",
    "    N_units['conv'] = [64, 128, 256, 512]\n",
    "    N_units['dense'] = [128, 64]\n",
    "\n",
    "N_units['pooling'] = [4 for _ in range(len(N_units['conv']))]\n",
    "kernel_size = [5 for _ in range(len(N_units['conv']))]\n",
    "\n",
    "if N_dims == 1:\n",
    "    inputs = []\n",
    "    relus = []\n",
    "    for var_name in var_names:\n",
    "        inp,rel = make_preprocessing_pipeline_1D(N_samples, N_units, kernel_size, var_name)\n",
    "        inputs.append(inp)\n",
    "        relus.append(rel)\n",
    "else:\n",
    "    inputs,relu = make_preprocessing_pipeline_2D(N_samples, N_units, kernel_size, '_'.join(var_names))\n",
    "    relus = [relu]\n",
    "\n",
    "if len(relus) > 1:\n",
    "    concat = layers.concatenate(relus)\n",
    "    flatten = layers.Flatten()(concat)\n",
    "else:\n",
    "    flatten = layers.Flatten()(relus[0])\n",
    "\n",
    "for i,n in enumerate(N_units['dense']):\n",
    "    if i == 0:\n",
    "        dense = layers.Dense(n, activation='relu')(flatten)\n",
    "    else:\n",
    "        dense = layers.Dense(n, activation='relu')(dense)\n",
    "\n",
    "if dropout_coeff > 0:\n",
    "    drop = layers.Dropout(dropout_coeff)(dense)\n",
    "    output_tensor = layers.Dense(y['train'].shape[1])(drop)\n",
    "else:\n",
    "    output_tensor = layers.Dense(y['train'].shape[1])(dense)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output_tensor)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = strftime('%Y%m%d-%H%M%S', localtime())\n",
    "path = 'experiments/' + ts\n",
    "checkpoint_path = path + '/checkpoints'\n",
    "os.makedirs(checkpoint_path)\n",
    "keras.utils.plot_model(model, show_shapes=True, dpi=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N_dims == 2:\n",
    "    for key in x:\n",
    "        x[key] = tf.transpose(x[key], perm=(1,2,0))\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path + \\\n",
    "                                                 '/weights.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "                                                 save_weights_only=False,\n",
    "                                                 save_best_only=True,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 verbose=0)\n",
    "\n",
    "N_epochs = 5\n",
    "batch_size = 128\n",
    "N_batches = np.ceil(N_training_traces / batch_size)\n",
    "steps_per_epoch = np.max([N_batches, 100])\n",
    "\n",
    "if N_dims == 1:\n",
    "    x_train = {name: x['train'][i] for i,name in enumerate(var_names)}\n",
    "    x_validation = {name: x['validation'][i] for i,name in enumerate(var_names)}\n",
    "else:\n",
    "    x_train = x['train']\n",
    "    x_validation = x['validation']\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y['train'],\n",
    "                    epochs = N_epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    steps_per_epoch = steps_per_epoch,\n",
    "                    validation_data = (x_validation, y['validation']),\n",
    "                    verbose = 1,\n",
    "                    callbacks = [cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the loss as a function of the epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.r_[0 : len(history.history['loss'])] + 1\n",
    "plt.plot(epochs, history.history['loss'], 'k', label='Training')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best model based on the validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_files = glob.glob(checkpoint_path + '/*.h5')\n",
    "val_loss = [float(file[:-3].split('-')[-1]) for file in checkpoint_files]\n",
    "best_checkpoint = checkpoint_files[np.argmin(val_loss)]\n",
    "best_model = tf.keras.models.load_model(best_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the network prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N_dims == 1:\n",
    "    y_prediction = np.squeeze(best_model.predict({name: x['test'][i] for i,name in enumerate(var_names)}))\n",
    "else:\n",
    "    y_prediction = np.squeeze(best_model.predict(x['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Mean Absolute Percentage Error on the CNN prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.squeeze(y['test'].numpy())\n",
    "mape_prediction = tf.keras.losses.mean_absolute_percentage_error(y_test, y_prediction).numpy()\n",
    "print('MAPE on CNN prediction ... {:.2f}%'.format(mape_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results obtained with the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 1, figsize=(7,7))\n",
    "limits = np.squeeze(y['train'].numpy()[[0,-1]])\n",
    "limits[1] += 1\n",
    "ax.plot(limits, limits, 'g--')\n",
    "ax.plot(y['test'], y_prediction, 'o', color=[1,.7,1], markersize=4, \\\n",
    "        markerfacecolor='w', markeredgewidth=1)\n",
    "for i in range(int(limits[0]), int(limits[1])):\n",
    "    idx,_ = np.where(np.abs(y['test'] - (i + 1/3)) < 1e-3)\n",
    "    m = np.mean(y_prediction[idx])\n",
    "    s = np.std(y_prediction[idx])\n",
    "    ax.plot(i+1/3 + np.zeros(2), m + s * np.array([-1,1]), 'm-', linewidth=2)\n",
    "    ax.plot(i+1/3, m, 'ms', markersize=8, markerfacecolor='w', \\\n",
    "            markeredgewidth=2)\n",
    "ax.set_xlabel('Expected value')\n",
    "ax.set_ylabel('Predicted value')\n",
    "ax.axis([1.8, limits[1], 1.8, limits[1]])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "parameters = {'N_samples': N_samples, 'seed': seed, 'with_dropout': with_dropout,\n",
    "              'depth_level': depth_level, 'N_units': N_units, 'kernel_size': kernel_size,\n",
    "              'N_epochs': N_epochs, 'batch_size': batch_size, 'steps_per_epoch': steps_per_epoch,\n",
    "              'mape_prediction': mape_prediction, 'learning_rate': learning_rate, 'y_test': y['test'],\n",
    "              'y_prediction': y_prediction}\n",
    "\n",
    "best_model.save(path)\n",
    "pickle.dump(parameters, open(path + '/parameters.pkl', 'wb'))\n",
    "pickle.dump(history.history, open(path + '/history.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
