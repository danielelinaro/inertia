{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import tables\n",
    "from tqdm import tqdm\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from dlml.utils import collect_experiments\n",
    "from dlml.data import read_area_values, load_data_areas, load_data_slide\n",
    "from dlml.nn import predict, compute_receptive_field, compute_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52309d7",
   "metadata": {},
   "source": [
    "#### A function for plotting the correlations analaysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4038c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlations(R, p, edges, F, Xf, idx, sort_F=1.0, vmin=None, vmax=None):\n",
    "    if p is not None:\n",
    "        R_sig = R.copy()\n",
    "        R_sig[p > 0.05] = 0\n",
    "        R_mean = R_sig.mean(axis=0)\n",
    "    else:\n",
    "        R_mean = R.mean(axis=0)\n",
    "\n",
    "    edge = np.abs(edges - sort_F).argmin()\n",
    "    kdx = np.argsort(R_mean[edge,:])\n",
    "    R_mean = R_mean[:,kdx]\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    gs = fig.add_gridspec(1, 4)\n",
    "    ax = [fig.add_subplot(gs[0,0]), fig.add_subplot(gs[0,1:])]\n",
    "    cmap = plt.get_cmap('tab10', 2)\n",
    "    for j,jdx in enumerate(idx):\n",
    "        mean = Xf[jdx].mean(axis=0)\n",
    "        stddev = Xf[jdx].std(axis=0)\n",
    "        ci = 1.96 * stddev / np.sqrt(jdx.size)\n",
    "        ax[0].fill_betweenx(F, mean + ci, mean - ci, color=cmap(j))\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[0].invert_xaxis()\n",
    "    ax[0].yaxis.tick_right()\n",
    "    cmap = plt.get_cmap('bwr')\n",
    "    make_symmetric = False\n",
    "    if vmin is None:\n",
    "        vmin = R_mean.min()\n",
    "        make_symmetric = True\n",
    "    if vmax is None:\n",
    "        vmax = R_mean.max()\n",
    "        if make_symmetric:\n",
    "            if vmax > np.abs(vmin):\n",
    "                vmin = -vmax\n",
    "            else:\n",
    "                vmax = -vmin\n",
    "    print(f'Color bar bounds: ({vmin:.2f},{vmax:.2f}).')\n",
    "    x = np.arange(R_mean.shape[-1])\n",
    "    y = edges[:-1] + np.diff(edges) / 2\n",
    "    im = ax[1].pcolormesh(x, y, R_mean, vmin=vmin, vmax=vmax, shading='auto', cmap=cmap)\n",
    "    ax[1].set_yscale('log')\n",
    "    cbar = plt.colorbar(im, label='Correlation')\n",
    "    ax[1].set_xlabel('Filter #')\n",
    "    ax[1].set_ylabel('Frequency [Hz]')\n",
    "    for a in ax:\n",
    "        a.set_ylim(edges[[0,-2]])\n",
    "    for side in 'left','top':\n",
    "        ax[0].spines[side].set_visible(False)\n",
    "    ax[0].set_yticklabels([])\n",
    "    for side in 'right','top':\n",
    "        ax[1].spines[side].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    return fig, vmin, vmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36871b",
   "metadata": {},
   "source": [
    "#### Find the best experiment given a set of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b90255",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_ID = 1\n",
    "area_measure = 'momentum'\n",
    "stoch_load_bus_IDs = []\n",
    "rec_bus_IDs = [3]\n",
    "H_G1, D, DZA = None, None, None # 500, 2, 0\n",
    "additional_tags = ['ReLU_none', 'converted_from_PowerFactory', 'all_stoch_loads', 'data_subset']\n",
    "missing_tags = []\n",
    "use_FFT = False\n",
    "if use_FFT:\n",
    "    additional_tags.append('fft')\n",
    "else:\n",
    "    missing_tags.append('fft')\n",
    "pooling_type = ''\n",
    "if pooling_type is not None and pooling_type != '':\n",
    "    additional_tags.append(pooling_type + '_pooling')\n",
    "\n",
    "# training on frequency data, 2 output values\n",
    "# experiment_ID = '9ea493c789b542bf979c51a6031f4044'\n",
    "# training on frequency data, 4 output values\n",
    "# experiment_ID = 'f6d9a03f1cfe450288e9cb86da94235f'\n",
    "# training on time series data, 2 output values\n",
    "experiment_ID = '034a1edb0797475b985f0e1335dab383'\n",
    "# training on time series data, 4 output values\n",
    "# experiment_ID = 'b346a89d384c4db2ba4058a2c83c4f12'\n",
    "# training on time series data, 2 output values, with MaxPooling1DWithArgmax layer\n",
    "# experiment_ID = '9034f8bc4f874c938dfa5f1f9ee04e82'\n",
    "# experiment_ID = None\n",
    "\n",
    "if experiment_ID is not None:\n",
    "    from comet_ml.api import API, APIExperiment\n",
    "    api = API(api_key = os.environ['COMET_API_KEY'])\n",
    "    experiment = api.get_experiment('danielelinaro', 'inertia', experiment_ID)\n",
    "    sys.stdout.write(f'Getting metrics for experiment {experiment_ID[:6]}... ')\n",
    "    sys.stdout.flush()\n",
    "    metrics = experiment.get_metrics()\n",
    "    sys.stdout.write('done.\\n')\n",
    "    val_loss = []\n",
    "    for m in metrics:\n",
    "        if m['metricName'] == 'val_loss':\n",
    "            val_loss.append(float(m['metricValue']))\n",
    "        elif m['metricName'] == 'mape_prediction':\n",
    "            MAPE = float(m['metricValue'])\n",
    "    val_loss = np.array(val_loss)\n",
    "else:\n",
    "    # find the best experiment that matches the set of tags above\n",
    "    experiments = collect_experiments(area_ID, area_measure=area_measure, D=D, DZA=DZA, \\\n",
    "                                      stoch_load_bus_IDs=stoch_load_bus_IDs, H_G1=H_G1, \\\n",
    "                                      rec_bus_IDs=rec_bus_IDs, additional_tags=additional_tags, \\\n",
    "                                      missing_tags=missing_tags, verbose=True)\n",
    "    experiment_IDs = list(experiments.keys())\n",
    "    experiment_ID = experiment_IDs[np.argmin([expt['val_loss'].min() for expt in experiments.values()])]\n",
    "#     experiment_ID = experiment_IDs[np.argmin([expt['MAPE'] for expt in experiments.values()])]\n",
    "\n",
    "    MAPE = experiments[experiment_ID]['MAPE']\n",
    "    loss = experiments[experiment_ID]['loss']\n",
    "    val_loss = experiments[experiment_ID]['val_loss']\n",
    "    batch_loss = experiments[experiment_ID]['batch_loss']\n",
    "    tags = experiments[experiment_ID]['tags']\n",
    "print(f'Selected experiment is {experiment_ID[:6]} (val_loss = {val_loss.min():.4f}, MAPE = {MAPE:.4f}%).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0a7fe",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81320651",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_path = '../experiments/neural_network/'\n",
    "network_parameters = pickle.load(open(os.path.join(experiments_path, experiment_ID, 'parameters.pkl'), 'rb'))\n",
    "checkpoint_path = experiments_path + experiment_ID + '/checkpoints/'\n",
    "checkpoint_files = glob.glob(checkpoint_path + '*.h5')\n",
    "try:\n",
    "    epochs = [int(os.path.split(file)[-1].split('.')[1].split('-')[0]) for file in checkpoint_files]\n",
    "    best_checkpoint = checkpoint_files[epochs.index(np.argmin(val_loss) + 1)]\n",
    "except:\n",
    "    best_checkpoint = checkpoint_files[-1]\n",
    "try:\n",
    "    model = keras.models.load_model(best_checkpoint)\n",
    "except:\n",
    "    if pooling_type == 'downsample':\n",
    "        custom_objects = {'DownSampling1D': DownSampling1D}\n",
    "    elif pooling_type == 'spectral':\n",
    "        custom_objects = {'SpectralPooling': SpectralPooling}\n",
    "    elif pooling_type == 'argmax':\n",
    "        custom_objects = {'MaxPooling1DWithArgmax': MaxPooling1DWithArgmax}\n",
    "    with keras.utils.custom_object_scope(custom_objects):\n",
    "        model = keras.models.load_model(best_checkpoint)\n",
    "if pooling_type == 'argmax':\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, MaxPooling1DWithArgmax):\n",
    "            print(f'Setting store_argmax = True for layer \"{layer.name}\".')\n",
    "            layer.store_argmax = True\n",
    "x_train_mean = network_parameters['x_train_mean']\n",
    "x_train_std  = network_parameters['x_train_std']\n",
    "try:\n",
    "    x_train_min = network_parameters['x_train_min']\n",
    "    x_train_max = network_parameters['x_train_max']\n",
    "except:\n",
    "    pass\n",
    "var_names = network_parameters['var_names']\n",
    "print(f'Loaded network from {best_checkpoint}.')\n",
    "print(f'Variable names: {var_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e540f",
   "metadata": {},
   "source": [
    "#### Plot the model topology"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9635a2c2",
   "metadata": {},
   "source": [
    "keras.utils.plot_model(model, show_shapes=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae77a2a3",
   "metadata": {},
   "source": [
    "### Effective receptive field size and stride of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97025c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_RF_size,effective_stride = compute_receptive_field(model, stop_layer=keras.layers.Flatten)\n",
    "print('Effective receptive field size:')\n",
    "for i,(k,v) in enumerate(effective_RF_size.items()):\n",
    "    print(f'{i}. {k} ' + '.' * (20 - len(k)) + ' {:d}'.format(v))\n",
    "print()\n",
    "print('Effective stride:')\n",
    "for i,(k,v) in enumerate(effective_stride.items()):\n",
    "    print(f'{i}. {k} ' + '.' * (20 - len(k)) + ' {:d}'.format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f8316",
   "metadata": {},
   "source": [
    "#### Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08887b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = 'test'\n",
    "use_fft = network_parameters['use_fft'] if 'use_fft' in network_parameters else False\n",
    "data_dirs = []\n",
    "for area_ID,data_dir in zip(network_parameters['area_IDs'], network_parameters['data_dirs']):\n",
    "    data_dirs.append(data_dir.format(area_ID))\n",
    "data_dir = os.path.join('..', data_dirs[0])\n",
    "data_files = sorted(glob.glob(data_dir + os.path.sep + f'*_{set_name}_set.h5'))\n",
    "ret = load_data_areas({set_name: data_files}, network_parameters['var_names'],\n",
    "                        network_parameters['generators_areas_map'][:1],\n",
    "                        network_parameters['generators_Pnom'],\n",
    "                        network_parameters['area_measure'],\n",
    "                        trial_dur=network_parameters['trial_duration'],\n",
    "                        max_block_size=100,\n",
    "                        use_tf=False, add_omega_ref=True,\n",
    "                        use_fft=use_fft)\n",
    "y = ret[2][set_name]\n",
    "if use_fft:\n",
    "    X = [(ret[1][set_name][i] - m) / (M - m) for i,(m,M) in enumerate(zip(x_train_min, x_train_max))]\n",
    "    F = ret[0]\n",
    "else:\n",
    "    X = [(ret[1][set_name][i] - m) / s for i,(m,s) in enumerate(zip(x_train_mean, x_train_std))]\n",
    "    t = ret[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f46974",
   "metadata": {},
   "source": [
    "#### Predict the momentum using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6537dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = [np.where(y == mom)[0] for mom in np.unique(y)]\n",
    "n_mom_values = len(IDX)\n",
    "momentum = [np.squeeze(model.predict(X[0][jdx])) for jdx in IDX]\n",
    "mean_momentum = [m.mean() for m in momentum]\n",
    "stddev_momentum = [m.std() for m in momentum]\n",
    "print('Mean momentum:', mean_momentum)\n",
    "print(' Std momentum:', stddev_momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595751ee",
   "metadata": {},
   "source": [
    "#### Clone the above model\n",
    "This initializes the cloned model with new random weights and will be used in the following as a control for the correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ea8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reinit_model = keras.models.clone_model(model)\n",
    "reinit_momentum = [np.squeeze(reinit_model.predict(X[0][jdx])) for jdx in IDX]\n",
    "mean_reinit_momentum = [m.mean() for m in reinit_momentum]\n",
    "stddev_reinit_momentum = [m.std() for m in reinit_momentum]\n",
    "print('Mean momentum:', mean_reinit_momentum)\n",
    "print(' Std momentum:', stddev_reinit_momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4a70d",
   "metadata": {},
   "source": [
    "### Plot the inputs and their FFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8366a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('tab10', n_mom_values)\n",
    "if use_fft:\n",
    "    Xf = X\n",
    "else:\n",
    "    data_files_training = sorted(glob.glob(data_dir + os.path.sep + f'*_training_set.h5'))\n",
    "    if len(data_files_training) == 0:\n",
    "        data_files_training = sorted(glob.glob(data_dir + os.path.sep + f'*_test_set.h5'))\n",
    "    ret_fft = load_data_areas({'training': data_files_training}, network_parameters['var_names'],\n",
    "                        network_parameters['generators_areas_map'][:1],\n",
    "                        network_parameters['generators_Pnom'],\n",
    "                        network_parameters['area_measure'],\n",
    "                        trial_dur=network_parameters['trial_duration'],\n",
    "                        max_block_size=200,\n",
    "                        use_tf=False, add_omega_ref=True,\n",
    "                        use_fft=True)\n",
    "    x_train_min_fft = np.array([val.min() for val in ret_fft[1]['training']], dtype=np.float32)\n",
    "    x_train_max_fft = np.array([val.max() for val in ret_fft[1]['training']], dtype=np.float32)\n",
    "    ret = load_data_areas({set_name: data_files}, network_parameters['var_names'],\n",
    "                        network_parameters['generators_areas_map'][:1],\n",
    "                        network_parameters['generators_Pnom'],\n",
    "                        network_parameters['area_measure'],\n",
    "                        trial_dur=network_parameters['trial_duration'],\n",
    "                        max_block_size=100,\n",
    "                        use_tf=False, add_omega_ref=True,\n",
    "                        use_fft=True)\n",
    "    F = ret[0]\n",
    "    Xf = [(ret[1][set_name][i] - m) / (M - m) for i,(m,M) in enumerate(zip(x_train_min_fft,\n",
    "                                                                           x_train_max_fft))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for j,jdx in enumerate(IDX):\n",
    "    mean = X[0][jdx].mean(axis=0)\n",
    "    stddev = X[0][jdx].std(axis=0)\n",
    "    ci = 1.96 * stddev / np.sqrt(jdx.size)\n",
    "    ax[0].fill_between(t, mean + ci, mean - ci, color=cmap(j))\n",
    "    mean = Xf[0][jdx].mean(axis=0)\n",
    "    stddev = Xf[0][jdx].std(axis=0)\n",
    "    ci = 1.96 * stddev / np.sqrt(jdx.size)\n",
    "    ax[1].fill_between(F, mean + ci, mean - ci, color=cmap(j))\n",
    "\n",
    "for a in ax:\n",
    "    for side in 'right','top':\n",
    "        a.spines[side].set_visible(False)\n",
    "    a.grid(which='major', axis='both', ls=':', lw=0.5, color=[.6,.6,.6])\n",
    "ax[1].set_xscale('log')\n",
    "ax[0].set_xlabel('Time [min]')\n",
    "ax[0].set_ylabel('Normalized trace')\n",
    "ax[1].set_xlabel('Frequency [Hz]')\n",
    "ax[1].set_ylabel('Normalized FFT')\n",
    "fig.tight_layout()\n",
    "fig.savefig(f'spectra_{n_mom_values}_momentum_levels.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0e5e4",
   "metadata": {},
   "source": [
    "### Build a model with as many outputs as there are convolutional or pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7aaddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [layer.output for layer in model.layers \\\n",
    "           if layer.name in effective_RF_size.keys() and not isinstance(layer, keras.layers.InputLayer)]\n",
    "multi_output_model = keras.Model(inputs=model.inputs, outputs=outputs)\n",
    "print(f'The model has {len(outputs)} outputs, corresponding to the following layers:')\n",
    "for i,layer in enumerate(multi_output_model.layers):\n",
    "    if not isinstance(layer, keras.layers.InputLayer):\n",
    "        print(f'    {i}. {layer.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a6f01",
   "metadata": {},
   "source": [
    "### Correlations in the actual model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722dbcc7",
   "metadata": {},
   "source": [
    "Define some variables used here and for the control model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab443b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_F = 1.1\n",
    "dt = np.diff(t[:2])[0]\n",
    "edges = np.logspace(np.log10(0.05), np.log10(0.5 / dt), 50)\n",
    "bands = [[a,b] for a,b in zip(edges[:-1], edges[1:])]\n",
    "N_bands = len(bands)\n",
    "_, N_neurons, N_filters = multi_output_model.layers[-1].output.shape\n",
    "N_trials = X[0].shape[0]\n",
    "output_file = f'correlations_{experiment_ID[:6]}_{N_bands}-bands_' + \\\n",
    "    f'{N_filters}-filters_{N_neurons}-neurons_{N_trials}-trials'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668d753",
   "metadata": {},
   "source": [
    "Compute the correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d804147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "force = False\n",
    "if not os.path.isfile(output_file + '.npz') or force:\n",
    "    R,p = compute_correlations(multi_output_model, X[0], dt, bands, effective_RF_size, effective_stride)\n",
    "    np.savez_compressed(output_file + '.npz', R=R, p=p, edges=edges)\n",
    "else:\n",
    "    data = np.load(output_file + '.npz')\n",
    "    R = data['R']\n",
    "    p = data['p']\n",
    "    edges = data['edges']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e820b",
   "metadata": {},
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a8b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,vmin,vmax = plot_correlations(R, p, edges, F, Xf[0], IDX, sort_F=sort_F)\n",
    "fig.savefig(output_file + '.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310169d",
   "metadata": {},
   "source": [
    "### Correlations in a control model with random weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019f354",
   "metadata": {},
   "source": [
    "Build a control model with the same (multiple-output) architecture as the previous one but random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718bd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [layer.output for layer in reinit_model.layers \\\n",
    "           if layer.name in effective_RF_size.keys() and not isinstance(layer, keras.layers.InputLayer)]\n",
    "ctrl_model = keras.Model(inputs=reinit_model.inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67223b39",
   "metadata": {},
   "source": [
    "Compute the correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21819052",
   "metadata": {},
   "outputs": [],
   "source": [
    "force = False\n",
    "if not os.path.isfile(output_file + '_CTRL.npz') or force:\n",
    "    R_ctrl,p_ctrl = compute_correlations(ctrl_model, X[0], dt, bands, effective_RF_size, effective_stride)\n",
    "    np.savez_compressed(output_file + '_CTRL.npz', R=R_ctrl, p=p_ctrl, edges=edges)\n",
    "else:\n",
    "    data = np.load(output_file + '_CTRL.npz')\n",
    "    R_ctrl = data['R']\n",
    "    p_ctrl = data['p']\n",
    "    edges = data['edges']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f8c8d",
   "metadata": {},
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f8303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,vmin,vmax = plot_correlations(R_ctrl, p_ctrl, edges, F, Xf[0], IDX, sort_F=sort_F, vmin=vmin, vmax=vmax)\n",
    "fig.savefig(output_file + '_CTRL.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
