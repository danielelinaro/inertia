{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import tables\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from comet_ml.api import API, APIExperiment\n",
    "from comet_ml.query import Tag\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from deep_utils import *\n",
    "import pypan.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best experiment given a set of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = API(api_key = os.environ['COMET_API_KEY'])\n",
    "workspace = 'danielelinaro'\n",
    "project_name = 'inertia'\n",
    "D = 2\n",
    "DZA = 60\n",
    "network_name = 'IEEE39'\n",
    "inertia_units = 'GW s'\n",
    "area_ID = 1\n",
    "# inertia of generator 1\n",
    "H_G1 = 500 # [s]\n",
    "# the bus(es) where the stochastic load is connected\n",
    "stoch_load_bus_IDs = [3]\n",
    "stoch_load_bus_list = 'stoch_load_bus_' + '-'.join(map(str, stoch_load_bus_IDs))\n",
    "# the bus(es) used for recording: an empy list means that the corresponding experiment tag won't be used\n",
    "rec_bus_IDs = []\n",
    "\n",
    "query = Tag(network_name) & \\\n",
    "        Tag(f'D={D}') & \\\n",
    "        Tag(f'DZA={DZA}') & \\\n",
    "        Tag('1D_pipeline') & \\\n",
    "        Tag(stoch_load_bus_list) & \\\n",
    "        Tag(f'H_G1_{H_G1}') & \\\n",
    "        Tag(f'area{area_ID}')\n",
    "\n",
    "if len(rec_bus_IDs) > 1:\n",
    "    rec_bus_list = 'buses_' + '-'.join(map(str, rec_bus_IDs))\n",
    "    query &= Tag(rec_bus_list)\n",
    "\n",
    "experiments = api.query(workspace, project_name, query, archived=False)\n",
    "experiment_IDs = []\n",
    "MAPE = []\n",
    "val_loss = []\n",
    "tags =  []\n",
    "for experiment in experiments:\n",
    "    ID = experiment.id\n",
    "    experiment_IDs.append(ID)\n",
    "    sys.stdout.write(f'Downloading data for experiment ID {ID}... ')\n",
    "    metrics = experiment.get_metrics()\n",
    "    sys.stdout.write('done.\\n')\n",
    "    val_loss.append(np.array([float(m['metricValue']) for m in metrics if m['metricName'] == 'val_loss']))\n",
    "    has_MAPE = False\n",
    "    for m in metrics:\n",
    "        if m['metricName'] == 'mape_prediction':\n",
    "            val = m['metricValue']\n",
    "            try:\n",
    "                MAPE.append(float(val))\n",
    "            except:\n",
    "                MAPE.append(list(map(float, [v for v in val[1:-1].split(' ') if len(v)])))\n",
    "            has_MAPE = True\n",
    "            break\n",
    "    tags.append(experiment.get_tags())\n",
    "    print(f'  val_loss: {val_loss[-1].min():.4f}')\n",
    "    if has_MAPE:\n",
    "        print(f'      MAPE: {MAPE[-1]}%')\n",
    "    else:\n",
    "        print('      MAPE: [experiment not terminated]')\n",
    "    print('      Tags: \"{}\"'.format('\" \"'.join(tags[-1])))\n",
    "# idx = np.argmin(MAPE)\n",
    "idx = np.argmin([loss.min() for loss in val_loss])\n",
    "experiment_ID = experiment_IDs[idx]\n",
    "MAPE = MAPE[idx]\n",
    "val_loss = val_loss[idx]\n",
    "tags = tags[idx]\n",
    "print(f'The best experiment is {experiment_ID[:6]} (val_loss = {val_loss.min():.4f}, MAPE = {MAPE}%).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_path = '../experiments/neural_network/'\n",
    "checkpoint_path = experiments_path + experiment_ID + '/checkpoints/'\n",
    "checkpoint_files = glob.glob(checkpoint_path + '*.h5')\n",
    "network_parameters = pickle.load(open(experiments_path + experiment_ID \\\n",
    "                                      + '/parameters.pkl', 'rb'))\n",
    "epochs = [int(os.path.split(file)[-1].split('.')[1].split('-')[0]) for file in checkpoint_files]\n",
    "best_checkpoint = checkpoint_files[epochs.index(np.argmin(val_loss) + 1)]\n",
    "model = keras.models.load_model(best_checkpoint, compile=True)\n",
    "data_dirs = ['..' + os.path.sep +\n",
    "             os.path.sep.join([d for d in data_dir.split(os.path.sep) if '{}' not in d]) +\n",
    "             f'/H_G1_{H_G1}/stoch_load_bus_' + '-'.join(map(str, stoch_load_bus_IDs))\n",
    "             for data_dir in network_parameters['data_dirs']]\n",
    "# we need mean and standard deviation of the training set to normalize the data\n",
    "x_train_mean = network_parameters['x_train_mean']\n",
    "x_train_std  = network_parameters['x_train_std']\n",
    "data_dir = data_dirs[0]\n",
    "tmp = [re.findall('.*_bus', var_name)[0] for var_name in network_parameters['var_names']]\n",
    "var_names_fmt = list(OrderedDict({k + '{}': [] for k in tmp}).keys())\n",
    "if len(rec_bus_IDs) == 0:\n",
    "    rec_bus_IDs = list(np.unique([int(re.findall('\\d+', var_name)[0]) \\\n",
    "                                  for var_name in network_parameters['var_names']]))\n",
    "    rec_bus_list = 'buses_' + '-'.join(map(str, rec_bus_IDs))\n",
    "if not os.path.isdir(data_dir):\n",
    "    raise Exception(f'{data_dir}: no such directory')\n",
    "print(f'Loaded network from {best_checkpoint}.')\n",
    "print(f'Data directory is {data_dir}.')\n",
    "print(f'Variable names: {var_names_fmt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=False, dpi=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_H = OrderedDict([\n",
    "    ('Pg30', H_G1), ('Pg31', 30.3), ('Pg32', 35.8), ('Pg33', 28.6), ('Pg34', 26),\n",
    "    ('Pg35', 34.8), ('Pg36', 26.4), ('Pg37', 24.3), ('Pg38', 34.5), ('Pg39', 42)\n",
    "\n",
    "])\n",
    "P_nom = {gen_ID: 100e6 for gen_ID in default_H}\n",
    "\n",
    "generators_areas_map = [\n",
    "    ['Pg31', 'Pg32', 'Pg39']\n",
    "]\n",
    "\n",
    "window_dur = 60\n",
    "window_step = 10\n",
    "\n",
    "H_values = [\n",
    "    default_H,\n",
    "    OrderedDict([\n",
    "        ('Pg30', H_G1), ('Pg31', 34), ('Pg32', 40), ('Pg33', 28.6), ('Pg34', 26),\n",
    "        ('Pg35', 34.8), ('Pg36', 26.4), ('Pg37', 24.3), ('Pg38', 34.5), ('Pg39', 46)\n",
    "\n",
    "    ])\n",
    "]\n",
    "N_H = len(H_values)\n",
    "\n",
    "area_inertia = []\n",
    "\n",
    "data_normalized = []\n",
    "inertia = []\n",
    "\n",
    "var_names = [var_name.format(bus_ID) for bus_ID in np.unique(rec_bus_IDs) for var_name in var_names_fmt]\n",
    "data_mean = {var_name: x_train_mean[k] for k,var_name in enumerate(var_names)}\n",
    "data_std = {var_name: x_train_std[k] for k,var_name in enumerate(var_names)}\n",
    "\n",
    "for H in H_values:\n",
    "    tmp_inertia = []\n",
    "    for generator_IDs in generators_areas_map:\n",
    "        tmp_inertia.append(np.sum([P_nom[gen_ID] * H[gen_ID] for gen_ID in generator_IDs]) * 1e-9)\n",
    "    area_inertia.append(tmp_inertia)\n",
    "    data_file = data_dir + '/ieee39_' + '_'.join(map(lambda h: f'{h:.3f}', H.values())) + '.h5'\n",
    "\n",
    "    t, _, data_norm, data_sliding, _ = load_data_slide([data_file],\n",
    "                                                        var_names,\n",
    "                                                        data_mean,\n",
    "                                                        data_std,\n",
    "                                                        window_dur,\n",
    "                                                        window_step,\n",
    "                                                        add_omega_ref = False,\n",
    "                                                        verbose = True)\n",
    "    data_normalized.append(data_norm)\n",
    "    dt = np.diff(t[:2])[0]\n",
    "    time, HH, _ = predict(model, data_sliding, window_step, dt)\n",
    "    inertia.append(HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vars = len(var_names_fmt)\n",
    "fig = plt.figure(figsize=(8, n_vars * 2.5))\n",
    "gs = fig.add_gridspec(n_vars+1, 4)\n",
    "ax = []\n",
    "for i in range(n_vars):\n",
    "    ax.append([fig.add_subplot(gs[i, :3]), fig.add_subplot(gs[i, 3])]),\n",
    "ax.append([fig.add_subplot(gs[-1, :2]), fig.add_subplot(gs[-1, 2:])])\n",
    "\n",
    "col = [[.2,.2,.2], [.8,0,0], [0,.7,0]]\n",
    "\n",
    "bus_ID = rec_bus_IDs[1]\n",
    "\n",
    "idx = t < 60\n",
    "for i in range(N_H):\n",
    "    for j,var_name in enumerate(var_names_fmt):\n",
    "        key = var_name.format(bus_ID)\n",
    "        value = data_normalized[i][key]\n",
    "        n,edges = np.histogram(value, bins=25, range=(-4,4), density=True)\n",
    "        ax[j][0].plot(t[idx], value[idx], color=col[i], lw=1, label=f'H = {area_inertia[i][0]} {inertia_units}')\n",
    "        ax[j][1].plot(n, edges[:-1] + np.diff(edges[:2])[0] / 2, color=col[i], lw=1)\n",
    "        for a in ax[j]:\n",
    "            a.set_ylim([-4,4])\n",
    "        ax[j][0].set_ylabel(key)\n",
    "    ax[-1][i].plot(time / 60, inertia[i], 'k', lw=1)\n",
    "    ax[-1][i].plot(time[[0,-1]] / 60, area_inertia[i][0] + np.zeros(2), 'k--', lw=1)\n",
    "    ax[-1][i].set_ylim([10, 13])\n",
    "    ax[-1][i].set_xlabel('Time [min]')\n",
    "\n",
    "for a in ax:\n",
    "    for side in 'right','top':\n",
    "        for i in range(2):\n",
    "            a[i].spines[side].set_visible(False)\n",
    "\n",
    "ax[0][0].legend(loc='best')\n",
    "ax[-1][0].get_shared_x_axes().join(ax[-1][0], ax[-1][1])\n",
    "\n",
    "ax[-1][0].set_xlim([0,30])\n",
    "ax[-1][0].set_ylabel(f'Inertia [{inertia_units}]')\n",
    "ax[-2][0].set_xlabel('Time [s]')\n",
    "ax[-2][1].set_xlabel('Fraction')\n",
    "fig.tight_layout()\n",
    "output_filename = f'{network_name}_area{area_ID}_H_G1={H_G1}_' + \\\n",
    "    f'rec_buses={rec_bus_list}_load_buses={stoch_load_bus_list}_const_H.pdf'\n",
    "fig.savefig(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step of inertia with transient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_values = [\n",
    "    OrderedDict([\n",
    "        ('Pg30', H_G1), ('Pg31', 20), ('Pg32', 28), ('Pg33', 28.6), ('Pg34', 26),\n",
    "        ('Pg35', 34.8), ('Pg36', 26.4), ('Pg37', 24.3), ('Pg38', 34.5), ('Pg39', 36)\n",
    "\n",
    "    ]),\n",
    "    OrderedDict([\n",
    "        ('Pg30', H_G1), ('Pg31', 42), ('Pg32', 46), ('Pg33', 28.6), ('Pg34', 26),\n",
    "        ('Pg35', 34.8), ('Pg36', 26.4), ('Pg37', 24.3), ('Pg38', 34.5), ('Pg39', 50)\n",
    "\n",
    "    ])\n",
    "]\n",
    "\n",
    "H = []\n",
    "for H_val in H_values:\n",
    "    for generators in generators_areas_map:\n",
    "        H.append(np.sum([H_val[gen_id] * P_nom[gen_id] for gen_id in generators]) * 1e-9)\n",
    "\n",
    "data_file = data_dir + '/' + network_name.lower()\n",
    "for gen_id in H_values[0]:\n",
    "    data_file += '_' + '-'.join(map(lambda h: f'{h:.3f}', np.unique([H[gen_id] for H in H_values])))\n",
    "data_file += '.h5'\n",
    "\n",
    "window_dur = 60\n",
    "window_step = 10\n",
    "\n",
    "var_names = [var_name.format(bus_ID) for bus_ID in rec_bus_IDs for var_name in var_names_fmt]\n",
    "data_mean = {var_name: x_train_mean[k] for k,var_name in enumerate(var_names)}\n",
    "data_std = {var_name: x_train_std[k] for k,var_name in enumerate(var_names)}\n",
    "\n",
    "t, _, data_normalized, data_sliding, _ = load_data_slide([data_file],\n",
    "                                                         var_names,\n",
    "                                                         data_mean,\n",
    "                                                         data_std,\n",
    "                                                         window_dur,\n",
    "                                                         window_step,\n",
    "                                                         add_omega_ref = False,\n",
    "                                                         verbose = True)\n",
    "\n",
    "dt = np.diff(t[:2])[0]\n",
    "time, inertia, _ = predict(model, data_sliding, window_step, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (6,9))\n",
    "gs = fig.add_gridspec(2 * n_vars + 3 + 1, 1)\n",
    "ax = []\n",
    "for i in range(n_vars):\n",
    "    ax.append(fig.add_subplot(gs[i*2:(i+1)*2]))\n",
    "ax.append(fig.add_subplot(gs[-4:-1,0]))\n",
    "ax.append(fig.add_subplot(gs[-1,0]))\n",
    "\n",
    "ds = 5\n",
    "for i,var_name in enumerate(var_names_fmt):\n",
    "    for j,bus_id in enumerate(rec_bus_IDs[::-1]):\n",
    "        ax[i].plot(t[::ds] / 60, data_normalized[var_name.format(bus_id)][::ds],\n",
    "                   color=col[j], lw=1, label=f'bus{bus_id}')\n",
    "    ax[i].set_ylim([-5,5])\n",
    "    ax[i].set_ylabel(var_name.split('_')[0])\n",
    "\n",
    "ylim = [H[0] - 0.5, H[1] + 0.5]\n",
    "ax[-2].plot(time[::ds] / 60, inertia[::ds,0], color=col[0])\n",
    "ax[-2].set_ylim(ylim)\n",
    "ax[-2].plot([0, 30], H[0] + np.zeros(2), '--', color=col[0], lw=1)\n",
    "ax[-2].plot([30, 60], H[1] + np.zeros(2), '--', color=col[0], lw=1)\n",
    "\n",
    "ax[-1].plot([0, 30], H[0] + np.zeros(2), '-', color=col[0], lw=2)\n",
    "ax[-1].plot([30, 60], H[1] + np.zeros(2), '-', color=col[0], lw=2)\n",
    "ax[-1].plot(30 + np.zeros(2), ylim, '--', color=col[0], lw=1)\n",
    "ax[-1].set_ylim(ylim)\n",
    "ax[-1].set_yticks(np.arange(8,15,2))\n",
    "ax[-1].set_xlabel('Time [min]')\n",
    "\n",
    "ax[0].legend(loc='lower right')\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlim([-1,61])\n",
    "    for side in 'right', 'top':\n",
    "        a.spines[side].set_visible(False)\n",
    "    if a != ax[-1]:\n",
    "        a.set_xticklabels([])\n",
    "\n",
    "ax[-2].set_ylabel(f'Inertia [{inertia_units}]')\n",
    "ax[-1].set_ylabel(f'Inertia [{inertia_units}]')\n",
    "\n",
    "fig.tight_layout()\n",
    "output_filename = f'{network_name}_area{area_ID}_H_G1={H_G1}_' + \\\n",
    "    f'rec_buses={rec_bus_list}_load_buses={stoch_load_bus_list}_step_H.pdf'\n",
    "fig.savefig(output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
