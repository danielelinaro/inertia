{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import shelve\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "from comet_ml.api import API, APIExperiment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from dlml.utils import collect_experiments\n",
    "from dlml.data import read_area_values, load_data_areas\n",
    "from dlml.nn import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be12d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full grid without varying compensator's inertia\n",
    "experiment_ID = 'f64bde90cab54d1ea770bb21f33c3ed1'\n",
    "# full grid with variable compensator's inertia\n",
    "experiment_ID = 'a40658acee3c4e419c0ee34d0c59f4df'\n",
    "\n",
    "workspace = 'danielelinaro'\n",
    "project_name = 'inertia'\n",
    "\n",
    "api = API(api_key = os.environ['COMET_API_KEY'])\n",
    "experiment = api.get_experiment(workspace, project_name, experiment_ID)\n",
    "tags = experiment.get_tags()\n",
    "\n",
    "print(f'\\nExperiment {experiment_ID[:6]} has the following tags:')\n",
    "nchar = 0\n",
    "for tag in tags:\n",
    "    if 'buses_' in tag:\n",
    "        rec_bus_IDs = sorted(list(map(int, re.findall('\\d+', tag))))\n",
    "    elif 'area_measure_' in tag:\n",
    "        area_measure = tag.split('_')[-1]\n",
    "    elif 'area' in tag:\n",
    "        area_ID = int(tag[4:])\n",
    "    elif tag == 'all_stoch_loads':\n",
    "        stoch_load_bus_IDs = []\n",
    "    sys.stdout.write('\"' + tag + '\"   ')\n",
    "    nchar += len(tag) + 5\n",
    "    if nchar >= 70:\n",
    "        nchar = 0\n",
    "        sys.stdout.write('\\n')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c56284",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = os.path.join('..', 'experiments', 'neural_network', experiment_ID)\n",
    "weights_file = os.path.join(experiment_dir, 'checkpoints', 'weights.h5')\n",
    "model = keras.models.load_model(weights_file, compile=False)\n",
    "model.compile()\n",
    "network_pars = pickle.load(open(os.path.join(experiment_dir, 'parameters.pkl'), 'rb'))\n",
    "data_dirs = [os.path.join('..', d.format(a)) if '{}' in d else os.path.join('..', d) \\\n",
    "             for d,a in zip(network_pars['data_dirs'], network_pars['area_IDs'])]\n",
    "# we need mean and standard deviation of the training set to normalize the data\n",
    "x_train_mean = network_pars['x_train_mean']\n",
    "x_train_std  = network_pars['x_train_std']\n",
    "data_dir = data_dirs[0]\n",
    "if not os.path.isdir(data_dir):\n",
    "    raise Exception(f'{data_dir}: no such directory')\n",
    "\n",
    "print(f'Loaded network from {weights_file}.')\n",
    "print(f'Data directory is {data_dir}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee7533",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = 'test'\n",
    "data_files = {set_name: glob.glob(os.path.join(*data_dir.split(os.path.sep)[:-1],\n",
    "                                               'wide_grid_test', f'inertia_*_{set_name}_set.h5'))}\n",
    "_,X_raw,y = load_data_areas(data_files,\n",
    "                            network_pars['var_names'],\n",
    "                            network_pars['generators_areas_map'],\n",
    "                            network_pars['generators_Pnom'],\n",
    "                            network_pars['area_measure'],  \n",
    "                            trial_dur=network_pars['trial_duration'],\n",
    "                            max_block_size=network_pars['max_block_size'],\n",
    "                            use_fft=network_pars['use_fft'],\n",
    "                            use_tf=False,\n",
    "                            verbose=False)\n",
    "X_test = tf.constant([(X_raw[set_name][i] - m) / s for i,(m,s) in enumerate(zip(x_train_mean, x_train_std))])\n",
    "y_test = y[set_name][:,0]\n",
    "y_predict = model.predict({var_name: X_test[i] for i,var_name in enumerate(network_pars['var_names'])}).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54490d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = len(data_files[set_name])\n",
    "block_size = y_test.shape[0] // n_files\n",
    "data = {'H_G02': [], 'H_G03': [], 'M_area1': [], 'M_area1_pred_mean': [], 'M_area1_pred_std': [], 'MAPE': []}\n",
    "for i,data_file in enumerate(data_files[set_name]):\n",
    "    names, H, M, _ = read_area_values(data_file,\n",
    "                                      network_pars['generators_areas_map'],\n",
    "                                      network_pars['generators_Pnom'],\n",
    "                                      network_pars['area_measure'])\n",
    "    for j,name in enumerate(('G02','G03')):\n",
    "        key = 'H_' + name\n",
    "        data[key].append(H[j+1])\n",
    "    data['M_area1'].append(M[0])\n",
    "    idx = slice(i*block_size, (i+1)*block_size)\n",
    "    data['M_area1_pred_mean'].append(y_predict[idx].mean())\n",
    "    data['M_area1_pred_std'].append(y_predict[idx].std())\n",
    "    data['MAPE'].append(mape(y_test[idx], y_predict[idx]))\n",
    "df = pd.DataFrame(data)\n",
    "df.to_parquet(f'wide_grid_test_{experiment_ID[:6]}.parquet.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3678da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 1, figsize=(5,4))\n",
    "x = [3.33, 5.33, 5.33, 3.33, 3.33]\n",
    "y = [3.47, 3.47, 5.47, 5.47, 3.47]\n",
    "ax.plot(x, y, '-', color=.7+np.zeros(3), lw=2.5)\n",
    "ax.fill_between(x[:2], y[:2], y[2:4], fc=.8+np.zeros(3), alpha=0.5)\n",
    "im = ax.scatter(df['H_G02'], df['H_G03'], s=df['MAPE']*1000, c=df['MAPE'],\n",
    "           cmap='RdYlBu_r', vmin=0.0, vmax=0.6)\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('MAPE')\n",
    "ticks = np.r_[0 : 0.65 : 0.1]\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.set_ticklabels([f'{tick*100:.0f}' for tick in ticks])\n",
    "lim = [1.2,10.8]\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "ticks = np.arange(2, 11, 2)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xlabel(r'$H_{G2}$ [s]')\n",
    "ax.set_ylabel(r'$H_{G3}$ [s]')\n",
    "sns.despine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
