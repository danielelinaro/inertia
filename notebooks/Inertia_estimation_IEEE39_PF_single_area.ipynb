{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from dlml.utils import collect_experiments\n",
    "from dlml.data import read_area_values, load_data_slide\n",
    "from dlml.nn import predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best experiment given a set of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_ID = 1\n",
    "area_measure = 'momentum'\n",
    "stoch_load_bus_IDs = []\n",
    "rec_bus_IDs = [3, 14, 17, 39]\n",
    "H_G1, D, DZA = None, None, None # 500, 2, 0\n",
    "additional_tags = ['ReLU_none', 'converted_from_PowerFactory', 'all_stoch_loads']\n",
    "\n",
    "experiments = collect_experiments(area_ID, area_measure=area_measure, D=D, DZA=DZA, \\\n",
    "                                  stoch_load_bus_IDs=stoch_load_bus_IDs, H_G1=H_G1, \\\n",
    "                                  rec_bus_IDs=rec_bus_IDs, additional_tags=additional_tags, \\\n",
    "                                  verbose=True)\n",
    "experiment_IDs = list(experiments.keys())\n",
    "experiment_ID = experiment_IDs[np.argmin([expt['val_loss'].min() for expt in experiments.values()])]\n",
    "experiment_ID = '7d55f784f6b64f2caeb866804bda1a8b'\n",
    "MAPE = experiments[experiment_ID]['MAPE']\n",
    "loss = experiments[experiment_ID]['loss']\n",
    "val_loss = experiments[experiment_ID]['val_loss']\n",
    "batch_loss = experiments[experiment_ID]['batch_loss']\n",
    "tags = experiments[experiment_ID]['tags']\n",
    "print(f'The best experiment is {experiment_ID[:6]} (val_loss = {val_loss.min():.4f}, MAPE = {MAPE:.4f}%).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_path = '../experiments/neural_network/'\n",
    "checkpoint_path = experiments_path + experiment_ID + '/checkpoints/'\n",
    "checkpoint_files = glob.glob(checkpoint_path + '*.h5')\n",
    "if len(checkpoint_files) == 1:\n",
    "    best_checkpoint = checkpoint_files[0]\n",
    "else:\n",
    "    epochs = [int(os.path.split(file)[-1].split('.')[1].split('-')[0]) for file in checkpoint_files]\n",
    "    best_checkpoint = checkpoint_files[epochs.index(np.argmin(val_loss) + 1)]\n",
    "model = keras.models.load_model(best_checkpoint, compile=False)\n",
    "model.compile()\n",
    "network_parameters = pickle.load(open(os.path.join(experiments_path, experiment_ID, 'parameters.pkl'), 'rb'))\n",
    "data_dirs = [os.path.join('..', d.format(a)) if '{}' in d else os.path.join('..', d) \\\n",
    "             for d,a in zip(network_parameters['data_dirs'], network_parameters['area_IDs'])]\n",
    "# we need mean and standard deviation of the training set to normalize the data\n",
    "x_train_mean = network_parameters['x_train_mean']\n",
    "x_train_std  = network_parameters['x_train_std']\n",
    "data_dir = data_dirs[0]\n",
    "tmp = [re.findall('.*_bus', var_name)[0] for var_name in network_parameters['var_names'] if 'bus' in var_name]\n",
    "var_names_fmt = OrderedDict({k + '{}': [] for k in tmp})\n",
    "tmp = [re.findall('.*_line', var_name)[0] for var_name in network_parameters['var_names'] if 'line' in var_name]\n",
    "for k in tmp:\n",
    "    var_names_fmt[k + '_{}_{}'] = []\n",
    "var_names_fmt = list(var_names_fmt.keys())\n",
    "if len(rec_bus_IDs) == 0:\n",
    "    rec_bus_IDs = list(np.unique([int(re.findall('\\d+', var_name)[0]) \\\n",
    "                                  for var_name in network_parameters['var_names']]))\n",
    "    rec_bus_list = 'buses_' + '-'.join(map(str, rec_bus_IDs))\n",
    "if not os.path.isdir(data_dir):\n",
    "    raise Exception(f'{data_dir}: no such directory')\n",
    "print(f'Loaded network from {best_checkpoint}.')\n",
    "print(f'Data directory is {data_dir}.')\n",
    "print(f'Variable names: {var_names_fmt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=False, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_H = OrderedDict([\n",
    "    ('G01', 5.00), ('G02', 4.33), ('G03', 4.47), ('G04', 3.57), ('G05', 4.33),\n",
    "    ('G06', 4.35), ('G07', 3.77), ('G08', 3.47), ('G09', 3.45), ('G10', 4.20)\n",
    "])\n",
    "default_H_with_comp = OrderedDict([\n",
    "    ('G01', 5.00), ('G02', 4.33), ('G03', 4.47), ('G04', 3.57), ('G05', 4.33),\n",
    "    ('G06', 4.35), ('G07', 3.77), ('G08', 3.47), ('G09', 3.45), ('G10', 4.20),\n",
    "    ('Comp11', 0.1), ('Comp21', 0.1), ('Comp31', 0.1)\n",
    "])\n",
    "\n",
    "\n",
    "generators_areas_map = {\n",
    "    'default': [\n",
    "        ['G02', 'G03'],\n",
    "        ['G04', 'G05', 'G06', 'G07'],\n",
    "        ['G08', 'G09', 'G10'],\n",
    "        ['G01']\n",
    "    ]\n",
    "}\n",
    "generators_areas_map_with_comp = {\n",
    "    'default': [\n",
    "        ['G02', 'G03', 'Comp11'],\n",
    "        ['G04', 'G05', 'G06', 'G07', 'Comp21'],\n",
    "        ['G08', 'G09', 'G10', 'Comp31'],\n",
    "        ['G01']\n",
    "    ]\n",
    "}\n",
    "\n",
    "P_nom = {'G01': 10000e6, 'G02': 700e6, 'G03': 800e6, 'G04':  800e6, 'G05':  300e6,\n",
    "         'G06':   800e6, 'G07': 700e6, 'G08': 700e6, 'G09': 1000e6, 'G10': 1000e6}\n",
    "P_nom_with_comp = {'G01': 10000e6, 'G02': 700e6, 'G03': 800e6, 'G04':  800e6, 'G05':  300e6,\n",
    "                   'G06':   800e6, 'G07': 700e6, 'G08': 700e6, 'G09': 1000e6, 'G10': 1000e6,\n",
    "                   'Comp11': 100e6, 'Comp21': 100e6, 'Comp31': 100e6}\n",
    "\n",
    "\n",
    "window_dur = 60\n",
    "window_step = 1\n",
    "\n",
    "var_names = network_parameters['var_names']\n",
    "data_mean = {var_name: x_train_mean[k] for k,var_name in enumerate(var_names)}\n",
    "data_std = {var_name: x_train_std[k] for k,var_name in enumerate(var_names)}\n",
    "\n",
    "if area_measure == 'inertia':\n",
    "    measure_units = 's'\n",
    "elif area_measure == 'energy':\n",
    "    measure_units = r'GW$\\cdot$s'\n",
    "elif area_measure == 'momentum':\n",
    "    measure_units = r'GW$\\cdot$s$^2$'\n",
    "    \n",
    "stoch_load_bus_list = 'stoch_load_bus_' + '-'.join(map(str, stoch_load_bus_IDs))\n",
    "rec_bus_list = 'buses_' + '-'.join(map(str, rec_bus_IDs))\n",
    "\n",
    "abbrv = {'inertia': 'H', 'energy': 'E', 'momentum': 'M'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_values = [\n",
    "    default_H_with_comp,\n",
    "    OrderedDict([\n",
    "        ('G01', 5.00), ('G02', 3.83), ('G03', 3.97), ('G04', 3.57), ('G05', 4.33),\n",
    "        ('G06', 4.35), ('G07', 3.77), ('G08', 3.47), ('G09', 3.45), ('G10', 4.2),\n",
    "        ('Comp11', 0.1), ('Comp21', 0.1), ('Comp31', 0.1)\n",
    "    ]),\n",
    "    OrderedDict([\n",
    "        ('G01', 5.00), ('G02', 4.83), ('G03', 4.97), ('G04', 3.57), ('G05', 4.33),\n",
    "        ('G06', 4.35), ('G07', 3.77), ('G08', 3.47), ('G09', 3.45), ('G10', 4.2),\n",
    "        ('Comp11', 0.1), ('Comp21', 0.1), ('Comp31', 0.1)\n",
    "    ])\n",
    "]\n",
    "N_H = len(H_values)\n",
    "\n",
    "\n",
    "measure_exact = []\n",
    "\n",
    "data_normalized = []\n",
    "measure = []\n",
    "area_inertia = []\n",
    "\n",
    "for H in H_values:\n",
    "    data_file = data_dir + '/ieee39_PF_stoch_loads_compensators_' + \\\n",
    "        '_'.join(map(lambda h: f'{h:.3f}', H.values())) + '.h5'\n",
    "    print(f'Reading data from {data_file}...')\n",
    "    _,_,v,_ = read_area_values(data_file, generators_areas_map['default'], P_nom, area_measure)\n",
    "    _,_,h,_ = read_area_values(data_file, generators_areas_map['default'], P_nom, 'inertia')\n",
    "    measure_exact.append(v[area_ID - 1])\n",
    "    area_inertia.append(h[area_ID - 1])\n",
    "\n",
    "    t, _, data_norm, data_sliding, _ = load_data_slide([data_file],\n",
    "                                                        var_names,\n",
    "                                                        None,\n",
    "                                                        data_std,\n",
    "                                                        window_dur,\n",
    "                                                        window_step,\n",
    "                                                        add_omega_ref = False,\n",
    "                                                        verbose = True)\n",
    "    print(f'Duration of the simulation: {t[-1]} sec.')\n",
    "    data_normalized.append(data_norm)\n",
    "    dt = np.diff(t[:2])[0]\n",
    "    time, HH, _ = predict(model, data_sliding, window_step)\n",
    "    measure.append(HH)\n",
    "measure_exact = np.array(measure_exact)\n",
    "area_inertia = np.array(area_inertia)\n",
    "measure_predicted = np.array(list(map(np.nanmean, measure)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vars = len(var_names_fmt)\n",
    "fig = plt.figure(figsize=(8, n_vars * 4))\n",
    "if N_H < 3:\n",
    "    gs = fig.add_gridspec(n_vars+1, 4)\n",
    "elif N_H == 3:\n",
    "    gs = fig.add_gridspec(n_vars+1, 6)\n",
    "else:\n",
    "    raise Exception('Supported values of N_H are 1, 2 or 3')\n",
    "ax = []\n",
    "for i in range(n_vars):\n",
    "    if N_H < 3:\n",
    "        ax.append([fig.add_subplot(gs[i, :3]), fig.add_subplot(gs[i, 3])])\n",
    "    else:\n",
    "        ax.append([fig.add_subplot(gs[i, :4]), fig.add_subplot(gs[i, 4:])])\n",
    "if N_H < 3:\n",
    "    ax.append([fig.add_subplot(gs[-1, :2]), fig.add_subplot(gs[-1, 2:])])\n",
    "else:\n",
    "    ax.append([fig.add_subplot(gs[-1, :2]), fig.add_subplot(gs[-1, 2:4]), fig.add_subplot(gs[-1, 4:])])\n",
    "\n",
    "col = [[.2,.2,.2], [.8,0,0], [0,.7,0]]\n",
    "\n",
    "bus_ID = rec_bus_IDs[0]\n",
    "line_IDs = rec_bus_IDs[0], rec_bus_IDs[0] + 1\n",
    "\n",
    "idx = t < 60\n",
    "\n",
    "dm = np.max(measure_exact) - np.min(measure_exact)\n",
    "ylim = [np.min(measure_exact) - dm / 2, np.max(measure_exact) + dm / 2]\n",
    "\n",
    "for i in range(N_H):\n",
    "    for j,var_name in enumerate(var_names_fmt):\n",
    "        if 'bus' in var_name:\n",
    "            key = var_name.format(bus_ID)\n",
    "        elif 'line' in var_name:\n",
    "            key = var_name.format(line_IDs[0], line_IDs[1])\n",
    "        value = data_normalized[i][key]\n",
    "        n,edges = np.histogram(value, bins=25, range=(-4,4), density=True)\n",
    "        ax[j][0].plot(t[idx], value[idx], color=col[i], lw=1, \\\n",
    "                      label=f'{abbrv[area_measure]} = {measure_exact[i]:.2f} {measure_units}')\n",
    "        ax[j][1].plot(n, edges[:-1] + np.diff(edges[:2])[0] / 2, color=col[i], lw=1)\n",
    "        for a in ax[j]:\n",
    "            a.set_ylim([-4,4])\n",
    "        ax[j][0].set_ylabel(key)\n",
    "    ax[-1][i].plot(time / 60, measure[i], 'k', lw=1)\n",
    "    ax[-1][i].plot(time[[0,-1]] / 60, measure_exact[i] + np.zeros(2), 'k--', lw=1)\n",
    "    ax[-1][i].set_ylim(ylim)\n",
    "    ax[-1][i].set_xlabel('Time [min]')\n",
    "\n",
    "for a in ax:\n",
    "    for side in 'right','top':\n",
    "        for aa in a:\n",
    "            aa.spines[side].set_visible(False)\n",
    "\n",
    "ax[0][0].legend(loc='best')\n",
    "# ax[-1][0].set_ylim([0.2, 0.25])\n",
    "for i in range(1, N_H):\n",
    "    ax[-1][i].set_xlim(ax[-1][0].get_xlim())\n",
    "    ax[-1][i].set_ylim(ax[-1][0].get_ylim())\n",
    "#     ax[-1][0].get_shared_x_axes().join(ax[-1][0], ax[-1][i])\n",
    "\n",
    "# ax[-1][0].set_xlim([0,30])\n",
    "ax[-1][0].set_ylabel(f'{area_measure.capitalize()} [{measure_units}]')\n",
    "ax[-2][0].set_xlabel('Time [s]')\n",
    "ax[-2][1].set_xlabel('Fraction')\n",
    "fig.tight_layout()\n",
    "# output_filename = f'IEEE39_area{area_ID}_H_G1={H_G1}_' + \\\n",
    "#     f'rec_buses={rec_bus_list}_load_buses={stoch_load_bus_list}_const_H_{experiment_ID[:6]}.pdf'\n",
    "# fig.savefig(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different combinations of inertia corresponding to the same area momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = lambda H, S, fn=60.: 2 * H@S / fn * 1e-3\n",
    "S = np.array([700, 800, 100])\n",
    "H2 = np.linspace(3.33, 5.33, 6)\n",
    "H3 = np.linspace(3.47, 5.47, 6)\n",
    "dH = 0.2\n",
    "H = np.array([4.33, 4.47, 0.1])\n",
    "print('H = {} -> M = {:.4f} GW.s2'.format(H, momentum(H, S)))\n",
    "H = np.array([H2[2], H3[3], 0.1]) + np.array([-dH, dH, 0])\n",
    "print('H = {} -> M = {:.4f} GW.s2'.format(H, momentum(H, S)))\n",
    "H = np.array([H2[3], H3[2], 0.1]) + np.array([dH, -dH, 0])\n",
    "print('H = {} -> M = {:.4f} GW.s2'.format(H, momentum(H, S)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_values = [\n",
    "    default_H_with_comp,\n",
    "    OrderedDict([\n",
    "        ('G01', 5.00), ('G02', 3.93), ('G03', 4.87), ('G04', 3.57), ('G05', 4.33),\n",
    "        ('G06', 4.35), ('G07', 3.77), ('G08', 3.47), ('G09', 3.45), ('G10', 4.2),\n",
    "        ('Comp11', 0.1), ('Comp21', 0.1), ('Comp31', 0.1)\n",
    "    ]),\n",
    "    OrderedDict([\n",
    "        ('G01', 5.00), ('G02', 4.73), ('G03', 4.07), ('G04', 3.57), ('G05', 4.33),\n",
    "        ('G06', 4.35), ('G07', 3.77), ('G08', 3.47), ('G09', 3.45), ('G10', 4.2),\n",
    "        ('Comp11', 0.1), ('Comp21', 0.1), ('Comp31', 0.1)\n",
    "    ])\n",
    "]\n",
    "N_H = len(H_values)\n",
    "\n",
    "\n",
    "measure_exact = []\n",
    "\n",
    "data_normalized = []\n",
    "measure = []\n",
    "area_inertia = []\n",
    "\n",
    "for H in H_values:\n",
    "    data_file = data_dir + '/ieee39_PF_stoch_loads_compensators_' + \\\n",
    "        '_'.join(map(lambda h: f'{h:.3f}', H.values())) + '.h5'\n",
    "    print(f'Reading data from {data_file}...')\n",
    "    _,_,v,_ = read_area_values(data_file, generators_areas_map['default'], P_nom, area_measure)\n",
    "    _,_,h,_ = read_area_values(data_file, generators_areas_map['default'], P_nom, 'inertia')\n",
    "    measure_exact.append(v[area_ID - 1])\n",
    "    area_inertia.append(h[area_ID - 1])\n",
    "\n",
    "    t, _, data_norm, data_sliding, _ = load_data_slide([data_file],\n",
    "                                                        var_names,\n",
    "                                                        None,\n",
    "                                                        data_std,\n",
    "                                                        window_dur,\n",
    "                                                        window_step,\n",
    "                                                        add_omega_ref = False,\n",
    "                                                        verbose = True)\n",
    "    print(f'Duration of the simulation: {t[-1]} sec.')\n",
    "    data_normalized.append(data_norm)\n",
    "    dt = np.diff(t[:2])[0]\n",
    "    time, HH, _ = predict(model, data_sliding, window_step)\n",
    "    measure.append(HH)\n",
    "measure_exact = np.array(measure_exact)\n",
    "area_inertia = np.array(area_inertia)\n",
    "measure_predicted = np.array(list(map(np.nanmean, measure)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vars = len(var_names_fmt)\n",
    "fig = plt.figure(figsize=(8, n_vars * 4))\n",
    "if N_H < 3:\n",
    "    gs = fig.add_gridspec(n_vars+1, 4)\n",
    "elif N_H == 3:\n",
    "    gs = fig.add_gridspec(n_vars+1, 6)\n",
    "else:\n",
    "    raise Exception('Supported values of N_H are 1, 2 or 3')\n",
    "ax = []\n",
    "for i in range(n_vars):\n",
    "    if N_H < 3:\n",
    "        ax.append([fig.add_subplot(gs[i, :3]), fig.add_subplot(gs[i, 3])])\n",
    "    else:\n",
    "        ax.append([fig.add_subplot(gs[i, :4]), fig.add_subplot(gs[i, 4:])])\n",
    "if N_H < 3:\n",
    "    ax.append([fig.add_subplot(gs[-1, :2]), fig.add_subplot(gs[-1, 2:])])\n",
    "else:\n",
    "    ax.append([fig.add_subplot(gs[-1, :2]), fig.add_subplot(gs[-1, 2:4]), fig.add_subplot(gs[-1, 4:])])\n",
    "\n",
    "col = [[.2,.2,.2], [.8,0,0], [0,.7,0]]\n",
    "\n",
    "bus_ID = rec_bus_IDs[0]\n",
    "line_IDs = rec_bus_IDs[0], rec_bus_IDs[0] + 1\n",
    "\n",
    "idx = t < 60\n",
    "\n",
    "dm = np.max(measure_exact) - np.min(measure_exact)\n",
    "ylim = [np.min(measure_exact) - dm / 2, np.max(measure_exact) + dm / 2]\n",
    "\n",
    "for i in range(N_H):\n",
    "    for j,var_name in enumerate(var_names_fmt):\n",
    "        if 'bus' in var_name:\n",
    "            key = var_name.format(bus_ID)\n",
    "        elif 'line' in var_name:\n",
    "            key = var_name.format(line_IDs[0], line_IDs[1])\n",
    "        value = data_normalized[i][key]\n",
    "        n,edges = np.histogram(value, bins=25, range=(-4,4), density=True)\n",
    "        ax[j][0].plot(t[idx], value[idx], color=col[i], lw=1, \\\n",
    "                      label=f'{abbrv[area_measure]} = {measure_exact[i]:.2f} {measure_units}')\n",
    "        ax[j][1].plot(n, edges[:-1] + np.diff(edges[:2])[0] / 2, color=col[i], lw=1)\n",
    "        for a in ax[j]:\n",
    "            a.set_ylim([-4,4])\n",
    "        ax[j][0].set_ylabel(key)\n",
    "    ax[-1][i].plot(time / 60, measure[i], 'k', lw=1)\n",
    "    ax[-1][i].plot(time[[0,-1]] / 60, measure_exact[i] + np.zeros(2), 'k--', lw=1)\n",
    "    ax[-1][i].set_ylim(ylim)\n",
    "    ax[-1][i].set_xlabel('Time [min]')\n",
    "\n",
    "for a in ax:\n",
    "    for side in 'right','top':\n",
    "        for aa in a:\n",
    "            aa.spines[side].set_visible(False)\n",
    "\n",
    "ax[0][0].legend(loc='best')\n",
    "ax[-1][0].set_ylim([0.2, 0.25])\n",
    "for i in range(1, N_H):\n",
    "    ax[-1][i].set_xlim(ax[-1][0].get_xlim())\n",
    "    ax[-1][i].set_ylim(ax[-1][0].get_ylim())\n",
    "#     ax[-1][0].get_shared_x_axes().join(ax[-1][0], ax[-1][i])\n",
    "\n",
    "# ax[-1][0].set_xlim([0,30])\n",
    "ax[-1][0].set_ylabel(f'{area_measure.capitalize()} [{measure_units}]')\n",
    "ax[-2][0].set_xlabel('Time [s]')\n",
    "ax[-2][1].set_xlabel('Fraction')\n",
    "fig.tight_layout()\n",
    "# output_filename = f'IEEE39_area{area_ID}_H_G1={H_G1}_' + \\\n",
    "#     f'rec_buses={rec_bus_list}_load_buses={stoch_load_bus_list}_const_H_{experiment_ID[:6]}.pdf'\n",
    "# fig.savefig(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'ieee39_PF_stoch_loads_compensators_5.000_4.330-3.930-4.730_4.470-4.870-4.070_' + \\\n",
    "    '3.570_4.330_4.350_3.770_3.470_3.450_4.200_0.100_0.100_0.100.h5'\n",
    "# data_file = 'ieee39_PF_stoch_loads_compensators_5.000_4.330-3.830-4.830_4.470-3.970-4.970_' + \\\n",
    "#     '3.570_4.330_4.350_3.770_3.470_3.450_4.200_0.100_0.100_0.100.h5'\n",
    "t, data, data_norm, data_sliding, _ = load_data_slide([os.path.join(data_dir, data_file)],\n",
    "                                                    var_names,\n",
    "                                                    None,\n",
    "                                                    data_std,\n",
    "                                                    window_dur,\n",
    "                                                    window_step,\n",
    "                                                    add_omega_ref = False,\n",
    "                                                    verbose = True)\n",
    "dt = np.diff(t[:2])[0]\n",
    "time, HH, _ = predict(model, data_sliding, window_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, HH)\n",
    "plt.ylim([0.2,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant measure while varying a compensator's inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_compensator = True\n",
    "\n",
    "H_values = [default_H_with_comp]\n",
    "if change_compensator:\n",
    "    H_values.append(\n",
    "        OrderedDict([\n",
    "            ('G01', 5.00), ('G02', 4.33), ('G03', 4.47), ('G04', 3.57), ('G05', 4.33),\n",
    "            ('G06', 4.35), ('G07', 3.77), ('G08', 3.47), ('G09', 3.45), ('G10', 4.20),\n",
    "            ('Comp11', 4.0), ('Comp21', 0.1), ('Comp31', 0.1)\n",
    "        ])\n",
    "    )\n",
    "    H_values.append(\n",
    "        OrderedDict([\n",
    "            ('G01', 5.00), ('G02', 4.33), ('G03', 4.47), ('G04', 3.57), ('G05', 4.33),\n",
    "            ('G06', 4.35), ('G07', 3.77), ('G08', 3.47), ('G09', 3.45), ('G10', 4.2),\n",
    "            ('Comp11', 8.0), ('Comp21', 0.1), ('Comp31', 0.1)\n",
    "        ])\n",
    "    )\n",
    "else:\n",
    "    H_values.append(\n",
    "        OrderedDict([\n",
    "            ('G01', 5.00), ('G02', 4.58), ('G03', 4.72), ('G04', 3.57), ('G05', 4.33),\n",
    "            ('G06', 4.35), ('G07', 3.77), ('G08', 3.47), ('G09', 3.45), ('G10', 4.20),\n",
    "            ('Comp11', 0.1), ('Comp21', 0.1), ('Comp31', 0.1)\n",
    "        ])\n",
    "    )\n",
    "    H_values.append(\n",
    "        OrderedDict([\n",
    "            ('G01', 5.00), ('G02', 4.83), ('G03', 4.97), ('G04', 3.57), ('G05', 4.33),\n",
    "            ('G06', 4.35), ('G07', 3.77), ('G08', 3.47), ('G09', 3.45), ('G10', 4.2),\n",
    "            ('Comp11', 0.1), ('Comp21', 0.1), ('Comp31', 0.1)\n",
    "        ])\n",
    "    )\n",
    "\n",
    "N_H = len(H_values)\n",
    "\n",
    "measure_exact = []\n",
    "\n",
    "data_normalized = []\n",
    "measure = []\n",
    "area_inertia = []\n",
    "\n",
    "for H in H_values:\n",
    "    data_file = data_dir + '/ieee39_PF_stoch_loads_compensators_' + \\\n",
    "        '_'.join(map(lambda h: f'{h:.3f}', H.values())) + '.h5'\n",
    "    print(f'Reading data from {data_file}.')\n",
    "    _,_,v,_ = read_area_values(data_file, generators_areas_map_with_comp['default'],\n",
    "                               P_nom_with_comp, area_measure)\n",
    "    _,_,h,_ = read_area_values(data_file, generators_areas_map_with_comp['default'],\n",
    "                               P_nom_with_comp, 'inertia')\n",
    "    measure_exact.append(v[area_ID - 1])\n",
    "    area_inertia.append(h[area_ID - 1])\n",
    "\n",
    "    t, _, data_norm, data_sliding, _ = load_data_slide([data_file],\n",
    "                                                        var_names,\n",
    "                                                        None,\n",
    "                                                        data_std,\n",
    "                                                        window_dur,\n",
    "                                                        window_step,\n",
    "                                                        add_omega_ref = False,\n",
    "                                                        verbose = True)\n",
    "    print(f'Duration of the simulation: {t[-1]} sec.')\n",
    "    data_normalized.append(data_norm)\n",
    "    dt = np.diff(t[:2])[0]\n",
    "    time, HH, _ = predict(model, data_sliding, window_step)\n",
    "    measure.append(HH)\n",
    "measure_exact = np.array(measure_exact)\n",
    "area_inertia = np.array(area_inertia)\n",
    "measure_predicted = np.array(list(map(np.nanmean, measure)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vars = len(var_names_fmt)\n",
    "fig = plt.figure(figsize=(8, n_vars * 2.5))\n",
    "if N_H < 3:\n",
    "    gs = fig.add_gridspec(n_vars+1, 4)\n",
    "elif N_H == 3:\n",
    "    gs = fig.add_gridspec(n_vars+1, 6)\n",
    "else:\n",
    "    raise Exception('Supported values of N_H are 1, 2 or 3')\n",
    "ax = []\n",
    "for i in range(n_vars):\n",
    "    if N_H < 3:\n",
    "        ax.append([fig.add_subplot(gs[i, :3]), fig.add_subplot(gs[i, 3])])\n",
    "    else:\n",
    "        ax.append([fig.add_subplot(gs[i, :4]), fig.add_subplot(gs[i, 4:])])\n",
    "if N_H < 3:\n",
    "    ax.append([fig.add_subplot(gs[-1, :2]), fig.add_subplot(gs[-1, 2:])])\n",
    "else:\n",
    "    ax.append([fig.add_subplot(gs[-1, :2]), fig.add_subplot(gs[-1, 2:4]), fig.add_subplot(gs[-1, 4:])])\n",
    "\n",
    "col = [[.2,.2,.2], [.8,0,0], [0,.7,0]]\n",
    "\n",
    "bus_ID = rec_bus_IDs[0]\n",
    "line_IDs = rec_bus_IDs[0], rec_bus_IDs[0] + 1\n",
    "\n",
    "idx = t < 60\n",
    "\n",
    "dm = np.max(measure_exact) - np.min(measure_exact)\n",
    "ylim = [np.min(measure_exact) - dm / 2, np.max(measure_exact) + dm / 2]\n",
    "\n",
    "for i in range(N_H):\n",
    "    for j,var_name in enumerate(var_names_fmt):\n",
    "        if 'bus' in var_name:\n",
    "            key = var_name.format(bus_ID)\n",
    "        elif 'line' in var_name:\n",
    "            key = var_name.format(line_IDs[0], line_IDs[1])\n",
    "        value = data_normalized[i][key]\n",
    "        n,edges = np.histogram(value, bins=25, range=(-4,4), density=True)\n",
    "        ax[j][0].plot(t[idx], value[idx], color=col[i], lw=1, \\\n",
    "                     label=f'{abbrv[area_measure]} = {measure_exact[i]:.2f} {measure_units}')\n",
    "        ax[j][1].plot(n, edges[:-1] + np.diff(edges[:2])[0] / 2, color=col[i], lw=1)\n",
    "        for a in ax[j]:\n",
    "            a.set_ylim([-4,4])\n",
    "        ax[j][0].set_ylabel(key)\n",
    "    ax[-1][i].plot(time / 60, measure[i], 'k', lw=1)\n",
    "    ax[-1][i].plot(time[[0,-1]] / 60, measure_exact[i] + np.zeros(2), 'k--', lw=1)\n",
    "    ax[-1][i].set_ylim(ylim)\n",
    "    ax[-1][i].set_xlabel('Time [min]')\n",
    "\n",
    "for a in ax:\n",
    "    for side in 'right','top':\n",
    "        for aa in a:\n",
    "            aa.spines[side].set_visible(False)\n",
    "\n",
    "ax[0][0].legend(loc='best')\n",
    "for i in range(1, N_H):\n",
    "    ax[-1][0].get_shared_x_axes().join(ax[-1][0], ax[-1][i])\n",
    "\n",
    "# ax[-1][0].set_xlim([0,30])\n",
    "ax[-1][0].set_ylabel(f'{area_measure.capitalize()} [{measure_units}]')\n",
    "ax[-2][0].set_xlabel('Time [s]')\n",
    "ax[-2][1].set_xlabel('Fraction')\n",
    "fig.tight_layout()\n",
    "# output_filename = f'IEEE39_area{area_ID}_H_G1={H_G1}_' + \\\n",
    "#     f'rec_buses={rec_bus_list}_load_buses={stoch_load_bus_list}_const_H_{experiment_ID[:6]}.pdf'\n",
    "# fig.savefig(output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
